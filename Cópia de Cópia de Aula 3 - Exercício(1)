{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cópia de Cópia de Aula 3 - Exercício","provenance":[{"file_id":"1FFRcCT8Jagd0iKHi_xuMpp97iWzOuck9","timestamp":1630536572774},{"file_id":"1Y3rRUiQGW5CEcPRkx_sfZGAEjNwVsw-b","timestamp":1630526672721},{"file_id":"1ONeS-lZ3vVqThueoTvQRMnZ_rJJB1yOl","timestamp":1629906878859}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1OG5DT_dm6mk"},"source":["# Notebook de referência \n","\n","Nome: "]},{"cell_type":"code","metadata":{"id":"Ckei-yP304GV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Od7iUgHy5SSi"},"source":["## Instruções\n","\n","- Treinar uma rede neural como classificador binário na tarefa de análise de sentimentos usando dataset IMDB.\n","\n","- Experimentar e reportar a acurácia usando 3 diferentes tipos de features como entrada:\n","    1) Bag-of-words booleano\n","    2) Bag-of-words com contagem das palavras (histograma das palavras)\n","    3) TF-IDF\n","\n","Deve-se implementar o laço de treinamento e validação da rede neural.\n","\n","Neste exercício usaremos o IMDB com 20l exemplos para treino, 5k para desenvolvimento e 25k para teste."]},{"cell_type":"markdown","metadata":{"id":"CXFdJz2KVeQw"},"source":["## Preparando Dados"]},{"cell_type":"markdown","metadata":{"id":"gHMi_Kq65fPM"},"source":["Primeiro, fazemos download do dataset:"]},{"cell_type":"code","metadata":{"id":"2wbnfzst5O3k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630536689632,"user_tz":180,"elapsed":19264,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"e010ea89-1eda-4dcc-8b43-36662fdc56d0"},"source":["!wget -nc http://files.fast.ai/data/aclImdb.tgz \n","!tar -xzf aclImdb.tgz"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-09-01 22:51:12--  http://files.fast.ai/data/aclImdb.tgz\n","Resolving files.fast.ai (files.fast.ai)... 104.26.3.19, 172.67.69.159, 104.26.2.19, ...\n","Connecting to files.fast.ai (files.fast.ai)|104.26.3.19|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://files.fast.ai/data/aclImdb.tgz [following]\n","--2021-09-01 22:51:12--  https://files.fast.ai/data/aclImdb.tgz\n","Connecting to files.fast.ai (files.fast.ai)|104.26.3.19|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 145982645 (139M) [application/x-gtar-compressed]\n","Saving to: ‘aclImdb.tgz’\n","\n","aclImdb.tgz         100%[===================>] 139.22M  39.8MB/s    in 3.7s    \n","\n","2021-09-01 22:51:16 (37.9 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"0Giyi5Rv_NIm"},"source":["## Carregando o dataset\n","\n","Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n","\n","Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."]},{"cell_type":"code","metadata":{"id":"0HIN_xLI_TuT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630536695811,"user_tz":180,"elapsed":2262,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"0141209d-1733-449e-e472-687c1786c89f"},"source":["import os\n","import random\n","\n","\n","def load_texts(folder):\n","    texts = []\n","    for path in os.listdir(folder):\n","        with open(os.path.join(folder, path)) as f:\n","            texts.append(f.read())\n","    return texts\n","\n","x_train_pos = load_texts('aclImdb/train/pos')\n","x_train_neg = load_texts('aclImdb/train/neg')\n","x_test_pos = load_texts('aclImdb/test/pos')\n","x_test_neg = load_texts('aclImdb/test/neg')\n","\n","x_train = x_train_pos + x_train_neg\n","x_test = x_test_pos + x_test_neg\n","y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n","y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n","\n","# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n","c = list(zip(x_train, y_train))\n","random.shuffle(c)\n","x_train, y_train = zip(*c)\n","\n","n_train = int(0.8 * len(x_train))\n","\n","x_valid = x_train[n_train:]\n","y_valid = y_train[n_train:]\n","x_train = x_train[:n_train]\n","y_train = y_train[:n_train]\n","\n","print(len(x_train), 'amostras de treino.')\n","print(len(x_valid), 'amostras de desenvolvimento.')\n","print(len(x_test), 'amostras de teste.')\n","\n","print('3 primeiras amostras treino:')\n","for x, y in zip(x_train[:3], y_train[:3]):\n","    print(y, x[:100])\n","\n","print('3 últimas amostras treino:')\n","for x, y in zip(x_train[-3:], y_train[-3:]):\n","    print(y, x[:100])\n","\n","print('3 primeiras amostras validação:')\n","for x, y in zip(x_valid[:3], y_test[:3]):\n","    print(y, x[:100])\n","\n","print('3 últimas amostras validação:')\n","for x, y in zip(x_valid[-3:], y_valid[-3:]):\n","    print(y, x[:100])"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["20000 amostras de treino.\n","5000 amostras de desenvolvimento.\n","25000 amostras de teste.\n","3 primeiras amostras treino:\n","True \"In April 1946, the University of Chicago agreed to operate Argonne National Laboratory, with an ass\n","True I happened to see this movie twice or more and found it well made! WWII had freshly ended and the so\n","False I really liked ZB1. Really, I did. I have no problem with extremely low-budget movies, and I have en\n","3 últimas amostras treino:\n","True Princess Tam Tam is without the trappings of racism, in the way we think of racism in the United Sta\n","True This is not Bela Lagosi's best movie, but it's got a good old style approach for some 40's horror en\n","False I won't say this movie was bad, but it wasn't good either. I expected something good but I guess Hum\n","3 primeiras amostras validação:\n","True This is a weird movie about an archaeologist studying the culture of the ancient Hohokam Indians. Sh\n","True This was one of the DVD's I recently bought in a set of six called \"Frenchfilm\" to brush up our Fren\n","True La Chute de la Maison Usher, or The Fall of the House of Usher as it's know amongst English audience\n","3 últimas amostras validação:\n","False i'm ask... what a f*** are whit the real-TV never i see some b******* in my life is: a******, dirty,\n","True Brilliant adaptation of the novel that made famous the relatives of Chilean President Salvador Allen\n","True A lot has already been said on this movie and I' d like to join those who praised it. It's a highly \n"]}]},{"cell_type":"code","metadata":{"id":"SgQKGxJa14XH","executionInfo":{"status":"ok","timestamp":1630536660292,"user_tz":180,"elapsed":4175,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["import os\n","import random\n","\n","from collections import Counter\n","import numpy as np\n","import torch\n","\n","from re import findall, sub\n","import string\n","\n","from bs4 import BeautifulSoup\n","\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"elgZH6K7j6q_","executionInfo":{"status":"ok","timestamp":1630536757408,"user_tz":180,"elapsed":569,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["class text2token():\n","    \n","    def __init__(self, train_texts, test_texts, valid_texts,\n","                 mode = \"BOW\", boolean=False, max_size=None, stopwords = [], use_unknown=False):       \n","        \n","        self.max_size = max_size\n","        self.boolean = boolean\n","        self.stopwords = stopwords\n","        self.use_unknown = use_unknown\n","        self.mode = mode\n","        \n","        splited_train_text = self.split(train_texts)\n","        self.createModel(splited_train_text)\n","        \n","        self.train_vector = self.tokenizer(splited_train_text)\n","        \n","        splited_test_text = self.split(test_texts)\n","        self.test_vector = self.tokenizer(splited_test_text)\n","        \n","        splited_valid_text = self.split(valid_texts)\n","        self.valid_vector = self.tokenizer(splited_valid_text)\n","        \n","    \n","# SPLIT ============================================================================================ \n","    \n","    def split(self, texts):\n","        \n","        tokenized_texts = []\n","        \n","        \n","        \n","        for text in texts:\n","            \n","            text = text.lower()\n","            \n","            # Remove as tags HTLM\n","            text = BeautifulSoup(text, \"lxml\").text\n","            \n","            # Remove os caracteres especiais\n","            text = sub('[^A-Za-z\\s]+', ' ', text)\n","\n","            # Remove os números\n","            text = sub(r'[0-9+]', ' ', text)\n","            \n","            # Remove alguns pontos e separa\n","            text_splited = findall(r'\\w+|[^?\\-!.,:;\"\\'/><\\s]', text)\n","            \n","            # Remove palavras com menos de 2 caracteres\n","            text_splited = [word for word in text_splited if len(word) > 2]\n","\n","            \n","            tokenized_texts.append(text_splited)\n","            \n","        return tokenized_texts\n","    \n","# CREATETF ============================================================================================    \n","    \n","    def createTF(self, tokenized_texts):\n","\n","        counter = Counter()\n","        for text in tokenized_texts:\n","            counter.update(set(text))\n","        for stop_word in self.stopwords:\n","            if stop_word in counter.keys():\n","                del counter[stop_word]\n","        return counter\n","\n","# CREATEIDF ============================================================================================ \n","    \n","    def createIDF(self, counter):\n","\n","        idf = [self.len_corpus/count for token, count in counter.most_common(self.max_size)]\n","        return np.log10(idf)\n","    \n","# CREATEMODEL ============================================================================================ \n","\n","    def createModel(self, tokenized_texts):\n","        \n","        if self.mode == \"BOW\":\n","            counter = Counter()\n","            for text in tokenized_texts:\n","                counter.update(text)\n","            for stop_word in self.stopwords:\n","                if stop_word in counter.keys():\n","                    del counter[stop_word]\n","            vocab = {element[0]: index for index, element in enumerate(counter.most_common(self.max_size))}\n","            if self.use_unknown:\n","                vocab['unknown'] = len(vocab)\n","                \n","            self.vocabulary = vocab\n","            \n","        if self.mode == \"TFIDF\":\n","            \n","            self.len_corpus = len(tokenized_texts)\n","            counter = self.createTF(tokenized_texts)\n","            print(counter)\n","            \n","            \n","            vocab = {element[0]: index for index, element in enumerate(counter.most_common(self.max_size))}\n","            self.vocabulary = vocab\n","            \n","            self.idf = self.createIDF(counter)\n","            print(self.idf)\n","        \n","# TOKENIZER ============================================================================================ \n","\n","    def tokenizer(self, texts):\n","        \n","        if self.mode == \"BOW\":\n","            transformed_texts = []\n","            if self.use_unknown:\n","                unknown = self.vocabulary.get('unknown')\n","\n","            for i, text in enumerate(texts):\n","                bow_text = torch.zeros(len(self.vocabulary))\n","                counter = Counter(text)\n","\n","                if self.use_unknown:\n","                    index = [self.vocabulary.get(key, unknown) for key in counter.keys()]\n","                else:\n","                    index = [self.vocabulary[key] for key in counter.keys() if key in self.vocabulary.keys()]\n","\n","                if self.boolean:\n","                    bow_text[index] = 1\n","                else:\n","                    values = [value.float() for key, value in counter.items() if self.use_unknown or key in self.vocabulary.keys()]\n","                    bow_text[index] = list(values)\n","\n","                transformed_texts.append(bow_text)\n","\n","            # print(transformed_texts)\n","            # print(type(bow_text))\n","            # return transformed_texts\n","            # return torch.cat(transformed_texts,dim=-1).float()\n","            return torch.vstack(transformed_texts).float()\n","            \n","            \n","        if self.mode == \"TFIDF\":\n","\n","            transformed_texts = []\n","\n","            for i, text in enumerate(texts):\n","                bow_text = torch.zeros(len(self.vocabulary))\n","                counter = Counter(text)\n","                print(counter)\n","                index = []\n","                values = []\n","                for key, value in counter.items():\n","                    if key in self.vocabulary.keys():\n","                        index.append(self.vocabulary[key])\n","                        values.append(value)\n","\n","                bow_text[index] = list(values)\n","                print(bow_text)\n","                transformed_texts.append(bow_text * self.idf)\n","\n","            # return transformed_texts\n","            return torch.vstack(transformed_texts).float()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ffs36MmPkLGl","executionInfo":{"status":"ok","timestamp":1630536843490,"user_tz":180,"elapsed":552,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["# CLASS VECTORIZEDATASET =================================================================================\n","\n","class VectorizedDataset(Dataset):\n","    def __init__(self, x, y):\n","        self.x = x\n","        self.y = y\n","\n","    def __len__(self) -> int:\n","        return len(self.x)\n","\n","    def __getitem__(self, idx):\n","        x = self.x[idx]\n","        y = self.y[idx]\n","        return x, y\n","\n","# CLASS MLP ==============================================================================================\n","    \n","class MLP(torch.nn.Module):\n","    def __init__(self, input_size, hidden_units=128):\n","        super().__init__()\n","        self.dense = torch.nn.Sequential(\n","            torch.nn.Linear(input_size, 128),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(128, 64),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(64, 32),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(32, 1),\n","            torch.nn.Sigmoid()\n","        )\n","    \n","    def forward(self, x):\n","        x = self.dense(x)\n","        return x\n","\n","# CLASS REDENEURAL =======================================================================================\n","    \n","class redeNeural():\n","    \n","    def __init__(self, max_size,\n","                 vectorized_texts_train, y_train,\n","                 vectorized_texts_valid, y_valid,\n","                 vectorized_texts_test, y_test,\n","                 vectorizer, batch_size, shuffle):       \n","        \n","        self.max_size = max_size\n","        \n","        self.vectorized_texts_train = vectorized_texts_train\n","        self.y_train = y_train\n","        self.vectorized_texts_valid = vectorized_texts_valid\n","        self.y_valid = y_valid\n","        self.vectorized_texts_test = vectorized_texts_test\n","        self.y_test = y_test\n","        \n","        self.vectorizer = vectorizer\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        \n","        \n","        \n","        if torch.cuda.is_available(): \n","            dev = \"cuda:0\"\n","            print(torch. cuda. get_device_name(dev))\n","        else: \n","            dev = \"cpu\" \n","        print(dev)\n","        device = torch.device(dev)\n","        \n","        \n","        self.mlp_bow_bool = MLP(self.max_size)\n","        self.mlp_bow_bool.to(device)\n","        \n","        self.create_nn_input()\n","\n","        \n","    def create_nn_input(self):\n","\n","        train_dataset = VectorizedDataset(self.vectorized_texts_train, torch.Tensor(self.y_train).reshape(-1,1))\n","        valid_dataset = VectorizedDataset(self.vectorized_texts_valid, torch.Tensor(self.y_valid).reshape(-1,1))\n","        test_dataset = VectorizedDataset(self.vectorized_texts_test, torch.Tensor(self.y_test).reshape(-1,1))\n","\n","        self.vectorized_texts_train = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n","        self.vectorized_texts_valid = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False)\n","        self.vectorized_texts_test = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n","\n","        return self.vectorized_texts_train, self.vectorized_texts_valid, self.vectorized_texts_test, self.vectorizer\n","\n","\n","    def train(self, model, train, valid, criterion, optimizer,\n","              max_size, filename_save, batch_size=128, n_epochs=10):\n","\n","        best_valid_loss = 10e9\n","        best_epoch = 0\n","\n","        for i in range(n_epochs):\n","            accumulated_loss = 0\n","            model.train()\n","            for x_train, y_train in train:\n","                x_train = x_train.to(device)\n","                y_train = y_train.to(device)\n","                outputs = model(x_train)\n","                batch_loss = criterion(outputs, y_train)\n","\n","                optimizer.zero_grad()\n","                batch_loss.backward()\n","                optimizer.step()\n","                accumulated_loss += batch_loss.item()\n","\n","            train_loss = accumulated_loss / len(train.dataset)\n","\n","            # Laço de Validação, um a cada época.\n","            accumulated_loss = 0\n","            accumulated_accuracy = 0\n","            model.eval()\n","            with torch.no_grad():\n","                for x_valid, y_valid in valid:\n","                    x_valid = x_valid.to(device)\n","                    y_valid = y_valid.to(device)\n","\n","                    # predict da rede\n","                    outputs = model(x_valid)\n","\n","                    # calcula a perda\n","                    batch_loss = criterion(outputs, y_valid)\n","                    preds = outputs > 0.5\n","\n","                    # calcula a acurácia\n","                    batch_accuracy = (preds == y_valid).sum()\n","                    accumulated_loss += batch_loss\n","                    accumulated_accuracy += batch_accuracy\n","\n","            valid_loss = accumulated_loss / len(valid.dataset)\n","            valid_acc = accumulated_accuracy / len(valid.dataset)\n","            print(f'Época: {i:d}/{n_epochs - 1:d} Train Loss: {train_loss:.6f} Valid Loss: {valid_loss:.6f} Valid Acc: {valid_acc:.3f}')\n","\n","            # Salvando o melhor modelo de acordo com a loss de validação\n","            if valid_loss < best_valid_loss:\n","                torch.save(model.state_dict(), filename_save + '.pt')\n","                best_valid_loss = valid_loss\n","                best_epoch = i\n","                print('best model')\n","\n","        return model\n","    \n","    def predict(self, model, state_dict, test):\n","        accumulated_accuracy = 0\n","        model.load_state_dict(torch.load(state_dict + '.pt'))\n","        model.eval()\n","        with torch.no_grad():\n","            for x_test, y_test in test:\n","                x_test = x_test.to(device)\n","                y_test = y_test.to(device)\n","\n","                # predict da rede\n","                outputs = model(x_test)\n","\n","                # calcula a perda\n","                batch_loss = criterion(outputs, y_test)\n","                preds = outputs > 0.5\n","\n","                # calcula a acurácia\n","                batch_accuracy = (preds == y_test).sum()\n","                accumulated_accuracy += batch_accuracy\n","\n","        test_acc = accumulated_accuracy / len(test.dataset)\n","        test_acc *= 100\n","        print('*' * 40)\n","        print(f'Acurácia de {test_acc:.3f} %')\n","        print('*' * 40)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXh2LN5uko2v","executionInfo":{"status":"ok","timestamp":1630537059633,"user_tz":180,"elapsed":205,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["max_size = 3000\n","batch_size = 64\n","n_epochs = 10\n","learningRate = 0.1\n","save_filename = 'bow_bool'"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"o5YBEH5dkPYm","executionInfo":{"status":"error","timestamp":1630537506929,"user_tz":180,"elapsed":32130,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"0da56b36-3ff5-4374-b224-3d79d02cc8af"},"source":["data = text2token(train_texts=x_train, test_texts=x_test, valid_texts=x_valid, boolean=True, max_size = max_size)\n","\n","RNA = redeNeural(max_size,\n","                 data.train_vector, y_train,\n","                 data.valid_vector, y_valid,\n","                 data.test_vector, y_test,\n","                 data, batch_size, True)\n","\n","vectorized_train = RNA.vectorized_texts_train,\n","vectorized_valid = RNA.vectorized_texts_valid\n","vectorized_test = RNA.vectorized_texts_test\n","_ = RNA.vectorizer\n","\n","criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(RNA.mlp_bow_bool.parameters(), lr=learningRate)\n","\n","print(type(vectorized_train))\n","_ = RNA.train(RNA.mlp_bow_bool, vectorized_train, vectorized_valid, criterion, optimizer,\n","              max_size, save_filename, batch_size, n_epochs=n_epochs)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla K80\n","cuda:0\n","<class 'tuple'>\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-f1567da51dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m _ = RNA.train(RNA.mlp_bow_bool, vectorized_train, vectorized_valid, criterion, optimizer,\n\u001b[0;32m---> 19\u001b[0;31m               max_size, save_filename, batch_size, n_epochs=n_epochs)\n\u001b[0m","\u001b[0;32m<ipython-input-8-a2214302133f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train, valid, criterion, optimizer, max_size, filename_save, batch_size, n_epochs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0maccumulated_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]}]}