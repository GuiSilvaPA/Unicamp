{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aula 2 - Guilherme Pereira Albino da Silva","provenance":[{"file_id":"1csLuFrwl5v7BY7BFBa_eVw0HT4R2R64P","timestamp":1629931671965},{"file_id":"10Ztjm12A7moD-9RMoDHO8h7WSFDypt5Z","timestamp":1629193880182},{"file_id":"1wVCPxMZb1TWVquVjO9BTqsea-_gS_5U5","timestamp":1629129871498},{"file_id":"1swETqRscXPb-uSKmdPMWOiPm99xDYCum","timestamp":1585238920033}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Od7iUgHy5SSi"},"source":["# Aula 2: Análise de Sentimentos usando Bag of Words e TF-IDF\n","Nome: Guilherme Pereira Albino da Silva\n","\n","Neste notebook iremos treinar um modelo para fazer análise de sentimento usando o dataset IMDB."]},{"cell_type":"markdown","metadata":{"id":"CXFdJz2KVeQw"},"source":["# Preparando Dados"]},{"cell_type":"markdown","metadata":{"id":"gHMi_Kq65fPM"},"source":["Primeiro, fazemos download do dataset:"]},{"cell_type":"code","metadata":{"id":"2wbnfzst5O3k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629932408668,"user_tz":180,"elapsed":783,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"4e884aa0-cac0-46fa-aceb-0d394ff679c4"},"source":["!wget -nc http://files.fast.ai/data/examples/imdb_sample.tgz\n","!tar -xzf imdb_sample.tgz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-08-25 23:00:08--  http://files.fast.ai/data/examples/imdb_sample.tgz\n","Resolving files.fast.ai (files.fast.ai)... 104.26.2.19, 104.26.3.19, 172.67.69.159, ...\n","Connecting to files.fast.ai (files.fast.ai)|104.26.2.19|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://files.fast.ai/data/examples/imdb_sample.tgz [following]\n","--2021-08-25 23:00:08--  https://files.fast.ai/data/examples/imdb_sample.tgz\n","Connecting to files.fast.ai (files.fast.ai)|104.26.2.19|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 571827 (558K) [application/x-gtar-compressed]\n","Saving to: ‘imdb_sample.tgz’\n","\n","imdb_sample.tgz     100%[===================>] 558.42K  3.52MB/s    in 0.2s    \n","\n","2021-08-25 23:00:09 (3.52 MB/s) - ‘imdb_sample.tgz’ saved [571827/571827]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0Giyi5Rv_NIm"},"source":["Carregamos o dataset .csv usando o pandas:"]},{"cell_type":"code","metadata":{"id":"0HIN_xLI_TuT","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1629932410605,"user_tz":180,"elapsed":8,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"00fbc5b3-862d-49d1-e1c5-45f6fe360190"},"source":["import pandas as pd\n","df = pd.read_csv('imdb_sample/texts.csv')\n","df.shape\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>is_valid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>negative</td>\n","      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>positive</td>\n","      <td>This is a extremely well-made film. The acting...</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>negative</td>\n","      <td>Every once in a long while a movie will come a...</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>positive</td>\n","      <td>Name just says it all. I watched this movie wi...</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>negative</td>\n","      <td>This movie succeeds at being one of the most u...</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      label                                               text  is_valid\n","0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n","1  positive  This is a extremely well-made film. The acting...     False\n","2  negative  Every once in a long while a movie will come a...     False\n","3  positive  Name just says it all. I watched this movie wi...     False\n","4  negative  This movie succeeds at being one of the most u...     False"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"S8dfjdJ-AV79"},"source":["Iremos agora dividir o dataset em conjuntos de treino e teste:"]},{"cell_type":"code","metadata":{"id":"KCoftmPmAfXE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629932412118,"user_tz":180,"elapsed":6,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"2e745cf5-cb21-4f11-8840-b34228603d36"},"source":["treino = df[df['is_valid'] == False]\n","valid = df[df['is_valid'] == True]\n","\n","print('treino.shape:', treino.shape)\n","print('valid.shape:', valid.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["treino.shape: (800, 3)\n","valid.shape: (200, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZHus6FH7DftH"},"source":["E iremos dividir estes dois conjuntos em entrada (X) e saída desejada (Y, ground-truth) do modelo:"]},{"cell_type":"code","metadata":{"id":"I2HyoywGDcW8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629932413883,"user_tz":180,"elapsed":210,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"f836657c-4af8-4cf9-a300-93324ac4223d"},"source":["X_treino = treino['text']\n","Y_treino = treino['label']\n","X_valid = valid['text']\n","Y_valid = valid['label']\n","\n","print('X_treino.head():', X_treino.head())\n","print('Y_treino.head():', Y_treino.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_treino.head(): 0    Un-bleeping-believable! Meg Ryan doesn't even ...\n","1    This is a extremely well-made film. The acting...\n","2    Every once in a long while a movie will come a...\n","3    Name just says it all. I watched this movie wi...\n","4    This movie succeeds at being one of the most u...\n","Name: text, dtype: object\n","Y_treino.head(): 0    negative\n","1    positive\n","2    negative\n","3    positive\n","4    negative\n","Name: label, dtype: object\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M2yNXutfEXQ7"},"source":["Ainda falta converter as strings \"positive\" e \"negative\" do ground-truth para valores booleanos:"]},{"cell_type":"code","metadata":{"id":"46RdLFLkEW-X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629932415648,"user_tz":180,"elapsed":217,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"d69675e4-9a79-4571-a96c-1c48fb90849c"},"source":["mapeamento = {'positive': True, 'negative': False}\n","Y_treino_bool = Y_treino.map(mapeamento)\n","Y_valid_bool = Y_valid.map(mapeamento)\n","print(Y_treino_bool.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0    False\n","1     True\n","2    False\n","3     True\n","4    False\n","Name: label, dtype: bool\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mOSHsobyfplY"},"source":["import pandas as pd\n","\n","import collections\n","import re\n","import random\n","\n","import string\n","\n","from collections import Counter\n","\n","import numpy as np\n","\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5M4HpLagTEU"},"source":["# Classe Tokenizer\n","\n","Abaixo irei criar a classe \"Tokenizer\", ela tem como dados de entrada:\n","\n","\n","*   train_text: todos os textos usados para treinamento e vocabulário\n","*   valid_text: textos que serão usados para validação\n","*   vocab_len=None: número máximo de palavras no vocabulário\n","\n","Como saída, tem-se:\n","\n","*   bool_train: textos tokenizados com BoW usando booleano\n","*   freq_train: textos tokenizados com BoW usando frequência\n","*   tfidf_train: textos tokenizados com tf-idf\n","\n","*   bool_valid: textos tokenizados com BoW usando booleano\n","*   freq_valid: textos tokenizados com BoW usando frequência\n","*   tfidf_valid: textos tokenizados com tf-idf\n","\n","Ao forncecer os dois conjuntos de textos na entrada, a classe se encarrega de realizar todos os cálculos automaticamente"]},{"cell_type":"code","metadata":{"id":"HIWZ0QmagPh3"},"source":["class tokenizer():\n","    \n","    \n","    def __init__(self, train_text, valid_text, vocab_len=None):\n","        \n","        self.data_train = train_text\n","        self.data_valid = valid_text\n","        self.vocab_len = vocab_len\n","        # self.stop_words = {'br', 'if', 'll', 'with', 'y', 'have', \"are\", 'into', 'all', 'by', \"must\", 'during', 'now', 'when', 'once', \"might\", 'very', 're', 'doing', \"would\", 'wouldn', 'her', 'itself', 'only', 'no', 'each', 'couldn', 'needn', 'so', 'aren', \"you'll\", 'i', 'against', 'at', 'most', 'both', 'we', 'some', 'didn', 'them', 'does', 'him', 'or', 'whom', 'has', 'after', 'to', \"could\", 'he', 'an', 'himself', 'why', 'won', 'yourself', 'down', \"it's\", 'isn', 'be', 'will', 'she', 'and', 's', 'further', 'mightn', 'for', 'in', 'me', 'they', 'of', 'few', 'those', 've', 'then', 'am', 'the', 'my', 'did', 'before', 'such', 'just', 'who', 'hers', 'a', 'above', 'while', 'how', 'nor', 'weren', 'ain', 'ourselves', \"sha\", 'our', 'your', \"you'd\", 'because', 'any', 'up', \"do\", 'doesn', 'which', 'on', \"you've\", 'theirs', 'shan', \"have\", 'off', \"had\", 'don', 'between', 'until', 'these', 'this', 'can', 'herself', 'out', 'where', \"that'll\", \"she's\", 'had', 'haven', 'ours', \"wo\", 'same', 'were', 'from', 'yours', 'being', 'mustn', 'shouldn', 'other', 't', 'their', 'do', 'should', 'over', 'what', 'ma', 'below', 'd', 'more', 'is', 'it', \"was\", 'through', 'there', 'was', 'had', 'yourselves', 'his', 'has', \"is\", 'are', 'here', 'myself', 'themselves', 'having', \"has\", 'its', 'again', 'than', \"did\", 'o', 'about', 'you', 'but', 'too', 'that', 'been', 'm', 'under', 'own', \"should've\", \"were\", \"does\", 'as', 'was', \"need\", \"you're\"}\n","        \n","        self.createVocab()\n","        self.trainModel()\n","        self.validModel()\n","    \n","    def textClean(self, text):\n","        \n","        self.text = text\n","        \n","        self.text = self.text.lower()\n","  \n","        self.text = re.sub('[^A-Za-z\\s]+', ' ', self.text)\n","        self.text = re.sub(r'[0-9+]', ' ', self.text)\n","        self.text = re.sub(r'\\s+', ' ', self.text)\n","        \n","        self.text = self.text.replace(\"br\", \"\")\n","    \n","        # self.text = self.text.replace(\"n't\", \" \"+\"not\"+\" \")\n","    \n","        self.text_splited = re.findall(r\"[\\w']+|[!#$%&()*+,-./:;<=>?@[\\]^_{|}~]\", self.text)\n","        # print(len(text_splited))\n","    \n","        self.table = str.maketrans('', '', string.punctuation)\n","        self.text_splited = [w.translate(self.table) for w in self.text_splited]\n","        # print(len(text_splited))\n","    \n","        self.text_splited = [word for word in self.text_splited if word.isalpha()]\n","        # print(len(text_splited))\n","    \n","        # self.text_splited = [w for w in self.text_splited if not w in self.stop_words]\n","        # print(len(text_splited))\n","    \n","        self.text_splited = [word for word in self.text_splited if len(word) > 2]\n","        # print(len(text_splited))\n","\n","        return self.text_splited\n","    \n","    def createVocab(self):\n","        \n","        self.all_texts = ' '.join(self.data_train.values)\n","        self.words_list = self.textClean(self.all_texts)\n","    \n","    \n","        # Build vocabulary\n","        if self.vocab_len:\n","            c = Counter()\n","            c.update(self.words_list)\n","            self.vocab = list(dict(c.most_common(self.vocab_len)))\n","        else:\n","            self.vocab = list(set(self.words_list))\n","        self.vocab.sort()\n","        \n","    def word_counter(self, text):\n","        self.to_count = text\n","        \n","        c = Counter()\n","        c.update(self.to_count)\n","        self.dict_c = dict(c)\n","        \n","        return {word: self.dict_c[word] if word in self.dict_c.keys() else 0 for word in self.vocab}\n","    \n","    def bow_bool(self, tf):\n","\n","        token_bool = [np.array(list(i.values())) for i in tf]\n","        token_bool = np.stack(token_bool)\n","        token_bool[token_bool != 0]  = 1\n","\n","        return token_bool\n","     \n","    def bow_freq(self, tf):\n","\n","        token_freq = [np.array(list(i.values())) for i in tf]\n","        token_freq = np.stack(token_freq)\n","\n","        return token_freq\n","    \n","    def tf_idf(self, tf, dados):\n","\n","        num_documentos = len(dados)\n","\n","        ttf = [np.array(list(x.values())) for x in tf]\n","        ttf = np.stack(ttf)\n","        ttf[ttf != 0]  = 1\n","\n","        ttf_sum = ttf.sum(axis = 0)\n","\n","        idf = np.array([[math.log(num_documentos/i, 10) if i!=0 else 0.0 for i in ttf_sum] for y in range(len(dados))])\n","\n","        token_tfidf = [np.array(list(i.values())) for i in tf]\n","        token_tfidf = np.stack(token_tfidf)\n","        token_tfidf = np.multiply(token_tfidf,idf)\n","\n","        return token_tfidf\n","    \n","    def trainModel(self):\n","        \n","        self.data_train_list = np.array([self.textClean(text) for text in self.data_train.values],dtype=list)\n","        self.train_token = [self.word_counter(i) for i in self.data_train_list]\n","        \n","        self.bool_train = self.bow_bool(self.train_token)\n","        self.freq_train = self.bow_freq(self.train_token)\n","        self.tfidf_train = self.tf_idf(self.train_token, self.data_train)\n","        \n","    def validModel(self):\n","        \n","        self.data_valid_list = np.array([self.textClean(text) for text in self.data_valid.values],dtype=list)\n","        self.valid_token = [self.word_counter(i) for i in self.data_valid_list]\n","        \n","        self.bool_valid = self.bow_bool(self.valid_token)\n","        self.freq_valid = self.bow_freq(self.valid_token)\n","        self.tfidf_valid = self.tf_idf(self.valid_token, self.data_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Of0eMXJrjqu7"},"source":["Agora chamamos a classe e realizamos alguns testes"]},{"cell_type":"code","metadata":{"id":"pcESHyXXinz7"},"source":["toke = tokenizer(X_treino, X_valid, vocab_len=1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7T7zK1ABjkHG","executionInfo":{"status":"ok","timestamp":1629932656311,"user_tz":180,"elapsed":216,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"a976850d-e72b-4448-cb5e-dbbbcb1de616"},"source":["print(type(toke.vocab))\n","print(len(toke.vocab))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'list'>\n","1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcAyIfGZj0ga","executionInfo":{"status":"ok","timestamp":1629932714411,"user_tz":180,"elapsed":202,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"cce9a58e-e649-43b4-e5ad-c4c22f0ee0b4"},"source":["print(type(toke.data_train_list))\n","print(type(toke.data_train_list[0]))\n","print(type(toke.data_train_list[0][0]))\n","print(toke.data_train_list[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","<class 'list'>\n","<class 'str'>\n","['bleeping', 'believable', 'meg', 'ryan', 'doesn', 'even', 'look', 'her', 'usual', 'pert', 'lovable', 'self', 'this', 'which', 'normally', 'makes', 'forgive', 'her', 'shallow', 'ticky', 'acting', 'schtick', 'hard', 'believe', 'she', 'was', 'the', 'producer', 'this', 'dog', 'plus', 'kevin', 'kline', 'what', 'kind', 'suicide', 'trip', 'has', 'his', 'career', 'been', 'whoosh', 'banzai', 'finally', 'this', 'was', 'directed', 'the', 'guy', 'who', 'did', 'big', 'chill', 'must', 'replay', 'jonestown', 'hollywood', 'style', 'wooofff']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HN9aos3Rj4nE","executionInfo":{"status":"ok","timestamp":1629932734259,"user_tz":180,"elapsed":200,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"cf763030-0cd5-4da4-84a8-f1284a36aa99"},"source":["print(type(toke.train_token))\n","print(type(toke.train_token[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'list'>\n","<class 'dict'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W-gmhAIQkCP1"},"source":["Abaixo, eu utilizei \"RandomForestClassifier\" como classificador"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9vxTttHIj-3c","executionInfo":{"status":"ok","timestamp":1629932885755,"user_tz":180,"elapsed":554,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"48e6c556-2425-4060-bb91-2735b357d176"},"source":["clf1 = RandomForestClassifier(n_estimators=100, max_depth=30, random_state=0)\n","\n","clf1.fit(toke.bool_train, Y_treino)\n","\n","preds_boll = clf1.predict(toke.bool_valid)\n","\n","bow_bool_precisao = (Y_valid == preds_boll).sum() / len(Y_valid)\n","print(\"Precisão de BoW usando booleano: \" + str(bow_bool_precisao*100) + \"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Precisão de BoW usando booleano: 80.5%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7cmydSsj_UG","executionInfo":{"status":"ok","timestamp":1629932955242,"user_tz":180,"elapsed":768,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"599ff218-57f7-43c7-d585-e8e205b51678"},"source":["clf2 = RandomForestClassifier(n_estimators=150, max_depth=20, random_state=0)\n","\n","clf2.fit(toke.freq_train, Y_treino)\n","\n","preds_freq = clf2.predict(toke.freq_valid)\n","\n","bow_freq_precisao = (Y_valid == preds_freq).sum() / len(Y_valid)\n","print(\"Precisão de BoW usando frequencia: \" + str(bow_freq_precisao*100) + \"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Precisão de BoW usando frequencia: 80.0%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1mW0K7Lvj_lL","executionInfo":{"status":"ok","timestamp":1629933828549,"user_tz":180,"elapsed":970,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"87f6ae78-5077-4186-d9cc-f58571580701"},"source":["clf3 = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=0)\n","\n","clf3.fit(toke.tfidf_train, Y_treino)\n","\n","preds_tfidf = clf3.predict(toke.tfidf_valid)\n","\n","tfidf_precisao = (Y_valid == preds_tfidf).sum() / len(Y_valid)\n","print(\"Precisão de tf-idf: \" + str(tfidf_precisao*100) + \"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Precisão de tf-idf: 80.0%\n"],"name":"stdout"}]}]}