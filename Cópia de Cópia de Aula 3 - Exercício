{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cópia de Cópia de Aula 3 - Exercício","provenance":[{"file_id":"1FFRcCT8Jagd0iKHi_xuMpp97iWzOuck9","timestamp":1630537581262},{"file_id":"1Y3rRUiQGW5CEcPRkx_sfZGAEjNwVsw-b","timestamp":1630526672721},{"file_id":"1ONeS-lZ3vVqThueoTvQRMnZ_rJJB1yOl","timestamp":1629906878859}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1OG5DT_dm6mk"},"source":["# Notebook de referência \n","\n","Nome: "]},{"cell_type":"code","metadata":{"id":"Ckei-yP304GV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Od7iUgHy5SSi"},"source":["## Instruções\n","\n","- Treinar uma rede neural como classificador binário na tarefa de análise de sentimentos usando dataset IMDB.\n","\n","- Experimentar e reportar a acurácia usando 3 diferentes tipos de features como entrada:\n","    1) Bag-of-words booleano\n","    2) Bag-of-words com contagem das palavras (histograma das palavras)\n","    3) TF-IDF\n","\n","Deve-se implementar o laço de treinamento e validação da rede neural.\n","\n","Neste exercício usaremos o IMDB com 20l exemplos para treino, 5k para desenvolvimento e 25k para teste."]},{"cell_type":"markdown","metadata":{"id":"CXFdJz2KVeQw"},"source":["## Preparando Dados"]},{"cell_type":"markdown","metadata":{"id":"gHMi_Kq65fPM"},"source":["Primeiro, fazemos download do dataset:"]},{"cell_type":"code","metadata":{"id":"2wbnfzst5O3k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630537650548,"user_tz":180,"elapsed":20225,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"84a03946-1115-4fb5-edf5-c2c8da19176f"},"source":["!wget -nc http://files.fast.ai/data/aclImdb.tgz \n","!tar -xzf aclImdb.tgz"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-09-01 23:07:12--  http://files.fast.ai/data/aclImdb.tgz\n","Resolving files.fast.ai (files.fast.ai)... 172.67.69.159, 104.26.3.19, 104.26.2.19, ...\n","Connecting to files.fast.ai (files.fast.ai)|172.67.69.159|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://files.fast.ai/data/aclImdb.tgz [following]\n","--2021-09-01 23:07:12--  https://files.fast.ai/data/aclImdb.tgz\n","Connecting to files.fast.ai (files.fast.ai)|172.67.69.159|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 145982645 (139M) [application/x-gtar-compressed]\n","Saving to: ‘aclImdb.tgz’\n","\n","aclImdb.tgz         100%[===================>] 139.22M  40.1MB/s    in 3.7s    \n","\n","2021-09-01 23:07:16 (37.6 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"0Giyi5Rv_NIm"},"source":["## Carregando o dataset\n","\n","Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n","\n","Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."]},{"cell_type":"code","metadata":{"id":"0HIN_xLI_TuT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630537693345,"user_tz":180,"elapsed":2285,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"f071437d-bbe5-4c10-c3e7-7390106a1872"},"source":["import os\n","import random\n","\n","\n","def load_texts(folder):\n","    texts = []\n","    for path in os.listdir(folder):\n","        with open(os.path.join(folder, path)) as f:\n","            texts.append(f.read())\n","    return texts\n","\n","x_train_pos = load_texts('aclImdb/train/pos')\n","x_train_neg = load_texts('aclImdb/train/neg')\n","x_test_pos = load_texts('aclImdb/test/pos')\n","x_test_neg = load_texts('aclImdb/test/neg')\n","\n","x_train = x_train_pos + x_train_neg\n","x_test = x_test_pos + x_test_neg\n","y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n","y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n","\n","# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n","c = list(zip(x_train, y_train))\n","random.shuffle(c)\n","x_train, y_train = zip(*c)\n","\n","n_train = int(0.8 * len(x_train))\n","\n","x_valid = x_train[n_train:]\n","y_valid = y_train[n_train:]\n","x_train = x_train[:n_train]\n","y_train = y_train[:n_train]\n","\n","print(len(x_train), 'amostras de treino.')\n","print(len(x_valid), 'amostras de desenvolvimento.')\n","print(len(x_test), 'amostras de teste.')\n","\n","print('3 primeiras amostras treino:')\n","for x, y in zip(x_train[:3], y_train[:3]):\n","    print(y, x[:100])\n","\n","print('3 últimas amostras treino:')\n","for x, y in zip(x_train[-3:], y_train[-3:]):\n","    print(y, x[:100])\n","\n","print('3 primeiras amostras validação:')\n","for x, y in zip(x_valid[:3], y_test[:3]):\n","    print(y, x[:100])\n","\n","print('3 últimas amostras validação:')\n","for x, y in zip(x_valid[-3:], y_valid[-3:]):\n","    print(y, x[:100])"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["20000 amostras de treino.\n","5000 amostras de desenvolvimento.\n","25000 amostras de teste.\n","3 primeiras amostras treino:\n","False Creep - \"Your journey terminates here.\" Some very graphic scenes and...well, yeah, that's about all \n","True I ran across this movie on the tv and could not turn it off. Peter Sellers plays an unlikable fellow\n","True Tigerland follows the lives of a group of recently drafted men into the army who are called up to fi\n","3 últimas amostras treino:\n","False This film was terrible. I have given it the high score of 2 as I have seen worse, but very few.<br /\n","True Ride With The Devil directed by Ang Lee(Crouching Tiger) is another gem in this fine directors cap. \n","True This is one military drama I like a lot! Tom Berenger playing military assassin Thomas Beckett. This\n","3 primeiras amostras validação:\n","True <br /><br />I take issue with the other reviewer's comments for the simple reason that this is a MYS\n","True Cinderella is a beautiful young woman who is treated cruelly by her wicked stepmother and stepsister\n","True The dubbing/translation in this movie is downright hilarious and provides the only entertainment in \n","3 últimas amostras validação:\n","True Though this series only ran a season, it has stayed with me for 20 years. It was by far and above my\n","False I rented this one on DVD without any prior knowledge. I was suspicious seeing Michael Madsen appeari\n","False Usually when a movie receives a vote of one it is because someone simply dislikes it and is annoyed \n"]}]},{"cell_type":"code","metadata":{"id":"SgQKGxJa14XH","executionInfo":{"status":"ok","timestamp":1630537896046,"user_tz":180,"elapsed":4430,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["import os\n","import random\n","\n","from collections import Counter\n","import numpy as np\n","import torch\n","\n","from re import findall, sub\n","import string\n","\n","from bs4 import BeautifulSoup"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_N-426i050-","executionInfo":{"status":"ok","timestamp":1630537898346,"user_tz":180,"elapsed":544,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["class text2token():\n","    \n","    def __init__(self, train_texts, test_texts, valid_texts,\n","                 mode = \"BOW\", boolean=False, max_size=None, stopwords = [], use_unknown=False):       \n","        \n","        self.max_size = max_size\n","        self.boolean = boolean\n","        self.stopwords = stopwords\n","        self.use_unknown = use_unknown\n","        self.mode = mode\n","        \n","        splited_train_text = self.split(train_texts)\n","        self.createModel(splited_train_text)\n","        \n","        self.train_vector = self.tokenizer(splited_train_text)\n","        \n","        splited_test_text = self.split(test_texts)\n","        self.test_vector = self.tokenizer(splited_test_text)\n","        \n","        splited_valid_text = self.split(valid_texts)\n","        self.valid_vector = self.tokenizer(splited_valid_text)\n","        \n","    \n","# SPLIT ============================================================================================ \n","    \n","    def split(self, texts):\n","        \n","        tokenized_texts = []\n","        \n","        \n","        \n","        for text in texts:\n","            \n","            text = text.lower()\n","            \n","            # Remove as tags HTLM\n","            text = BeautifulSoup(text, \"lxml\").text\n","            \n","            # Remove os caracteres especiais\n","            text = sub('[^A-Za-z\\s]+', ' ', text)\n","\n","            # Remove os números\n","            text = sub(r'[0-9+]', ' ', text)\n","            \n","            # Remove alguns pontos e separa\n","            text_splited = findall(r'\\w+|[^?\\-!.,:;\"\\'/><\\s]', text)\n","            \n","            # Remove palavras com menos de 2 caracteres\n","            text_splited = [word for word in text_splited if len(word) > 2]\n","\n","            \n","            tokenized_texts.append(text_splited)\n","            \n","        return tokenized_texts\n","    \n","# CREATETF ============================================================================================    \n","    \n","    def createTF(self, tokenized_texts):\n","\n","        counter = Counter()\n","        for text in tokenized_texts:\n","            counter.update(set(text))\n","        for stop_word in self.stopwords:\n","            if stop_word in counter.keys():\n","                del counter[stop_word]\n","        return counter\n","\n","# CREATEIDF ============================================================================================ \n","    \n","    def createIDF(self, counter):\n","\n","        idf = [self.len_corpus/count for token, count in counter.most_common(self.max_size)]\n","        return np.log10(idf)\n","    \n","# CREATEMODEL ============================================================================================ \n","\n","    def createModel(self, tokenized_texts):\n","        \n","        if self.mode == \"BOW\":\n","            counter = Counter()\n","            for text in tokenized_texts:\n","                counter.update(text)\n","            for stop_word in self.stopwords:\n","                if stop_word in counter.keys():\n","                    del counter[stop_word]\n","            vocab = {element[0]: index for index, element in enumerate(counter.most_common(self.max_size))}\n","            if self.use_unknown:\n","                vocab['unknown'] = len(vocab)\n","                \n","            self.vocabulary = vocab\n","            \n","        if self.mode == \"TFIDF\":\n","            \n","            self.len_corpus = len(tokenized_texts)\n","            counter = self.createTF(tokenized_texts)\n","            print(counter)\n","            \n","            \n","            vocab = {element[0]: index for index, element in enumerate(counter.most_common(self.max_size))}\n","            self.vocabulary = vocab\n","            \n","            self.idf = self.createIDF(counter)\n","            print(self.idf)\n","        \n","# TOKENIZER ============================================================================================ \n","\n","    def tokenizer(self, texts):\n","        \n","        if self.mode == \"BOW\":\n","            transformed_texts = []\n","            if self.use_unknown:\n","                unknown = self.vocabulary.get('unknown')\n","\n","            for i, text in enumerate(texts):\n","                bow_text = torch.zeros(len(self.vocabulary))\n","                counter = Counter(text)\n","\n","                if self.use_unknown:\n","                    index = [self.vocabulary.get(key, unknown) for key in counter.keys()]\n","                else:\n","                    index = [self.vocabulary[key] for key in counter.keys() if key in self.vocabulary.keys()]\n","\n","                if self.boolean:\n","                    bow_text[index] = 1\n","                else:\n","                    values = [value.float() for key, value in counter.items() if self.use_unknown or key in self.vocabulary.keys()]\n","                    bow_text[index] = list(values)\n","\n","                transformed_texts.append(bow_text)\n","\n","            # print(transformed_texts)\n","            # print(type(bow_text))\n","            # return transformed_texts\n","            # return torch.cat(transformed_texts,dim=-1).float()\n","            return torch.vstack(transformed_texts).float()\n","            \n","            \n","        if self.mode == \"TFIDF\":\n","\n","            transformed_texts = []\n","\n","            for i, text in enumerate(texts):\n","                bow_text = torch.zeros(len(self.vocabulary))\n","                counter = Counter(text)\n","                print(counter)\n","                index = []\n","                values = []\n","                for key, value in counter.items():\n","                    if key in self.vocabulary.keys():\n","                        index.append(self.vocabulary[key])\n","                        values.append(value)\n","\n","                bow_text[index] = list(values)\n","                print(bow_text)\n","                transformed_texts.append(bow_text * self.idf)\n","\n","            # return transformed_texts\n","            return torch.vstack(transformed_texts).float()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDUmbW2E0-nl","executionInfo":{"status":"ok","timestamp":1630537904443,"user_tz":180,"elapsed":194,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["from torch.utils.data import Dataset\n","\n","class VectorizedDataset(Dataset):\n","    def __init__(self, x, y):\n","        self.x = x\n","        self.y = y\n","\n","    def __len__(self) -> int:\n","        return len(self.x)\n","\n","    def __getitem__(self, idx):\n","        x = self.x[idx]\n","        y = self.y[idx]\n","        return x, y"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"YWbt_cqr1A2K","executionInfo":{"status":"ok","timestamp":1630537909316,"user_tz":180,"elapsed":203,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["from torch.utils.data import DataLoader\n","\n","def create_nn_input(tokenized_text_train, y_train,\n","                    tokenized_text_valid, y_valid,\n","                    tokenized_text_test, y_test,\n","                    vectorizer, batch_size, shuffle):\n","    \n","    vectorized_texts_train = tokenized_text_train\n","    vectorized_texts_valid = tokenized_text_valid\n","    vectorized_texts_test = tokenized_text_test\n","\n","    train_dataset = VectorizedDataset(vectorized_texts_train, torch.Tensor(y_train).reshape(-1,1))\n","    valid_dataset = VectorizedDataset(vectorized_texts_valid, torch.Tensor(y_valid).reshape(-1,1))\n","    test_dataset = VectorizedDataset(vectorized_texts_test, torch.Tensor(y_test).reshape(-1,1))\n","\n","    vectorized_texts_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n","    vectorized_texts_valid = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","    vectorized_texts_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return vectorized_texts_train, vectorized_texts_valid, vectorized_texts_test, vectorizer"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAcxjShy1EfI","executionInfo":{"status":"ok","timestamp":1630537914418,"user_tz":180,"elapsed":189,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["class MLP(torch.nn.Module):\n","    def __init__(self, input_size, hidden_units=128):\n","        super().__init__()\n","        self.dense = torch.nn.Sequential(\n","            torch.nn.Linear(input_size, 128),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(128, 64),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(64, 32),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(32, 1),\n","            torch.nn.Sigmoid()\n","        )\n","    \n","    def forward(self, x):\n","        x = self.dense(x)\n","        return x"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"odWZyAd01FuS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630537917192,"user_tz":180,"elapsed":199,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"43ba2370-15ae-44f9-a475-df45d0361f75"},"source":["if torch.cuda.is_available(): \n","    dev = \"cuda:0\"\n","    print(torch. cuda. get_device_name(dev))\n","else: \n","    dev = \"cpu\" \n","print(dev)\n","device = torch.device(dev)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla K80\n","cuda:0\n"]}]},{"cell_type":"code","metadata":{"id":"Hzsu4XMO1GR-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630537928778,"user_tz":180,"elapsed":9749,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"03d3ae01-f046-4c90-a3ee-92948e23fc56"},"source":["mlp = MLP(1000)\n","mlp.to(device)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLP(\n","  (dense): Sequential(\n","    (0): Linear(in_features=1000, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=64, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=64, out_features=32, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=32, out_features=1, bias=True)\n","    (7): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"juRAcBut1JCV","executionInfo":{"status":"ok","timestamp":1630538101626,"user_tz":180,"elapsed":195,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["def train(model, train, valid, criterion, optimizer,\n","          max_size, filename_save, batch_size=128, n_epochs=10):\n","  \n","    best_valid_loss = 10e9\n","    best_epoch = 0\n","\n","    for i in range(n_epochs):\n","        accumulated_loss = 0\n","        model.train()\n","        for x_train, y_train in train:\n","            x_train = x_train.to(device)\n","            y_train = y_train.to(device)\n","            outputs = model(x_train)\n","            batch_loss = criterion(outputs, y_train)\n","\n","            optimizer.zero_grad()\n","            batch_loss.backward()\n","            optimizer.step()\n","            accumulated_loss += batch_loss.item()\n","\n","        train_loss = accumulated_loss / len(train.dataset)\n","    \n","        # Laço de Validação, um a cada época.\n","        accumulated_loss = 0\n","        accumulated_accuracy = 0\n","        model.eval()\n","        with torch.no_grad():\n","            for x_valid, y_valid in valid:\n","                x_valid = x_valid.to(device)\n","                y_valid = y_valid.to(device)\n","\n","                # predict da rede\n","                outputs = model(x_valid)\n","\n","                # calcula a perda\n","                batch_loss = criterion(outputs, y_valid)\n","                preds = outputs > 0.5\n","\n","                # calcula a acurácia\n","                batch_accuracy = (preds == y_valid).sum()\n","                accumulated_loss += batch_loss\n","                accumulated_accuracy += batch_accuracy\n","\n","        valid_loss = accumulated_loss / len(valid.dataset)\n","        valid_acc = accumulated_accuracy / len(valid.dataset)\n","        print(f'Época: {i:d}/{n_epochs - 1:d} Train Loss: {train_loss:.6f} Valid Loss: {valid_loss:.6f} Valid Acc: {valid_acc:.3f}')\n","\n","        # Salvando o melhor modelo de acordo com a loss de validação\n","        if valid_loss < best_valid_loss:\n","            torch.save(model.state_dict(), filename_save + '.pt')\n","            best_valid_loss = valid_loss\n","            best_epoch = i\n","            print('best model')\n","\n","    return model"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"GOv4ZGBg1JIe","executionInfo":{"status":"ok","timestamp":1630538106036,"user_tz":180,"elapsed":185,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["learningRate = 0.01\n","\n","# Utilizaremos CrossEntropyLoss como função de perda\n","criterion = torch.nn.BCELoss()\n","\n","# Gradiente descendente\n","optimizer = torch.optim.SGD(mlp.parameters(), lr=learningRate)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Y5jwIDR1Mde","executionInfo":{"status":"ok","timestamp":1630538108119,"user_tz":180,"elapsed":5,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["def predict(model, state_dict, test):\n","    accumulated_accuracy = 0\n","    model.load_state_dict(torch.load(state_dict + '.pt'))\n","    model.eval()\n","    with torch.no_grad():\n","        for x_test, y_test in test:\n","            x_test = x_test.to(device)\n","            y_test = y_test.to(device)\n","\n","            # predict da rede\n","            outputs = model(x_test)\n","\n","            # calcula a perda\n","            batch_loss = criterion(outputs, y_test)\n","            preds = outputs > 0.5\n","\n","            # calcula a acurácia\n","            batch_accuracy = (preds == y_test).sum()\n","            accumulated_accuracy += batch_accuracy\n","\n","    test_acc = accumulated_accuracy / len(test.dataset)\n","    test_acc *= 100\n","    print('*' * 40)\n","    print(f'Acurácia de {test_acc:.3f} %')\n","    print('*' * 40)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"-k28fWPM1Nx0","executionInfo":{"status":"ok","timestamp":1630538425107,"user_tz":180,"elapsed":199,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["max_size = 3000\n","batch_size = 64\n","n_epochs = 10\n","learningRate = 0.1\n","save_filename = 'bow_bool'"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdiNUX-L2Z-m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630538427255,"user_tz":180,"elapsed":210,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"2dbb1f33-6d87-4212-a19a-9cca5e747b01"},"source":["mlp_bow_bool = MLP(max_size)\n","mlp_bow_bool.to(device)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLP(\n","  (dense): Sequential(\n","    (0): Linear(in_features=3000, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=64, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=64, out_features=32, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=32, out_features=1, bias=True)\n","    (7): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"MefZQj3C1QHZ","executionInfo":{"status":"ok","timestamp":1630538429399,"user_tz":180,"elapsed":7,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(mlp_bow_bool.parameters(), lr=learningRate)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRvijh-I1RkN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630538474257,"user_tz":180,"elapsed":43592,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"71d6f2eb-3efd-4029-bd53-2ea187173ad6"},"source":["data = text2token(train_texts=x_train, test_texts=x_test, valid_texts=x_valid, boolean=True, max_size = max_size)\n","\n","vectorized_train, vectorized_valid, vectorized_test, _ = create_nn_input(data.train_vector, y_train,\n","                                                                         data.valid_vector, y_valid,\n","                                                                         data.test_vector, y_test,\n","                                                                         data, batch_size, True)\n","\n","print(type(vectorized_train))\n","_ = train(mlp_bow_bool, vectorized_train, vectorized_valid, criterion, optimizer,\n","          max_size, save_filename, batch_size, n_epochs=n_epochs)\n","\n","# class text2token():\n","#     def __init__(self, train_texts, mode = \"BOW\", boolean=False, max_size=None, stopwords = [], use_unknown=False): "],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.utils.data.dataloader.DataLoader'>\n","Época: 0/9 Train Loss: 0.009896 Valid Loss: 0.006156 Valid Acc: 0.840\n","best model\n","Época: 1/9 Train Loss: 0.005693 Valid Loss: 0.004997 Valid Acc: 0.873\n","best model\n","Época: 2/9 Train Loss: 0.004606 Valid Loss: 0.005800 Valid Acc: 0.846\n","Época: 3/9 Train Loss: 0.004001 Valid Loss: 0.005246 Valid Acc: 0.865\n","Época: 4/9 Train Loss: 0.003561 Valid Loss: 0.005150 Valid Acc: 0.860\n","Época: 5/9 Train Loss: 0.003070 Valid Loss: 0.007248 Valid Acc: 0.803\n","Época: 6/9 Train Loss: 0.002504 Valid Loss: 0.005917 Valid Acc: 0.857\n","Época: 7/9 Train Loss: 0.001933 Valid Loss: 0.005839 Valid Acc: 0.866\n","Época: 8/9 Train Loss: 0.001306 Valid Loss: 0.017190 Valid Acc: 0.742\n","Época: 9/9 Train Loss: 0.001286 Valid Loss: 0.006757 Valid Acc: 0.872\n"]}]},{"cell_type":"code","metadata":{"id":"7bOqcqeV2oY_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630538398793,"user_tz":180,"elapsed":775,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"3a9ad483-960b-4e72-807a-e759c419009f"},"source":["predict(mlp_bow_bool, save_filename, vectorized_test)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["****************************************\n","Acurácia de 85.504 %\n","****************************************\n"]}]}]}