{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"authorship_tag":"ABX9TyOPtuaNU9QRjeTXG15lLEBf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"FR_d840mwed2","executionInfo":{"status":"ok","timestamp":1654758912545,"user_tz":180,"elapsed":5390,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}}},"outputs":[],"source":["import collections\n","import itertools\n","import functools\n","import math\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm_notebook\n","\n","import json"]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n","        super(Encoder, self).__init__()\n","\n","        self.embed_size         = embed_size\n","        self.device             = device\n","\n","        self.word_embedding     = nn.Embedding(src_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","        self.layers             = nn.ModuleList([TransformerBlock(embed_size, heads, dropout=dropout, forward_expansion=forward_expansion) for _ in range(num_layers)])\n","        self.dropout            = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","\n","        N, seq_length = x.shape\n","        positions     = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        out           = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n","\n","        # In the Encoder the query, key, value are all the same, it's in the\n","        # decoder this will change. This might look a bit odd in this case.\n","        for layer in self.layers:\n","            out = layer(out, out, out, mask)\n","\n","        return out"],"metadata":{"id":"KG5-Ka13wo2v","executionInfo":{"status":"ok","timestamp":1654750526911,"user_tz":180,"elapsed":261,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads\n","\n","        assert (\n","            self.head_dim * heads == embed_size\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n","\n","    def forward(self, values, keys, query, mask):\n","        # Get number of training examples\n","        N = query.shape[0]\n","\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","        print(value_len, key_len, query_len)\n","\n","        '''print('V0', values.shape)\n","        print('K0', keys.shape)\n","        print('Q0', query.shape)'''\n","\n","        # Split the embedding into self.heads different pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        query = query.reshape(N, query_len, self.heads, self.head_dim)\n","        '''print('V1', values.shape)\n","        print('K1', keys.shape)\n","        print('Q1', query.shape)'''\n","\n","        values = self.values(values)  # (N, value_len, heads, head_dim)\n","        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n","        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n","\n","        '''print('V2', values.shape)\n","        print('K2', keys.shape)\n","        print('Q2', query.shape)'''\n","\n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        # queries shape: (N, query_len, heads, heads_dim),\n","        # keys shape: (N, key_len, heads, heads_dim)\n","        # energy: (N, heads, query_len, key_len)\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability\n","        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n","        # attention shape: (N, heads, query_len, key_len)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","        # attention shape: (N, heads, query_len, key_len)\n","        # values shape: (N, value_len, heads, heads_dim)\n","        # out after matrix multiply: (N, query_len, heads, head_dim), then\n","        # we reshape and flatten the last two dimensions.\n","\n","        out = self.fc_out(out)\n","        # Linear layer doesn't modify the shape, final shape will be\n","        # (N, query_len, embed_size)\n","\n","        return out"],"metadata":{"id":"crd3_xQ7wj4o","executionInfo":{"status":"ok","timestamp":1654758766576,"user_tz":180,"elapsed":271,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = SelfAttention(embed_size, heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, forward_expansion * embed_size),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_size, embed_size),\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, value, key, query, mask):\n","        attention = self.attention(value, key, query, mask)\n","\n","        # Add skip connection, run through normalization and finally dropout\n","        x = self.dropout(self.norm1(attention + query))\n","        forward = self.feed_forward(x)\n","        out = self.dropout(self.norm2(forward + x))\n","        return out"],"metadata":{"id":"OcndU74RwmZR","executionInfo":{"status":"ok","timestamp":1654750525195,"user_tz":180,"elapsed":247,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class DecoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n","        super(DecoderBlock, self).__init__()\n","\n","        self.norm              = nn.LayerNorm(embed_size)\n","        self.attention         = SelfAttention(embed_size, heads=heads)\n","        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n","        self.dropout           = nn.Dropout(dropout)\n","\n","    def forward(self, x, value, key, src_mask, trg_mask):\n","\n","        attention = self.attention(x, x, x, trg_mask)\n","        print(\"decAtt\", attention.shape)\n","        query     = self.dropout(self.norm(attention + x))\n","        print(\"decQue\", query.shape)\n","        out       = self.transformer_block(value, key, query, src_mask)\n","        print(\"decOut\", out.shape)\n","\n","        return out"],"metadata":{"id":"I7VTMLECwrBh","executionInfo":{"status":"ok","timestamp":1654753880488,"user_tz":180,"elapsed":236,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length):\n","        super(Decoder, self).__init__()\n","        self.device = device\n","\n","        self.word_embedding     = nn.Embedding(trg_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","\n","        self.layers  = nn.ModuleList([DecoderBlock(embed_size, heads, forward_expansion, dropout, device) for _ in range(num_layers)])\n","        self.fc_out  = nn.Linear(embed_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_out, src_mask, trg_mask):\n","        print('xx', x.shape)\n","        N, seq_length = x.shape\n","        positions     = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        print('posi', positions.shape)\n","        x             = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n","        print('xx', x.shape)\n","\n","        for layer in self.layers:\n","            print('Newblock')\n","            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n","\n","        out = self.fc_out(x)\n","\n","        return out"],"metadata":{"id":"fnFrSKp0wtq5","executionInfo":{"status":"ok","timestamp":1654756943600,"user_tz":180,"elapsed":6,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=512,\n","                 num_layers=2, forward_expansion=2, heads=2, dropout=0, device=\"cpu\", max_length=9):\n","\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n","        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n","\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device      = device\n","\n","    def make_src_mask(self, src):\n","      \n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        # (N, 1, 1, src_len)\n","        return src_mask.to(self.device)\n","\n","    def make_trg_mask(self, trg):\n","\n","        N, trg_len = trg.shape\n","        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len)\n","\n","        return trg_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","\n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        # print('x', src.shape)\n","\n","        enc_src  = self.encoder(src, src_mask)\n","        # print('encoder', enc_src.shape)\n","\n","        print('===== DECODER =====')\n","        out      = self.decoder(trg, enc_src, src_mask, trg_mask)\n","        print('decoder', out.shape)\n","\n","        return out"],"metadata":{"id":"bLRRhE0Tww2_","executionInfo":{"status":"ok","timestamp":1654753913022,"user_tz":180,"elapsed":249,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","model = LanguageModel(vocab_size=10, max_seq_length=9, dim=512, n_layers=2, pad_token_id=0).to(device)\n","\n","x   = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n","trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n","\n","out = model(x)\n","print(out.shape) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zEoGIGzo2db2","executionInfo":{"status":"ok","timestamp":1654758772805,"user_tz":180,"elapsed":275,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"027eb558-8c19-46ac-f249-5d475021bdcb"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n","9 9 9\n","9 9 9\n","torch.Size([2, 9, 10])\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","x   = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n","trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n","\n","src_pad_idx    = 0\n","trg_pad_idx    = 0\n","src_vocab_size = 10\n","trg_vocab_size = 10\n","\n","print(trg[:, :-1])\n","\n","model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n","out = model(x, trg[:, :-1])\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tg8JGw9Yxbkd","executionInfo":{"status":"ok","timestamp":1654756949839,"user_tz":180,"elapsed":250,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"8530190e-3321-4dec-f7f7-8e35e8273b15"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n","tensor([[1, 7, 4, 3, 5, 9, 2],\n","        [1, 5, 6, 2, 4, 7, 6]])\n","9 9 9\n","9 9 9\n","===== DECODER =====\n","xx torch.Size([2, 7])\n","posi torch.Size([2, 7])\n","xx torch.Size([2, 7, 512])\n","Newblock\n","7 7 7\n","decAtt torch.Size([2, 7, 512])\n","decQue torch.Size([2, 7, 512])\n","9 9 7\n","decOut torch.Size([2, 7, 512])\n","Newblock\n","7 7 7\n","decAtt torch.Size([2, 7, 512])\n","decQue torch.Size([2, 7, 512])\n","9 9 7\n","decOut torch.Size([2, 7, 512])\n","decoder torch.Size([2, 7, 10])\n","torch.Size([2, 7, 10])\n"]}]},{"cell_type":"code","source":["def make_src_mask(src):\n","    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n","        # (N, 1, 1, src_len)\n","    return src_mask\n","\n","make_src_mask(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFp3HdJmyR7r","executionInfo":{"status":"ok","timestamp":1654750045915,"user_tz":180,"elapsed":256,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"09051ff8-ac0e-416b-a093-908a4b0e6208"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[ True,  True,  True,  True,  True,  True,  True,  True, False]]],\n","\n","\n","        [[[ True,  True,  True,  True,  True,  True,  True,  True,  True]]]])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["def make_src_mask(src):\n","    src_mask = (src != 0)\n","        # (N, 1, 1, src_len)\n","    return src_mask[:, None, None, :]\n","\n","make_src_mask(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ftXpBx2ygzv","executionInfo":{"status":"ok","timestamp":1654750077749,"user_tz":180,"elapsed":255,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"5f15dfd7-3ec2-4956-fa27-da6f6a26d30f"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[ True,  True,  True,  True,  True,  True,  True,  True, False]]],\n","\n","\n","        [[[ True,  True,  True,  True,  True,  True,  True,  True,  True]]]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["# Minha modificação"],"metadata":{"id":"-gEeJ0Zc0oyd"}},{"cell_type":"code","source":["class LanguageModel(torch.nn.Module):\n","\n","    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int):\n","        \"\"\"\n","        Implements the Self-attention, decoder-only.\"\n","\n","        Args:\n","            vocab_size (int): Size of the input vocabulary.\n","            max_seq_length (int): Size of the sequence to consider as context for prediction.\n","            dim (int): Dimension of the embedding layer for each word in the context.\n","            n_layers (int): number of self-attention layers.\n","            pad_token_id (int): id of the pad token that will be ignored in the attention.\n","        \"\"\"\n","        # Escreva seu código aqui.\n","\n","        super().__init__()\n","\n","        self.vocab_size     = vocab_size\n","        self.max_seq_length = max_seq_length\n","        self.dim            = dim\n","        self.n_layers       = n_layers\n","        self.pad_token_id   = pad_token_id\n","        self.n_heads        = 2\n","\n","        self.embedding_layer       = nn.Embedding(vocab_size,     dim, padding_idx=pad_token_id)\n","        self.positional_embeddings = nn.Embedding(max_seq_length, dim, padding_idx=pad_token_id)\n","\n","        self.attention1  = SelfAttention(self.dim, self.n_heads)\n","        self.attention2  = SelfAttention(self.dim, self.n_heads)\n","        \n","        self.linear1 = nn.Linear(self.dim, self.vocab_size, bias=False)\n","        self.dropout = nn.Dropout(p=0.2)\n","\n","        self.norm0 = nn.LayerNorm(dim)\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.norm2 = nn.LayerNorm(dim)\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(dim, 2*dim),\n","            nn.ReLU(),\n","            nn.Linear(2*dim, dim),\n","        )\n","\n","    def triMask(self, inputs):\n","\n","        N, tri_len = inputs.shape\n","        return torch.tril(torch.ones((tri_len, tri_len))).expand(N, 1, tri_len, tri_len)\n","\n","    def forward(self, inputs):\n","        \"\"\"\n","        Args:\n","            inputs is a LongTensor of shape (batch_size, max_seq_length)\n","            \n","        Returns:\n","            logits of shape (batch_size, vocab_size)\n","        \"\"\"\n","        # Escreva seu código aqui.\n","\n","        mask = inputs != self.pad_token_id\n","        j = self.triMask(inputs)\n","\n","        x = self.dropout(self.embedding_layer(inputs) + self.positional_embeddings.weight) # B, L, D\n","        a = self.attention1(x, x, x, j)\n","        q = self.dropout(self.norm0(a + x))\n","\n","        a = self.attention2(x, x, q, mask[:, None, None, :])\n","\n","        x = self.dropout(self.norm1(a + q))\n","        forward = self.feed_forward(x)\n","        x = self.dropout(self.norm2(forward + x))\n","\n","\n","        o = self.linear1(x)\n","                  \n","        return o"],"metadata":{"id":"xgrH5RjP2Zcd","executionInfo":{"status":"ok","timestamp":1654758548868,"user_tz":180,"elapsed":249,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["class SelfAttentionmm(nn.Module):\n","    def __init__(self, dim, max_seq_length, n_heads, pad_token_id):\n","        super(SelfAttentionmm, self).__init__()\n","\n","        self.pad_token_id   = pad_token_id\n","        self.max_seq_length = max_seq_length\n","\n","        self.n_heads        = n_heads\n","        self.dim            = dim\n","        self.D_k            = dim//n_heads\n","        \n","        self.W_q = torch.nn.Linear(self.dim, self.dim, bias=False) # D, D\n","        self.W_k = torch.nn.Linear(self.dim, self.dim, bias=False) # D, D\n","        self.W_v = torch.nn.Linear(self.dim, self.dim, bias=False) # D, D\n","        self.W_o = torch.nn.Linear(self.dim, self.dim, bias=False) # D, D\n","\n","        self.feed_forward = torch.nn.Sequential(torch.nn.LayerNorm(self.dim, eps=1e-6),\n","                                                nn.Linear(self.dim, self.dim),\n","                                                torch.nn.ReLU(),\n","                                                nn.Linear(self.dim, self.dim),\n","                                                nn.Dropout(p=0.2),\n","                                                torch.nn.LayerNorm(self.dim, eps=1e-6))\n","\n","\n","    def attention(self, Q, K, V, mask):   \n","\n","        '''\n","        1 torch.Size([5, 2, 9, 9])\n","        2 torch.Size([5, 9])\n","        3 torch.Size([5, 2, 9, 9])\n","        4 torch.Size([5, 2, 9, 9])\n","        5 torch.Size([5, 2, 9, 128])\n","        6 torch.Size([5, 9, 2, 128])\n","        7 torch.Size([5, 9, 256])\n","        8 torch.Size([5, 9, 256])\n","        '''\n","        \n","        scores = torch.matmul(Q, K.transpose(-1, -2))/math.sqrt(self.D_k) # B, HEADS, L, L -> 1\n","        # print(scores)\n","        mask_expanded = mask.expand_as(scores)                        # B, HEADS, L, L -> 3\n","        \n","        scores.masked_fill_(~mask_expanded, float('-inf'))                # B, HEADS, L, L\n","        # print(scores)\n","        probs = F.softmax(scores, dim=-1)                                 # B, HEADS, L, L -> 4\n","\n","        E = torch.matmul(probs, V)                                        # B, HEADS, L, D//HEADS -> 5\n","        E = E.transpose(1,2).contiguous()                                 # B, L, HEADS, D//HEADS -> 6\n","        E = E.reshape(mask.shape[0], self.max_seq_length, self.dim)       # B, L, D -> 7\n","        E = self.W_o(E)                                                   # B, L, D -> 8\n","\n","        return E\n","        \n","    def forward(self, x, inputs, tri_mask):\n","\n","        mask = inputs != self.pad_token_id\n","\n","        print('xSelf', x.shape) # B, L, D\n","\n","        q = self.W_q(x).reshape(mask.shape[0], self.max_seq_length, self.n_heads, self.D_k).transpose(1,2) # B, HEADS, L, D//HEADS -> torch.Size([5, 2, 9, 128])\n","        k = self.W_k(x).reshape(mask.shape[0], self.max_seq_length, self.n_heads, self.D_k).transpose(1,2) # B, HEADS, L, D//HEADS\n","        v = self.W_v(x).reshape(mask.shape[0], self.max_seq_length, self.n_heads, self.D_k).transpose(1,2) # B, HEADS, L, D//HEADS\n","\n","        y = self.attention(q, k, v, mask, tri_mask)   # B, L, D\n","        y = self.W_o(x)                               # B, L, D\n","\n","\n","        # print('x', x.shape)\n","        # print('inputs', inputs.shape)\n","        X = x + y\n","        \n","\n","        return self.feed_forward(X)"],"metadata":{"id":"1v6KQRiK0rS6","executionInfo":{"status":"ok","timestamp":1654752492600,"user_tz":180,"elapsed":371,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["src_pad_idx    = 0\n","trg_pad_idx    = 0\n","src_vocab_size = 10\n","trg_vocab_size = 10\n","\n","print(trg[:, :-1])\n","\n","model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n","out = model(x, trg[:, :-1])\n","print(out.shape)"],"metadata":{"id":"WjWMv6252nkr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Attention"],"metadata":{"id":"a--oba3d4aIU"}},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(SelfAttention, self).__init__()\n","\n","        self.embed_size = embed_size\n","        self.heads      = heads\n","        self.head_dim   = embed_size // heads\n","\n","        assert (\n","            self.head_dim * heads == embed_size\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n","\n","    def forward(self, values, keys, query, mask):\n","        # Get number of training examples\n","        N = query.shape[0]\n","\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","        print(value_len, key_len, query_len)\n","\n","        # Split the embedding into self.heads different pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        query = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        values = self.values(values)  # (N, value_len, heads, head_dim)\n","        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n","        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n","\n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        # queries shape: (N, query_len, heads, heads_dim),\n","        # keys shape: (N, key_len, heads, heads_dim)\n","        # energy: (N, heads, query_len, key_len)\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability\n","        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n","        # attention shape: (N, heads, query_len, key_len)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","        # attention shape: (N, heads, query_len, key_len)\n","        # values shape: (N, value_len, heads, heads_dim)\n","        # out after matrix multiply: (N, query_len, heads, head_dim), then\n","        # we reshape and flatten the last two dimensions.\n","\n","        out = self.fc_out(out)\n","        # Linear layer doesn't modify the shape, final shape will be\n","        # (N, query_len, embed_size)\n","\n","        return out"],"metadata":{"id":"1KlRYBtf4Zci","executionInfo":{"status":"ok","timestamp":1654753626730,"user_tz":180,"elapsed":246,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["a = torch.rand((2, 9))\n","m = torch.rand((2, 9, 512))\n","\n","teste = SelfAttention(embed_size=512, heads=2)\n","teste(m, m, m, make_src_mask(a))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFcz1mrV-1cx","executionInfo":{"status":"ok","timestamp":1654753627808,"user_tz":180,"elapsed":9,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"c2af41db-b68b-4505-affc-3abbf108ff46"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["9 9 9\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.3335, -0.0593, -0.0261,  ...,  0.1508, -0.2988, -0.0069],\n","         [-0.3329, -0.0577, -0.0257,  ...,  0.1506, -0.2985, -0.0077],\n","         [-0.3319, -0.0588, -0.0263,  ...,  0.1503, -0.2989, -0.0079],\n","         ...,\n","         [-0.3331, -0.0596, -0.0260,  ...,  0.1512, -0.2997, -0.0085],\n","         [-0.3335, -0.0592, -0.0259,  ...,  0.1506, -0.2991, -0.0082],\n","         [-0.3335, -0.0578, -0.0255,  ...,  0.1506, -0.2982, -0.0072]],\n","\n","        [[-0.2888, -0.1058,  0.1194,  ...,  0.0919, -0.2774, -0.0424],\n","         [-0.2871, -0.1064,  0.1195,  ...,  0.0916, -0.2778, -0.0417],\n","         [-0.2875, -0.1054,  0.1196,  ...,  0.0910, -0.2775, -0.0435],\n","         ...,\n","         [-0.2874, -0.1060,  0.1191,  ...,  0.0918, -0.2779, -0.0422],\n","         [-0.2868, -0.1061,  0.1193,  ...,  0.0912, -0.2775, -0.0421],\n","         [-0.2877, -0.1058,  0.1193,  ...,  0.0915, -0.2777, -0.0426]]],\n","       grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["m.reshape(2, 9, 2, 256)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEpJSSdGAV9x","executionInfo":{"status":"ok","timestamp":1654753726477,"user_tz":180,"elapsed":232,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"2d84759b-da34-4187-d49d-1389fcd90009"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[0.0033, 0.9380, 0.4886,  ..., 0.5543, 0.9401, 0.5505],\n","          [0.2118, 0.9130, 0.8602,  ..., 0.4050, 0.2701, 0.2778]],\n","\n","         [[0.0262, 0.5792, 0.0016,  ..., 0.1232, 0.8835, 0.2023],\n","          [0.3997, 0.9968, 0.8600,  ..., 0.4489, 0.6798, 0.3016]],\n","\n","         [[0.1478, 0.5773, 0.1737,  ..., 0.6511, 0.0846, 0.4102],\n","          [0.4746, 0.3968, 0.5910,  ..., 0.8221, 0.6617, 0.5427]],\n","\n","         ...,\n","\n","         [[0.5062, 0.2764, 0.7213,  ..., 0.3328, 0.1283, 0.8582],\n","          [0.0637, 0.4163, 0.8025,  ..., 0.2134, 0.7268, 0.8714]],\n","\n","         [[0.2920, 0.7578, 0.7532,  ..., 0.7742, 0.6510, 0.9469],\n","          [0.4326, 0.7127, 0.4063,  ..., 0.7641, 0.9083, 0.4737]],\n","\n","         [[0.1933, 0.0821, 0.7895,  ..., 0.3318, 0.9747, 0.5574],\n","          [0.8157, 0.3432, 0.1347,  ..., 0.2997, 0.3680, 0.3851]]],\n","\n","\n","        [[[0.9211, 0.0728, 0.6304,  ..., 0.3999, 0.7939, 0.5588],\n","          [0.6313, 0.2807, 0.7249,  ..., 0.8839, 0.0532, 0.8162]],\n","\n","         [[0.1017, 0.1403, 0.0193,  ..., 0.7317, 0.9593, 0.8123],\n","          [0.4284, 0.4843, 0.6779,  ..., 0.2512, 0.6076, 0.1931]],\n","\n","         [[0.1817, 0.8473, 0.7330,  ..., 0.9162, 0.6700, 0.9484],\n","          [0.4280, 0.3525, 0.6475,  ..., 0.0927, 0.4518, 0.0950]],\n","\n","         ...,\n","\n","         [[0.8129, 0.7432, 0.9182,  ..., 0.6777, 0.8187, 0.3936],\n","          [0.8944, 0.4754, 0.7667,  ..., 0.6949, 0.2454, 0.6513]],\n","\n","         [[0.3308, 0.0428, 0.5242,  ..., 0.3122, 0.5908, 0.2739],\n","          [0.7650, 0.3325, 0.4367,  ..., 0.4248, 0.4626, 0.0159]],\n","\n","         [[0.5916, 0.8194, 0.8202,  ..., 0.8283, 0.7486, 0.1566],\n","          [0.7829, 0.4091, 0.4000,  ..., 0.3211, 0.2056, 0.2754]]]])"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["class SelfAttentionmm(nn.Module):\n","    def __init__(self, dim, max_seq_length, n_heads, pad_token_id):\n","        super(SelfAttentionmm, self).__init__()\n","\n","        self.pad_token_id   = pad_token_id\n","        self.max_seq_length = max_seq_length\n","\n","        self.n_heads        = n_heads\n","        self.dim            = dim\n","        self.D_k            = dim//n_heads\n","        \n","        self.W_q = torch.nn.Linear(self.dim, self.dim, bias=False) # D, D\n","        self.W_k = torch.nn.Linear(self.dim, self.dim, bias=False) # D, D\n","        self.W_v = torch.nn.Linear(self.dim, self.dim, bias=False) # D, D\n","        self.W_o = torch.nn.Linear(self.dim, self.dim, bias=False) # D, D\n","\n","        self.feed_forward = torch.nn.Sequential(torch.nn.LayerNorm(self.dim, eps=1e-6),\n","                                                nn.Linear(self.dim, self.dim),\n","                                                torch.nn.ReLU(),\n","                                                nn.Linear(self.dim, self.dim),\n","                                                nn.Dropout(p=0.2),\n","                                                torch.nn.LayerNorm(self.dim, eps=1e-6))\n","\n","\n","    def attention(self, Q, K, V, mask, tri_mask):   \n","\n","        '''\n","        1 torch.Size([5, 2, 9, 9])\n","        2 torch.Size([5, 9])\n","        3 torch.Size([5, 2, 9, 9])\n","        4 torch.Size([5, 2, 9, 9])\n","        5 torch.Size([5, 2, 9, 128])\n","        6 torch.Size([5, 9, 2, 128])\n","        7 torch.Size([5, 9, 256])\n","        8 torch.Size([5, 9, 256])\n","        '''\n","        \n","        scores = torch.matmul(Q, K.transpose(-1, -2))/math.sqrt(self.D_k) # B, HEADS, L, L -> 1\n","        # print(scores)\n","        new_mask      = mask[:, None, None, :] & tri_mask\n","        mask_expanded = new_mask.expand_as(scores)                        # B, HEADS, L, L -> 3\n","        \n","        scores.masked_fill_(~mask_expanded, float('-inf'))                # B, HEADS, L, L\n","        # print(scores)\n","        probs = F.softmax(scores, dim=-1)                                 # B, HEADS, L, L -> 4\n","\n","        E = torch.matmul(probs, V)                                        # B, HEADS, L, D//HEADS -> 5\n","        E = E.transpose(1,2).contiguous()                                 # B, L, HEADS, D//HEADS -> 6\n","        E = E.reshape(mask.shape[0], self.max_seq_length, self.dim)       # B, L, D -> 7\n","        E = self.W_o(E)                                                   # B, L, D -> 8\n","\n","        return E\n","        \n","    def forward(self, x, inputs, tri_mask):\n","\n","        mask = inputs != self.pad_token_id\n","\n","        q = self.W_q(x).reshape(mask.shape[0], self.max_seq_length, self.n_heads, self.D_k).transpose(1,2) # B, HEADS, L, D//HEADS -> torch.Size([5, 2, 9, 128])\n","        k = self.W_k(x).reshape(mask.shape[0], self.max_seq_length, self.n_heads, self.D_k).transpose(1,2) # B, HEADS, L, D//HEADS\n","        v = self.W_v(x).reshape(mask.shape[0], self.max_seq_length, self.n_heads, self.D_k).transpose(1,2) # B, HEADS, L, D//HEADS\n","\n","        y = self.attention(q, k, v, mask, tri_mask)   # B, L, D\n","        y = self.W_o(x)                               # B, L, D\n","\n","\n","        # print('x', x.shape)\n","        # print('inputs', inputs.shape)\n","        X = x + y\n","        \n","\n","        return self.feed_forward(X)"],"metadata":{"id":"c7W83qLu4gAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","    def __init__(self, dim, heads):\n","        super(SelfAttention, self).__init__()\n","\n","        self.dim        = dim\n","        self.heads      = heads\n","        self.head_dim   = dim // heads\n","\n","        assert (self.head_dim * heads == dim), \"Embedding size needs to be divisible by heads\"\n","\n","        self.Wv = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.Wk = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.Wq = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc = nn.Linear(dim, dim)\n","\n","    def forward(self, values, keys, query, mask):\n","        N = query.shape[0]\n","\n","        v_len, k_len, q_len = values.shape[1], keys.shape[1], query.shape[1]\n","        print(v_len, k_len, q_len)\n","\n","        # Split the embedding into self.heads different pieces\n","        values = values.reshape(N, v_len, self.heads, self.head_dim)\n","        keys   = keys.reshape(N, k_len, self.heads, self.head_dim)\n","        query  = query.reshape(N, q_len, self.heads, self.head_dim)\n","\n","        values  = self.Wv(values)  # (N, value_len, heads, head_dim)\n","        keys    = self.Wk(keys)    # (N, key_len, heads, head_dim)\n","        queries = self.Wq(query)   # (N, query_len, heads, heads_dim)\n","\n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        # queries shape: (N, query_len, heads, heads_dim),\n","        # keys shape: (N, key_len, heads, heads_dim)\n","        # energy: (N, heads, query_len, key_len)\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability\n","        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n","        # attention shape: (N, heads, query_len, key_len)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","        # attention shape: (N, heads, query_len, key_len)\n","        # values shape: (N, value_len, heads, heads_dim)\n","        # out after matrix multiply: (N, query_len, heads, head_dim), then\n","        # we reshape and flatten the last two dimensions.\n","\n","        out = self.fc_out(out)\n","        # Linear layer doesn't modify the shape, final shape will be\n","        # (N, query_len, embed_size)\n","\n","        return out"],"metadata":{"id":"8c442DO46YHs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_embedding     = nn.Embedding(10, 2)\n","\n","k =  torch.ones((2,7), dtype=torch.long)\n","\n","word_embedding(k)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHGTXdfUFw7D","executionInfo":{"status":"ok","timestamp":1654755205656,"user_tz":180,"elapsed":8,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"73d37226-eba4-4ced-ed95-3c9c3a0d9fdd"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.2938, -0.6883],\n","         [-0.2938, -0.6883],\n","         [-0.2938, -0.6883],\n","         [-0.2938, -0.6883],\n","         [-0.2938, -0.6883],\n","         [-0.2938, -0.6883],\n","         [-0.2938, -0.6883]],\n","\n","        [[-0.2938, -0.6883],\n","         [-0.2938, -0.6883],\n","         [-0.2938, -0.6883],\n","         [-0.2938, -0.6883],\n","         [-0.2938, -0.6883],\n","         [-0.2938, -0.6883],\n","         [-0.2938, -0.6883]]], grad_fn=<EmbeddingBackward0>)"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["word_embedding     = nn.Linear(10, 2)\n","\n","k =  torch.ones((2,7), dtype=torch.long)\n","\n","word_embedding(k)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"id":"23X-ynl0F3Cj","executionInfo":{"status":"error","timestamp":1654755494501,"user_tz":180,"elapsed":290,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"98c4bc12-0287-4a55-8535-c34cb6036c08"},"execution_count":71,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-71-e3d66eec46da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mword_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x7 and 10x2)"]}]}]}