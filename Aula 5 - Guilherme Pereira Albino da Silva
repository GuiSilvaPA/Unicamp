{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aula 5 - Guilherme Pereira Albino da Silva","provenance":[{"file_id":"1YW4O0K7EfSsgUe1kaZR9NswLXKVwqCb-","timestamp":1631737455573},{"file_id":"1Y3rRUiQGW5CEcPRkx_sfZGAEjNwVsw-b","timestamp":1631184728325},{"file_id":"1ONeS-lZ3vVqThueoTvQRMnZ_rJJB1yOl","timestamp":1629906878859}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1OG5DT_dm6mk"},"source":["# Notebook de referência \n","\n","Nome: Guilherme Pereira Albino da Silva - Ra: 182786"]},{"cell_type":"markdown","metadata":{"id":"uhpAkifICdJo"},"source":["# Fixando a seed"]},{"cell_type":"code","metadata":{"id":"1ozXD-xYCcrT","executionInfo":{"status":"ok","timestamp":1631753031687,"user_tz":180,"elapsed":4610,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}}},"source":["import random\n","import torch\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHeZ9nAOEB0U","executionInfo":{"status":"ok","timestamp":1631753031692,"user_tz":180,"elapsed":16,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"996c30ea-5f95-4997-f8c5-87e17c3bf220"},"source":["random.seed(123)\n","np.random.seed(123)\n","torch.manual_seed(123)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f1a42ff3b30>"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"CXFdJz2KVeQw"},"source":["## Preparando Dados"]},{"cell_type":"markdown","metadata":{"id":"gHMi_Kq65fPM"},"source":["Primeiro, fazemos download do dataset:"]},{"cell_type":"code","metadata":{"id":"2wbnfzst5O3k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631753053345,"user_tz":180,"elapsed":21662,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"a7f6d19f-1d33-4ebf-c60b-479424af643b"},"source":["!wget -nc http://files.fast.ai/data/aclImdb.tgz \n","!tar -xzf aclImdb.tgz"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-09-16 00:43:51--  http://files.fast.ai/data/aclImdb.tgz\n","Resolving files.fast.ai (files.fast.ai)... 104.26.2.19, 172.67.69.159, 104.26.3.19, ...\n","Connecting to files.fast.ai (files.fast.ai)|104.26.2.19|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://files.fast.ai/data/aclImdb.tgz [following]\n","--2021-09-16 00:43:51--  https://files.fast.ai/data/aclImdb.tgz\n","Connecting to files.fast.ai (files.fast.ai)|104.26.2.19|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 145982645 (139M) [application/x-gtar-compressed]\n","Saving to: ‘aclImdb.tgz’\n","\n","aclImdb.tgz         100%[===================>] 139.22M  40.4MB/s    in 3.7s    \n","\n","2021-09-16 00:43:55 (37.7 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"0Giyi5Rv_NIm"},"source":["## Carregando o dataset\n","\n","Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n","\n","Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."]},{"cell_type":"code","metadata":{"id":"0HIN_xLI_TuT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631753055275,"user_tz":180,"elapsed":1946,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"9241dcad-8f82-4fa5-9847-0adb6500c938"},"source":["import os\n","import random\n","\n","\n","def load_texts(folder):\n","    texts = []\n","    for path in os.listdir(folder):\n","        with open(os.path.join(folder, path)) as f:\n","            texts.append(f.read())\n","    return texts\n","\n","x_train_pos = load_texts('aclImdb/train/pos')\n","x_train_neg = load_texts('aclImdb/train/neg')\n","x_test_pos = load_texts('aclImdb/test/pos')\n","x_test_neg = load_texts('aclImdb/test/neg')\n","\n","x_train = x_train_pos + x_train_neg\n","x_test = x_test_pos + x_test_neg\n","y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n","y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n","\n","# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n","c = list(zip(x_train, y_train))\n","random.shuffle(c)\n","x_train, y_train = zip(*c)\n","\n","n_train = int(0.8 * len(x_train))\n","\n","x_valid = x_train[n_train:]\n","y_valid = y_train[n_train:]\n","x_train = x_train[:n_train]\n","y_train = y_train[:n_train]\n","\n","print(len(x_train), 'amostras de treino.')\n","print(len(x_valid), 'amostras de desenvolvimento.')\n","print(len(x_test), 'amostras de teste.')\n","\n","print('3 primeiras amostras treino:')\n","for x, y in zip(x_train[:3], y_train[:3]):\n","    print(y, x[:100])\n","\n","print('3 últimas amostras treino:')\n","for x, y in zip(x_train[-3:], y_train[-3:]):\n","    print(y, x[:100])\n","\n","print('3 primeiras amostras validação:')\n","for x, y in zip(x_valid[:3], y_test[:3]):\n","    print(y, x[:100])\n","\n","print('3 últimas amostras validação:')\n","for x, y in zip(x_valid[-3:], y_valid[-3:]):\n","    print(y, x[:100])"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["20000 amostras de treino.\n","5000 amostras de desenvolvimento.\n","25000 amostras de teste.\n","3 primeiras amostras treino:\n","False I don't think any player in Hollywood history lasted as long as David Niven did given most of the we\n","False Although Twenty Minutes of Love is a harmless attempt at an early comedy, it was difficult to follow\n","True I watched this movie every chance I got, back in the Seventies when it came out on cable. It was my \n","3 últimas amostras treino:\n","False If you have plenty of time to waste ... it's OK. It moves at a good pace but to pull this movie off \n","True This movie is a remake of two movies that were a lot better. The last one, Heaven Can Wait, was grea\n","True basically, i like Verhoeven film because in his film, i enjoy a brilliant pscychosexual story that i\n","3 primeiras amostras validação:\n","True Yeah, I remember this one! Many years since I actually watched it. The story was entirely surreal, b\n","True In the 1930s studios would use short films like this one sort of as testing grounds for new actors, \n","True After watching the first 20mn of Blanche(sorry I couldn't take more of it), I have now confirmed she\n","3 últimas amostras validação:\n","True In THE BARBARIAN AND THE GEISHA, John Wayne plays Townsend Harris, a real envoy from the United Stat\n","True I saw this movie, and the play, and I have to add that this was the most touching story that I had e\n","True This \"tragicomedy\" written by famous Serbian theatre/film writer Dusan Kovacevic is probably one of \n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiLnZh8fKXvm","executionInfo":{"status":"ok","timestamp":1631753055520,"user_tz":180,"elapsed":256,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"5ed51942-afab-4976-9dc8-728deee0e1a0"},"source":["sum([len(item.split()) for item in x_train])"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4671194"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"g7siSe3QJHFM","executionInfo":{"status":"ok","timestamp":1631753056492,"user_tz":180,"elapsed":772,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}}},"source":["import os\n","import random\n","\n","import numpy as np\n","import torch\n","\n","from re import findall, sub\n","import string\n","\n","from bs4 import BeautifulSoup\n","\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","\n","from collections import *\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","def reset_random_seeds():\n","    random.seed(123)\n","    np.random.seed(123)\n","    torch.manual_seed(123)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJm9jQk6IlJx","executionInfo":{"status":"ok","timestamp":1631753056493,"user_tz":180,"elapsed":7,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}}},"source":["class text2token():\n","    \n","    def __init__(self, train_texts, \n","                 test_texts=None, valid_texts=None,\n","                 max_size=None, stopwords = [],\n","                 remove_number=None,\n","                 max_length=None, ignore_pad=True):\n","        \n","        \n","        self.max_length = max_length\n","        self.remove_number = remove_number\n","        self.max_size = max_size\n","        self.ignore_pad = ignore_pad\n","        self.stopwords = stopwords\n","                \n","        \n","        \n","        self.textos_de_treino = self.split(train_texts)\n","        self.model(self.textos_de_treino)\n","        \n","        self.bow_train = self.bow(self.textos_de_treino)\n","        self.idx_train = self.sentence_to_id(self.textos_de_treino)\n","        \n","        if test_texts:        \n","            self.textos_de_teste = self.split(test_texts)\n","            self.bow_test = self.bow(self.textos_de_teste)\n","            self.idx_test = self.sentence_to_id(self.textos_de_teste)\n","            \n","        if valid_texts:  \n","            self.textos_de_validacao = self.split(valid_texts)\n","            self.bow_valid = self.bow(self.textos_de_validacao)\n","            self.idx_valid = self.sentence_to_id(self.textos_de_validacao)\n","        \n","    \n","    \n","# SPLIT =================================================================================================================     \n","\n","    def split(self, texts):\n","        \n","        tokenized_texts = []\n","\n","        for text in texts:\n","            \n","            text = text.lower()\n","            \n","            # Remove as tags HTLM\n","            text = BeautifulSoup(text, \"lxml\").text\n","            \n","            # Remove os caracteres especiais\n","            text = sub('[^A-Za-z\\s]+', ' ', text)\n","\n","            # Remove os números\n","            text = sub(r'[0-9+]', ' ', text)\n","            \n","            # Remove alguns pontos e separa\n","            # text_splited = findall(r'\\w+|[^?\\-!.,:;\"\\'/><\\s]', text)\n","            text_splited = findall(r\"[\\w']+|[!#$%&()*+,-./:;<=.>?@[\\]^_{|}~]\", text) # ->  faz diferença?\n","            # xt_splited = findall(r'(\\w+\\'?\\w*|[?!.,:;\"])', text) # ->  faz diferença?\n","            \n","            if self.remove_number:\n","                # Remove palavras com menos de 2 caracteres\n","                text_splited = [word for word in text_splited if len(word) > self.remove_number]\n","\n","            \n","            \n","            if len(text_splited) > self.max_length:\n","                text_splited = text_splited[:self.max_length]\n","            else:\n","                pad_numbers = self.max_length - len(text_splited)\n","                text_splited = text_splited + ['[PAD]'] * pad_numbers\n","\n","            tokenized_texts.append(text_splited)\n","            \n","    \n","        return tokenized_texts\n","      \n","# MODEL =================================================================================================================        \n","\n","    def model(self, texts):    \n","        \n","        raw_vocab = []\n","        \n","        for text in texts:\n","            raw_vocab.extend([word for word in text if word != '[PAD]'])\n","\n","        c = Counter(raw_vocab)\n","\n","        self.vocab = ['[PAD]', '[UNK]'] + [word for word, _ in c.most_common(self.max_size) ]\n","    \n","        self.word_2_vec = {w:id for id, w in enumerate(self.vocab)}\n","        self.vocab_size = len(self.word_2_vec)\n","\n","# BOW ================================================================================================================= \n","\n","    def bow(self, texts):\n","        \n","        n = len(texts)    \n","        \n","        features = np.zeros((n, self.vocab_size))    \n","        \n","        for i, tokens in enumerate(texts):\n","            \n","            for w in tokens:\n","                \n","                if not self.ignore_pad or (self.ignore_pad and w != '[PAD]'):\n","                    \n","                    id = self.word_2_vec.get(w, self.word_2_vec['[UNK]'])          \n","                    features[i, id] += 1\n","        \n","        if self.ignore_pad:\n","            return np.delete(features, self.word_2_vec['[PAD]'], 1)\n","\n","        return features\n","\n","# SENTENCES TO ID ====================================================================================================== \n","    \n","    def sentence_to_id(self, texts):\n","        coded_sentence = []\n","        for tokens in texts:\n","            word_idx = []\n","            for w in tokens:        \n","                id = self.word_2_vec.get(w, self.word_2_vec['[UNK]'])\n","                word_idx.append(id)\n","            coded_sentence.append(word_idx)\n","        return coded_sentence"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AUP0TPbbIodV","executionInfo":{"status":"ok","timestamp":1631753056916,"user_tz":180,"elapsed":12,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"36ca95c0-55d5-4555-8c69-a4a3cf0840f9"},"source":["texts = ['Eu gosto muito de jogar bola, as vezes não', 'Não gosto de bola']\n","\n","tt =  text2token(train_texts=texts, \n","                 # test_texts, valid_texts,\n","                 max_size=2, stopwords = [],\n","                 remove_number=2,\n","                 max_length=5, ignore_pad=False)\n","\n","\n","print(\"Vocabulary:\", tt.vocab)\n","print(\"\\nDictionary:\", tt.word_2_vec)\n","print(\"\\nVocabulary size:\", tt.vocab_size, \"\\n\")\n","\n","for i, text in enumerate(tt.textos_de_treino):\n","    print(\"Sentence\", i, \":\", text)\n","\n","print(\"\")\n","for i, text in enumerate(tt.bow_train):\n","    print(\"BOW\", i, \":\", text)\n","\n","print(\"\")\n","for i, text in enumerate(tt.idx_train):\n","    print(\"Phrase\", i, \"replaced:\", text)\n","    "],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['[PAD]', '[UNK]', 'gosto', 'bola']\n","\n","Dictionary: {'[PAD]': 0, '[UNK]': 1, 'gosto': 2, 'bola': 3}\n","\n","Vocabulary size: 4 \n","\n","Sentence 0 : ['gosto', 'muito', 'jogar', 'bola', 'vezes']\n","Sentence 1 : ['gosto', 'bola', '[PAD]', '[PAD]', '[PAD]']\n","\n","BOW 0 : [0. 3. 1. 1.]\n","BOW 1 : [3. 0. 1. 1.]\n","\n","Phrase 0 replaced: [2, 1, 1, 3, 1]\n","Phrase 1 replaced: [2, 3, 0, 0, 0]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QxQSTuqIIqpN","executionInfo":{"status":"ok","timestamp":1631754232272,"user_tz":180,"elapsed":42008,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"e90ef49d-cf64-41be-c55b-96783899a814"},"source":["t2t =  text2token(train_texts=x_train, \n","                 test_texts=x_test, valid_texts=x_valid,\n","                 max_size=5000, stopwords = [],\n","                 remove_number=None,\n","                 max_length=200, ignore_pad=True)\n","\n","print(t2t.vocab_size)\n","print(type(t2t.vocab_size))\n","\n","bow_train = t2t.bow_train\n","bow_valid = t2t.bow_valid\n","bow_test = t2t.bow_test\n","\n","idx_train = t2t.idx_train\n","idx_valid = t2t.idx_valid\n","idx_test = t2t.idx_test"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["5002\n","<class 'int'>\n"]}]},{"cell_type":"code","metadata":{"id":"NjyWygbiIsPT","executionInfo":{"status":"ok","timestamp":1631753096293,"user_tz":180,"elapsed":28,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}}},"source":["def train(model, train, valid, n_epochs, lr):\n","    log = {'train_loss':[],\n","           'val_loss':[],\n","           'val_acc':[]}\n","\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adagrad(model.parameters(), lr=lr, lr_decay=1e-4)\n","\n","    # best_valid_acc = 0\n","    # best_epoch = 0\n","    \n","    best_valid_loss = 999999999\n","    best_epoch = 0\n","    \n","    for i in range(n_epochs):\n","        accumulated_loss = 0\n","        model.train()\n","        for x_train, y_train in train:\n","\n","            x_train = x_train.to(device)\n","            y_train = y_train.to(device)\n","            \n","            # Realização de predict\n","            outputs = model(x_train)\n","            \n","            # Calculo da loss\n","            batch_loss = criterion(outputs, y_train)\n","\n","            optimizer.zero_grad()\n","            batch_loss.backward()\n","            optimizer.step()\n","            accumulated_loss += batch_loss.item()\n","\n","            \n","\n","        train_loss = accumulated_loss / len(train.dataset)\n","        log['train_loss'].append(train_loss)\n","        \n","        # Laço de Validação, um a cada época.\n","        accumulated_loss = 0\n","        accumulated_accuracy = 0\n","        model.eval()\n","        \n","        with torch.no_grad():\n","            for x_valid, y_valid in valid:\n","                x_valid = x_valid.to(device)\n","                y_valid = y_valid.to(device)\n","\n","                # predict da rede\n","                outputs = model(x_valid)\n","\n","                # calcula a perda\n","                batch_loss = criterion(outputs, y_valid)\n","                preds = outputs.argmax(dim=1)\n","\n","                # calcula a acurácia\n","                batch_accuracy = (preds == y_valid).sum()\n","                accumulated_loss += batch_loss\n","                accumulated_accuracy += batch_accuracy\n","\n","        valid_loss = accumulated_loss / len(valid.dataset)\n","        valid_acc = accumulated_accuracy / len(valid.dataset)\n","\n","        log['val_loss'].append(valid_loss)\n","        log['val_acc'].append(valid_acc)\n","\n","        if valid_loss < best_valid_loss:\n","            torch.save(model.state_dict(), 'best_model.pt')\n","            best_valid_loss = valid_loss\n","            best_epoch = i\n","            print(f'Epoch = {i:d}/{n_epochs - 1:d}, Train Loss = {train_loss:.6f}, Valid Loss = {valid_loss:.6f}, Valid Acc = {valid_acc*100:.1f}% - BEST MODEL')\n","\n","        else:\n","            print(f'Epoch = {i:d}/{n_epochs - 1:d}, Train Loss = {train_loss:.6f}, Valid Loss = {valid_loss:.6f}, Valid Acc = {valid_acc*100:.1f}%')\n","\n","\n","    model.load_state_dict(torch.load('best_model.pt'))\n","    model.to(device)\n","    return model, log\n","\n","\n","def predict(model, data_loader):    \n","\n","    predictions = []\n","    true_labels = []\n","    model.eval()\n","    with torch.no_grad():\n","        for x_valid, y_valid in data_loader:\n","            x_valid = x_valid.to(device)\n","            y_valid = y_valid.to(device)\n","\n","            # predict da rede\n","            outputs = model(x_valid)\n","\n","            preds = outputs.argmax(dim=1)\n","\n","            predictions.extend(preds.tolist())\n","            true_labels.extend(y_valid.tolist())\n","          \n","    acc = 100*(np.array(predictions) == np.array(true_labels)).mean()\n","    print(f'Accuracy: {acc:.3f}')\n","    # return predictions, true_labels\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j8Om2r0gIyLi","executionInfo":{"status":"ok","timestamp":1631753096686,"user_tz":180,"elapsed":419,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"55b325b7-02aa-4278-ace9-6fdbe4e9a843"},"source":["class NaiveClassifier(torch.nn.Module):\n","    def __init__(self, input_size):\n","        super(NaiveClassifier, self).__init__()\n","\n","        self.dense = torch.nn.Sequential(\n","            torch.nn.Linear(input_size, 256, bias=False),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(256, 2, bias=False)\n","        )\n","        \n","    def forward(self, x):\n","        x = self.dense(x)              \n","        return x\n","\n","if torch.cuda.is_available(): \n","   dev = \"cuda:0\"\n","   print(torch. cuda. get_device_name(dev))\n","else: \n","   dev = \"cpu\" \n","print(dev)\n","device = torch.device(dev)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla K80\n","cuda:0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwSOuQk9I0LM","executionInfo":{"status":"ok","timestamp":1631754241174,"user_tz":180,"elapsed":8916,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"08215c81-cc84-4f10-8199-00d46d5d75b6"},"source":["reset_random_seeds()\n","n_epochs = 10\n","lr = 0.001\n","batch_size = 100\n","\n","\n","train_dataset = TensorDataset(torch.FloatTensor(bow_train),\n","                              torch.LongTensor(y_train))\n","valid_dataset = TensorDataset(torch.FloatTensor(bow_valid),\n","                              torch.LongTensor(y_valid))\n","\n","\n","train_bow_data_loader = DataLoader(train_dataset, \n","                              batch_size=batch_size,\n","                              shuffle=False)\n","\n","\n","valid_bow_data_loader = DataLoader(valid_dataset, \n","                              batch_size=batch_size,\n","                              shuffle=False)\n","\n","naive_model = NaiveClassifier(bow_train.shape[1])\n","initial_weights = [naive_model.dense[0].weight.clone(),\n","                   naive_model.dense[2].weight.clone()]\n","naive_model.to(device)\n","print(naive_model)\n","\n","\n","_ = train(naive_model, train_bow_data_loader, valid_bow_data_loader, n_epochs, lr)"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["NaiveClassifier(\n","  (dense): Sequential(\n","    (0): Linear(in_features=5001, out_features=256, bias=False)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=2, bias=False)\n","  )\n",")\n","Epoch = 0/9, Train Loss = 0.004525, Valid Loss = 0.003764, Valid Acc = 84.4% - BEST MODEL\n","Epoch = 1/9, Train Loss = 0.003272, Valid Loss = 0.003457, Valid Acc = 85.7% - BEST MODEL\n","Epoch = 2/9, Train Loss = 0.002923, Valid Loss = 0.003340, Valid Acc = 85.9% - BEST MODEL\n","Epoch = 3/9, Train Loss = 0.002723, Valid Loss = 0.003283, Valid Acc = 86.1% - BEST MODEL\n","Epoch = 4/9, Train Loss = 0.002585, Valid Loss = 0.003254, Valid Acc = 86.2% - BEST MODEL\n","Epoch = 5/9, Train Loss = 0.002481, Valid Loss = 0.003239, Valid Acc = 86.3% - BEST MODEL\n","Epoch = 6/9, Train Loss = 0.002399, Valid Loss = 0.003233, Valid Acc = 86.3% - BEST MODEL\n","Epoch = 7/9, Train Loss = 0.002330, Valid Loss = 0.003231, Valid Acc = 86.4% - BEST MODEL\n","Epoch = 8/9, Train Loss = 0.002271, Valid Loss = 0.003234, Valid Acc = 86.4%\n","Epoch = 9/9, Train Loss = 0.002220, Valid Loss = 0.003239, Valid Acc = 86.5%\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6b9vMy9DuWBc","executionInfo":{"status":"ok","timestamp":1631754245098,"user_tz":180,"elapsed":1234,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"ab7d8aa4-7d8a-41f6-f180-e50ec713f621"},"source":["test_dataset = TensorDataset(torch.FloatTensor(bow_test),\n","                              torch.LongTensor(y_test))\n","test_bow_data_loader = DataLoader(test_dataset, \n","                              batch_size=batch_size,\n","                              shuffle=False)\n","\n","predict(naive_model, test_bow_data_loader)\n"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 86.004\n"]}]},{"cell_type":"code","metadata":{"id":"i1MbX9cDI1oJ","executionInfo":{"status":"ok","timestamp":1631753123578,"user_tz":180,"elapsed":7,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}}},"source":["class EmbeddingClassifier(torch.nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(EmbeddingClassifier, self).__init__()\n","        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)  \n","        self.linear = torch.nn.Linear(embedding_dim, 2, bias=False)               \n","        \n","    def forward(self, x):\n","        x = self.embedding(x)           \n","        x = torch.sum(x, dim=1) \n","        x = torch.nn.functional.relu(x)\n","        x = self.linear(x)\n","        return x"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLP0dYN5I3MU","executionInfo":{"status":"ok","timestamp":1631754261423,"user_tz":180,"elapsed":8195,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"82fb0698-6a86-404e-969a-537af21efce5"},"source":["reset_random_seeds()\n","n_epochs = 10\n","lr = 0.001\n","batch_size = 100\n","\n","train_dataset = TensorDataset(torch.LongTensor(idx_train),\n","                              torch.LongTensor(y_train))\n","\n","valid_dataset = TensorDataset(torch.LongTensor(idx_valid),\n","                              torch.LongTensor(y_valid))\n","\n","train_id_data_loader = DataLoader(train_dataset, \n","                              batch_size=batch_size,\n","                              shuffle=False)\n","\n","\n","valid_id_data_loader = DataLoader(valid_dataset, \n","                              batch_size=batch_size,\n","                              shuffle=False)\n","\n","\n","embedding_dim=256\n","# add padding column\n","zero = torch.zeros(embedding_dim,1)\n","initial_weight_padding = torch.cat([zero, initial_weights[0]], dim=1)\n","weights = OrderedDict([\n","    ('embedding.weight',  initial_weight_padding.T),\n","    ('linear.weight',  initial_weights[1])])\n","\n","embedding_model = EmbeddingClassifier(vocab_size=t2t.vocab_size, embedding_dim=embedding_dim)\n","embedding_model.load_state_dict(weights)\n","embedding_model.to(device)\n","print(embedding_model)\n","\n","_ = train(embedding_model, train_id_data_loader, valid_id_data_loader, n_epochs, lr)"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["EmbeddingClassifier(\n","  (embedding): Embedding(5002, 256, padding_idx=0)\n","  (linear): Linear(in_features=256, out_features=2, bias=False)\n",")\n","Epoch = 0/9, Train Loss = 0.004525, Valid Loss = 0.003764, Valid Acc = 84.4% - BEST MODEL\n","Epoch = 1/9, Train Loss = 0.003272, Valid Loss = 0.003457, Valid Acc = 85.7% - BEST MODEL\n","Epoch = 2/9, Train Loss = 0.002923, Valid Loss = 0.003340, Valid Acc = 85.9% - BEST MODEL\n","Epoch = 3/9, Train Loss = 0.002723, Valid Loss = 0.003283, Valid Acc = 86.1% - BEST MODEL\n","Epoch = 4/9, Train Loss = 0.002585, Valid Loss = 0.003254, Valid Acc = 86.2% - BEST MODEL\n","Epoch = 5/9, Train Loss = 0.002481, Valid Loss = 0.003239, Valid Acc = 86.3% - BEST MODEL\n","Epoch = 6/9, Train Loss = 0.002399, Valid Loss = 0.003233, Valid Acc = 86.3% - BEST MODEL\n","Epoch = 7/9, Train Loss = 0.002330, Valid Loss = 0.003231, Valid Acc = 86.4% - BEST MODEL\n","Epoch = 8/9, Train Loss = 0.002271, Valid Loss = 0.003234, Valid Acc = 86.4%\n","Epoch = 9/9, Train Loss = 0.002220, Valid Loss = 0.003239, Valid Acc = 86.5%\n"]}]}]}