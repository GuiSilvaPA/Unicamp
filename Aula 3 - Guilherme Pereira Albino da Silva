{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Aula 3 - Guilherme Pereira Albino da Silva","provenance":[{"file_id":"1Y3rRUiQGW5CEcPRkx_sfZGAEjNwVsw-b","timestamp":1630526672721},{"file_id":"1ONeS-lZ3vVqThueoTvQRMnZ_rJJB1yOl","timestamp":1629906878859}],"collapsed_sections":["N4VZ6c-Rzpgd"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1OG5DT_dm6mk"},"source":["# Notebook de referência \n","\n","Nome: Guilherme Pereira Albino da Silva - RA: 182786"]},{"cell_type":"markdown","metadata":{"id":"Od7iUgHy5SSi"},"source":["## Instruções\n","\n","- Treinar uma rede neural como classificador binário na tarefa de análise de sentimentos usando dataset IMDB.\n","\n","- Experimentar e reportar a acurácia usando 3 diferentes tipos de features como entrada:\n","    1) Bag-of-words booleano\n","    2) Bag-of-words com contagem das palavras (histograma das palavras)\n","    3) TF-IDF\n","\n","Deve-se implementar o laço de treinamento e validação da rede neural.\n","\n","Neste exercício usaremos o IMDB com 20l exemplos para treino, 5k para desenvolvimento e 25k para teste."]},{"cell_type":"markdown","metadata":{"id":"CXFdJz2KVeQw"},"source":["## Preparando Dados"]},{"cell_type":"markdown","metadata":{"id":"gHMi_Kq65fPM"},"source":["Primeiro, fazemos download do dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2wbnfzst5O3k","executionInfo":{"status":"ok","timestamp":1630546721213,"user_tz":180,"elapsed":22185,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"731cf7e1-0067-47ca-8c69-299f07517fa9"},"source":["!wget -nc http://files.fast.ai/data/aclImdb.tgz \n","!tar -xzf aclImdb.tgz"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["File ‘aclImdb.tgz’ already there; not retrieving.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"0Giyi5Rv_NIm"},"source":["## Carregando o dataset\n","\n","Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n","\n","Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0HIN_xLI_TuT","executionInfo":{"status":"ok","timestamp":1630546745446,"user_tz":180,"elapsed":2033,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"bc0fd737-1231-45d7-f053-76678c4d4b49"},"source":["import os\n","import random\n","\n","\n","def load_texts(folder):\n","    texts = []\n","    for path in os.listdir(folder):\n","        with open(os.path.join(folder, path)) as f:\n","            texts.append(f.read())\n","    return texts\n","\n","x_train_pos = load_texts('aclImdb/train/pos')\n","x_train_neg = load_texts('aclImdb/train/neg')\n","x_test_pos = load_texts('aclImdb/test/pos')\n","x_test_neg = load_texts('aclImdb/test/neg')\n","\n","x_train = x_train_pos + x_train_neg\n","x_test = x_test_pos + x_test_neg\n","y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n","y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n","\n","# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n","c = list(zip(x_train, y_train))\n","random.shuffle(c)\n","x_train, y_train = zip(*c)\n","\n","n_train = int(0.8 * len(x_train))\n","\n","x_valid = x_train[n_train:]\n","y_valid = y_train[n_train:]\n","x_train = x_train[:n_train]\n","y_train = y_train[:n_train]\n","\n","print(len(x_train), 'amostras de treino.')\n","print(len(x_valid), 'amostras de desenvolvimento.')\n","print(len(x_test), 'amostras de teste.')\n","\n","print('3 primeiras amostras treino:')\n","for x, y in zip(x_train[:3], y_train[:3]):\n","    print(y, x[:100])\n","\n","print('3 últimas amostras treino:')\n","for x, y in zip(x_train[-3:], y_train[-3:]):\n","    print(y, x[:100])\n","\n","print('3 primeiras amostras validação:')\n","for x, y in zip(x_valid[:3], y_test[:3]):\n","    print(y, x[:100])\n","\n","print('3 últimas amostras validação:')\n","for x, y in zip(x_valid[-3:], y_valid[-3:]):\n","    print(y, x[:100])"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["20000 amostras de treino.\n","5000 amostras de desenvolvimento.\n","25000 amostras de teste.\n","3 primeiras amostras treino:\n","True this was a very good movie i wished i could find it in vhs to buy,i really enjoyed this movie i woul\n","True Based on an actual mining disaster, this early German talkie (with English subtitles) still remains \n","False When the folks at Kino Video assembled their fine \"Slapstick Encyclopedia\" collection, a multi-casse\n","3 últimas amostras treino:\n","False We see a body of dead girl in a morgue with the coroner trying to close the eyes of the girl, but wh\n","True I saw this film at the Chicago Reeling film festival. To pick up on the previous reviewer's remarks,\n","True It has been some years since I saw this, but remember it and would like to see it again. It kind of \n","3 primeiras amostras validação:\n","True There are some elements that save this movie from being a total catastrophe, but are overshadowed wi\n","True Not one of the better pokemon movies.<br /><br />Two legendary pokemon come into the story. You do g\n","True Presenting Lily Mars may have provided Judy Garland with one of the easier roles she had while at MG\n","3 últimas amostras validação:\n","True Jane Eyre has always been my favorite novel! When I stumbled upon this movie version in the late 90'\n","True Before Sunrise has many remarkable things going on, almost too many to fit into one review like this\n","False you can tell they spent 5$ making this.it is a waste of your time... ugh.. there is not anything rem\n"]}]},{"cell_type":"markdown","metadata":{"id":"N4VZ6c-Rzpgd"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"SgQKGxJa14XH","executionInfo":{"status":"ok","timestamp":1630546751141,"user_tz":180,"elapsed":900,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["import os\n","import random\n","\n","from collections import Counter\n","import numpy as np\n","import torch\n","\n","from re import findall, sub\n","import string\n","\n","from bs4 import BeautifulSoup\n","\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"acNLq4u8zuvN"},"source":["# text2token"]},{"cell_type":"code","metadata":{"id":"S_N-426i050-","executionInfo":{"status":"ok","timestamp":1630546752912,"user_tz":180,"elapsed":369,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["class text2token():\n","    \n","    def __init__(self, train_texts, test_texts, valid_texts,\n","                 mode = \"BOW\", boolean=False, max_size=None, stopwords = [], unk=False):       \n","        \n","        self.max_size = max_size\n","        self.boolean = boolean\n","        self.stopwords = stopwords\n","        self.unk = unk\n","        self.mode = mode\n","        \n","        splited_train_text = self.split(train_texts)\n","        self.createModel(splited_train_text)\n","        \n","        self.train_vector = self.tokenizer(splited_train_text)\n","        \n","        splited_test_text = self.split(test_texts)\n","        self.test_vector = self.tokenizer(splited_test_text)\n","        \n","        splited_valid_text = self.split(valid_texts)\n","        self.valid_vector = self.tokenizer(splited_valid_text)\n","        \n","    \n","# SPLIT ============================================================================================ \n","    \n","    def split(self, texts):\n","        \n","        tokenized_texts = []\n","        \n","        \n","        \n","        for text in texts:\n","            \n","            text = text.lower()\n","            \n","            # Remove as tags HTLM\n","            text = BeautifulSoup(text, \"lxml\").text\n","            \n","            # Remove os caracteres especiais\n","            text = sub('[^A-Za-z\\s]+', ' ', text)\n","\n","            # Remove os números\n","            text = sub(r'[0-9+]', ' ', text)\n","            \n","            # Remove alguns pontos e separa\n","            text_splited = findall(r'\\w+|[^?\\-!.,:;\"\\'/><\\s]', text)\n","            \n","            # Remove palavras com menos de 2 caracteres\n","            text_splited = [word for word in text_splited if len(word) > 2]\n","\n","            \n","            tokenized_texts.append(text_splited)\n","            \n","        return tokenized_texts\n","    \n","# CREATETF ============================================================================================    \n","    \n","    def createTF(self, token_texts):\n","\n","        c = Counter()\n","\n","        for text in token_texts:\n","            c.update(set(text))\n","\n","        for stop_word in self.stopwords:\n","            if stop_word in c.keys():\n","                del c[stop_word]\n","\n","        return c\n","\n","# CREATEIDF ============================================================================================ \n","    \n","    def createIDF(self, counter):\n","\n","        idf = [self.len_corpus/count for token, count in counter.most_common(self.max_size)]\n","        return np.log10(idf)\n","    \n","# CREATEMODEL ============================================================================================ \n","\n","    def createModel(self, tokenized_texts):\n","        \n","        if self.mode == \"BOW\":\n","\n","            c = Counter()\n","\n","            for text in tokenized_texts:\n","                c.update(text)\n","\n","            for stop_word in self.stopwords:\n","                if stop_word in c.keys():\n","                    del c[stop_word]\n","\n","            vocab = {element[0]: index for index, element in enumerate(c.most_common(self.max_size))}\n","            if self.unk:\n","                vocab['unknown'] = len(vocab)\n","                \n","            self.vocabulary = vocab\n","            \n","        if self.mode == \"TFIDF\":\n","            \n","            self.len_corpus = len(tokenized_texts)\n","            c = self.createTF(tokenized_texts)\n","            # print(counter)            \n","            \n","            vocab = {element[0]: index for index, element in enumerate(c.most_common(self.max_size))}\n","            self.vocabulary = vocab\n","            \n","            self.idf = self.createIDF(c)\n","            # print(self.idf)\n","        \n","# TOKENIZER ============================================================================================ \n","\n","    def tokenizer(self, texts):\n","        \n","        if self.mode == \"BOW\":\n","            token_texts = []\n","            if self.unk:\n","                unknown = self.vocabulary.get('unknown')\n","\n","            for i, text in enumerate(texts):\n","                bow_text = torch.zeros(len(self.vocabulary))\n","                counter = Counter(text)\n","\n","                if self.unk:\n","                    index = [self.vocabulary.get(key, unknown) for key in counter.keys()]\n","                else:\n","                    index = [self.vocabulary[key] for key in counter.keys() if key in self.vocabulary.keys()]\n","\n","                if self.boolean:\n","                    bow_text[index] = 1\n","                else:\n","                    values = [value for key, value in counter.items() if self.unk or key in self.vocabulary.keys()]\n","                    bow_text[index] = torch.Tensor(values)\n","\n","                token_texts.append(bow_text)\n","\n","            # print(transformed_texts)\n","            # print(type(bow_text))\n","            # return transformed_texts\n","            # return torch.cat(transformed_texts,dim=-1).float()\n","            return torch.vstack(token_texts).float()\n","            \n","            \n","        if self.mode == \"TFIDF\":\n","\n","            token_texts = []\n","\n","            for i, text in enumerate(texts):\n","\n","                tfidf_text = torch.zeros(len(self.vocabulary))\n","                c = Counter(text)\n","                index = []\n","                values = []\n","                for key, value in c.items():\n","                    if key in self.vocabulary.keys():\n","                        index.append(self.vocabulary[key])\n","                        values.append(value)\n","\n","                tfidf_text[index] = torch.Tensor(values)\n","                token_texts.append(tfidf_text * self.idf)\n","\n","            # return transformed_texts\n","            return torch.vstack(token_texts).float()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIUroGpvCPoN","executionInfo":{"status":"ok","timestamp":1630546759129,"user_tz":180,"elapsed":357,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"7ee66ca4-ece1-4d63-ca17-d166f8a1b676"},"source":["textos1 = [\"eu gosto de jogar bola\", \"eu nao gosto de jogar tenis\", \"tenis é um esporte com bola\"]\n","textos2 = [\"eu nao sei jogar futebol ou jogar tenis\"]\n","textos3 = [\"nao, nao gosto de tenis!\"]\n","\n","teste = text2token(train_texts=textos1, test_texts=textos2, valid_texts=textos3, boolean=True)\n","\n","print(teste.vocabulary)\n","print(\"\\n\\n====== BOOLEANO ======\\n\")\n","print(\"\\nVetor teste [0, 1, 0, 1, 1, 0, 0]: \" + str(teste.test_vector))\n","print(\"\\nVetor teste [1, 0, 0, 1, 1, 0, 0]: \" + str(teste.valid_vector))\n","\n","\n","teste = text2token(train_texts=textos1, test_texts=textos2, valid_texts=textos3, boolean=False)\n","\n","print(\"\\n\\n====== FREQUENCIA ======\\n\")\n","print(\"\\nVetor teste [0, 2, 0, 1, 1, 0, 0]: \" + str(teste.test_vector))\n","print(\"\\nVetor teste [1, 0, 0, 1, 2, 0, 0]: \" + str(teste.valid_vector))\n","\n","teste = text2token(train_texts=textos1, test_texts=textos2, valid_texts=textos3, mode=\"TFIDF\")\n","\n","print(\"\\n\\n====== TFIDF ======\\n\")\n","print(\"\\nVetor teste [0, 0.352, 0, 0.176, 0.477, 0, 0]: \" + str(teste.test_vector))\n","print(\"\\nVetor teste [0.176, 0, 0, 0.176, 0.954, 0, 0]: \" + str(teste.valid_vector))\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'gosto': 0, 'jogar': 1, 'bola': 2, 'tenis': 3, 'nao': 4, 'esporte': 5, 'com': 6}\n","\n","\n","====== BOOLEANO ======\n","\n","\n","Vetor teste [0, 1, 0, 1, 1, 0, 0]: tensor([[0., 1., 0., 1., 1., 0., 0.]])\n","\n","Vetor teste [1, 0, 0, 1, 1, 0, 0]: tensor([[1., 0., 0., 1., 1., 0., 0.]])\n","\n","\n","====== FREQUENCIA ======\n","\n","\n","Vetor teste [0, 2, 0, 1, 1, 0, 0]: tensor([[0., 2., 0., 1., 1., 0., 0.]])\n","\n","Vetor teste [1, 0, 0, 1, 2, 0, 0]: tensor([[1., 0., 0., 1., 2., 0., 0.]])\n","\n","\n","====== TFIDF ======\n","\n","\n","Vetor teste [0, 0.352, 0, 0.176, 0.477, 0, 0]: tensor([[0.0000, 0.3522, 0.0000, 0.1761, 0.4771, 0.0000, 0.0000]])\n","\n","Vetor teste [0.176, 0, 0, 0.176, 0.954, 0, 0]: tensor([[0.1761, 0.0000, 0.0000, 0.1761, 0.9542, 0.0000, 0.0000]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"LYmt4sU-zyDA"},"source":["# RNA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDUmbW2E0-nl","executionInfo":{"status":"ok","timestamp":1630547090230,"user_tz":180,"elapsed":372,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"5f6cc011-4b81-43a9-f165-4cd0f9dc760e"},"source":["# CLASS DATASETCREATOR ======================================================\n","\n","class datasetCreator(Dataset):\n","    def __init__(self, x, y):\n","        self.x = x\n","        self.y = y\n","\n","    def __len__(self) -> int:\n","        return len(self.x)\n","\n","    def __getitem__(self, idx):\n","        x = self.x[idx]\n","        y = self.y[idx]\n","        return x, y\n","\n","# NN_CREATOR ======================================================\n","\n","class NN_CREATOR(torch.nn.Module):\n","    def __init__(self, input_layer):\n","        super().__init__()\n","        self.dense = torch.nn.Sequential(\n","            \n","            torch.nn.Linear(input_layer, 128),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Linear(128, 64),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Linear(64, 32),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Linear(32, 16),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Linear(16, 1),\n","            torch.nn.Sigmoid()\n","        )\n","    \n","    def forward(self, x):\n","        x = self.dense(x)\n","        return x\n","\n","# NN_ENTRADA FUNCTION ======================================================\n","\n","def NN_entrada(vectorized_texts_train, y_train,\n","               vectorized_texts_valid, y_valid,\n","               vectorized_texts_test, y_test,\n","               vectorizer, batch_size, shuffle):\n","\n","    train_dataset = datasetCreator(vectorized_texts_train, torch.Tensor(y_train).reshape(-1,1))\n","    valid_dataset = datasetCreator(vectorized_texts_valid, torch.Tensor(y_valid).reshape(-1,1))\n","    test_dataset  = datasetCreator(vectorized_texts_test, torch.Tensor(y_test).reshape(-1,1))\n","\n","    trainLoad = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    validLoad = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","    testLoad  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return trainLoad, validLoad, testLoad, vectorizer\n","\n","# TRAIN FUNCTION ======================================================\n","\n","def trainModel(model, train, valid, criterion, optimizer,\n","              max_size, filename_save, batch_size=64, n_epochs=10):\n","  \n","    best_valid_loss = 999999999\n","    best_epoch = 0\n","\n","    for i in range(n_epochs):\n","        accumulated_loss = 0\n","        model.train()\n","\n","        for x_train, y_train in train:\n","            x_train = x_train.to(device)\n","            y_train = y_train.to(device)\n","            outputs = model(x_train)\n","            batch_loss = criterion(outputs, y_train)\n","\n","            optimizer.zero_grad()\n","            batch_loss.backward()\n","            optimizer.step()\n","            accumulated_loss += batch_loss.item()\n","\n","        train_loss = accumulated_loss / len(train.dataset)\n","    \n","        accumulated_loss = 0\n","        accumulated_accuracy = 0\n","        model.eval()\n","        \n","        # n_epochs vezes para treinar o modelo\n","        with torch.no_grad():\n","            for x_valid, y_valid in valid:\n","\n","                x_valid = x_valid.to(device)\n","                y_valid = y_valid.to(device)\n","\n","                # Predict\n","                outputs = model(x_valid)\n","\n","                # Loss\n","                batch_loss = criterion(outputs, y_valid)\n","                preds = outputs > 0.5\n","\n","                # Accuracy\n","                batch_accuracy = (preds == y_valid).sum()\n","                accumulated_loss += batch_loss\n","                accumulated_accuracy += batch_accuracy\n","\n","        valid_loss = accumulated_loss / len(valid.dataset)\n","        valid_acc = accumulated_accuracy / len(valid.dataset)\n","        \n","\n","        # Salvando o melhor modelo de acordo com a loss de validação\n","        if valid_loss < best_valid_loss:\n","            torch.save(model.state_dict(), filename_save + '.pt')\n","            best_valid_loss = valid_loss\n","            best_epoch = i\n","            print(f'Epoch = {i:d}/{n_epochs - 1:d}, Train Loss = {train_loss:.6f}, Valid Loss = {valid_loss:.6f}, Valid Acc = {valid_acc*100:.1f}% - BEST MODEL')\n","\n","        else:\n","            print(f'Epoch = {i:d}/{n_epochs - 1:d}, Train Loss = {train_loss:.6f}, Valid Loss = {valid_loss:.6f}, Valid Acc = {valid_acc*100:.1f}%')\n","\n","    return model\n","\n","# PREDICT FUNCTION ======================================================\n","\n","def predict(model, state_dict, test):\n","    accumulated_accuracy = 0\n","    model.load_state_dict(torch.load(state_dict + '.pt'))\n","    model.eval()\n","    with torch.no_grad():\n","        for x_test, y_test in test:\n","            x_test = x_test.to(device)\n","            y_test = y_test.to(device)\n","\n","            # predict da rede\n","            outputs = model(x_test)\n","\n","            # calcula a perda\n","            batch_loss = criterion(outputs, y_test)\n","            preds = outputs > 0.5\n","\n","            # calcula a acurácia\n","            batch_accuracy = (preds == y_test).sum()\n","            accumulated_accuracy += batch_accuracy\n","\n","    test_acc = accumulated_accuracy / len(test.dataset)\n","    test_acc *= 100\n","    print(f'Accuracy: {test_acc:.3f} %')\n","\n","if torch.cuda.is_available(): \n","    dev = \"cuda:0\"\n","    print(torch. cuda. get_device_name(dev))\n","else: \n","    dev = \"cpu\" \n","print(dev)\n","device = torch.device(dev)\n","\n","# Créditos ao aluno Pedro Gabriel Gengo Lourenço, em que me inspirei em algumas partes\n","# no momento estou estudando para solucionar o que está defasado."],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla K80\n","cuda:0\n"]}]},{"cell_type":"markdown","metadata":{"id":"L5Wfu8H2vaK_"},"source":["# Treinando para BOW booleano"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-k28fWPM1Nx0","executionInfo":{"status":"ok","timestamp":1630547095381,"user_tz":180,"elapsed":368,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"144838b9-d343-4ff7-b922-002da970d06b"},"source":["max_size = 3000\n","batch_size = 64\n","n_epochs = 10\n","learningRate = 0.05\n","save_filename = 'BOW_BOOLEANO'\n","\n","mlp_BOW_BOOLEANO = NN_CREATOR(max_size)\n","mlp_BOW_BOOLEANO.to(device)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NN_CREATOR(\n","  (dense): Sequential(\n","    (0): Linear(in_features=3000, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=64, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=64, out_features=32, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=32, out_features=16, bias=True)\n","    (7): ReLU()\n","    (8): Linear(in_features=16, out_features=1, bias=True)\n","    (9): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"MefZQj3C1QHZ","executionInfo":{"status":"ok","timestamp":1630547099319,"user_tz":180,"elapsed":368,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(mlp_BOW_BOOLEANO.parameters(), lr=learningRate)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArL6sn450wBT","executionInfo":{"status":"ok","timestamp":1630546885366,"user_tz":180,"elapsed":31732,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["data1 = text2token(train_texts=x_train, test_texts=x_test, valid_texts=x_valid, boolean=True, max_size = max_size)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RRvijh-I1RkN","executionInfo":{"status":"ok","timestamp":1630547110302,"user_tz":180,"elapsed":9079,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"6dd2288d-7b15-476f-a052-909a8b9f06ad"},"source":["vectorized_train, vectorized_valid, vectorized_test, _ = NN_entrada(data1.train_vector, y_train,\n","                                                                    data1.valid_vector, y_valid,\n","                                                                    data1.test_vector, y_test,\n","                                                                    data1, batch_size, True)\n","\n","_ = trainModel(mlp_BOW_BOOLEANO, vectorized_train, vectorized_valid, criterion, optimizer,\n","              max_size, save_filename, batch_size, n_epochs=n_epochs)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch = 0/9, Train Loss = 0.010850, Valid Loss = 0.010948, Valid Acc = 50.8% - BEST MODEL\n","Epoch = 1/9, Train Loss = 0.010841, Valid Loss = 0.010937, Valid Acc = 65.2% - BEST MODEL\n","Epoch = 2/9, Train Loss = 0.010815, Valid Loss = 0.010881, Valid Acc = 62.1% - BEST MODEL\n","Epoch = 3/9, Train Loss = 0.010356, Valid Loss = 0.008777, Valid Acc = 81.2% - BEST MODEL\n","Epoch = 4/9, Train Loss = 0.006433, Valid Loss = 0.005039, Valid Acc = 87.1% - BEST MODEL\n","Epoch = 5/9, Train Loss = 0.005008, Valid Loss = 0.004750, Valid Acc = 87.6% - BEST MODEL\n","Epoch = 6/9, Train Loss = 0.004444, Valid Loss = 0.005201, Valid Acc = 86.1%\n","Epoch = 7/9, Train Loss = 0.004059, Valid Loss = 0.004841, Valid Acc = 87.6%\n","Epoch = 8/9, Train Loss = 0.003704, Valid Loss = 0.004593, Valid Acc = 87.8% - BEST MODEL\n","Epoch = 9/9, Train Loss = 0.003311, Valid Loss = 0.004952, Valid Acc = 87.2%\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bOqcqeV2oY_","executionInfo":{"status":"ok","timestamp":1630547113640,"user_tz":180,"elapsed":1031,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"e4bb4b22-6951-4e58-99d6-7de882dece7c"},"source":["predict(mlp_BOW_BOOLEANO, save_filename, vectorized_test)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 87.208 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"fsZlBBXivkug"},"source":["# Treinando para BOW frequencia"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UG_9hCUmvkuh","executionInfo":{"status":"ok","timestamp":1630547137737,"user_tz":180,"elapsed":371,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"6a982c8e-1a60-43b9-a459-ce4ea0afbce9"},"source":["max_size = 3000\n","batch_size = 64\n","n_epochs = 10\n","learningRate = 0.1\n","save_filename = 'BOW_FREQ'\n","\n","mlp_BOW_FREQ = NN_CREATOR(max_size)\n","mlp_BOW_FREQ.to(device)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NN_CREATOR(\n","  (dense): Sequential(\n","    (0): Linear(in_features=3000, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=64, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=64, out_features=32, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=32, out_features=16, bias=True)\n","    (7): ReLU()\n","    (8): Linear(in_features=16, out_features=1, bias=True)\n","    (9): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"ATbi2gsZvku0","executionInfo":{"status":"ok","timestamp":1630547140084,"user_tz":180,"elapsed":6,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(mlp_BOW_FREQ.parameters(), lr=learningRate)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6FFEBDA4pgy","executionInfo":{"status":"ok","timestamp":1630547175561,"user_tz":180,"elapsed":33557,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["data2 = text2token(train_texts=x_train, test_texts=x_test, valid_texts=x_valid, boolean=False, max_size = max_size)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ciAb2j1vku1","executionInfo":{"status":"ok","timestamp":1630547187555,"user_tz":180,"elapsed":9520,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"61edca27-7b94-4f71-ee6d-742a62850496"},"source":["vectorized_train, vectorized_valid, vectorized_test, _ = NN_entrada(data2.train_vector, y_train,\n","                                                                    data2.valid_vector, y_valid,\n","                                                                    data2.test_vector, y_test,\n","                                                                    data2, batch_size, True)\n","\n","_ = trainModel(mlp_BOW_FREQ, vectorized_train, vectorized_valid, criterion, optimizer,\n","              max_size, save_filename, batch_size, n_epochs=n_epochs)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch = 0/9, Train Loss = 0.010544, Valid Loss = 0.009156, Valid Acc = 73.3% - BEST MODEL\n","Epoch = 1/9, Train Loss = 0.008694, Valid Loss = 0.009406, Valid Acc = 66.7%\n","Epoch = 2/9, Train Loss = 0.007130, Valid Loss = 0.006282, Valid Acc = 82.3% - BEST MODEL\n","Epoch = 3/9, Train Loss = 0.006363, Valid Loss = 0.005903, Valid Acc = 83.2% - BEST MODEL\n","Epoch = 4/9, Train Loss = 0.006038, Valid Loss = 0.006665, Valid Acc = 81.4%\n","Epoch = 5/9, Train Loss = 0.005526, Valid Loss = 0.006593, Valid Acc = 80.0%\n","Epoch = 6/9, Train Loss = 0.005263, Valid Loss = 0.005607, Valid Acc = 82.7% - BEST MODEL\n","Epoch = 7/9, Train Loss = 0.005111, Valid Loss = 0.004763, Valid Acc = 87.6% - BEST MODEL\n","Epoch = 8/9, Train Loss = 0.004826, Valid Loss = 0.004673, Valid Acc = 87.7% - BEST MODEL\n","Epoch = 9/9, Train Loss = 0.004609, Valid Loss = 0.005189, Valid Acc = 85.3%\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNX450Mgvku2","executionInfo":{"status":"ok","timestamp":1630547190381,"user_tz":180,"elapsed":902,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"79edd855-c334-4114-89bd-f31bc1e29170"},"source":["predict(mlp_BOW_FREQ, save_filename, vectorized_test)"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 87.064 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"zjqNmNxUw2lW"},"source":["# Treinando para TF-IDF"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mnoQn0xw2lt","executionInfo":{"status":"ok","timestamp":1630547214581,"user_tz":180,"elapsed":367,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"bbd58a75-0c02-4f88-b67f-b673e945bbf7"},"source":["max_size = 3000\n","batch_size = 64\n","n_epochs = 10\n","learningRate = 0.1\n","save_filename = 'TFIDF'\n","\n","mlp_TFIDF = NN_CREATOR(max_size)\n","mlp_TFIDF.to(device)"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NN_CREATOR(\n","  (dense): Sequential(\n","    (0): Linear(in_features=3000, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=64, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=64, out_features=32, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=32, out_features=16, bias=True)\n","    (7): ReLU()\n","    (8): Linear(in_features=16, out_features=1, bias=True)\n","    (9): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"1ckJU8lJw2lu","executionInfo":{"status":"ok","timestamp":1630547217622,"user_tz":180,"elapsed":359,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(mlp_TFIDF.parameters(), lr=learningRate)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"fN8N9um94uu8","executionInfo":{"status":"ok","timestamp":1630547260003,"user_tz":180,"elapsed":40933,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}}},"source":["data3 = text2token(train_texts=x_train, test_texts=x_test, valid_texts=x_valid, mode=\"TFIDF\", max_size = max_size)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DpbXHLZQw2lv","executionInfo":{"status":"ok","timestamp":1630547269336,"user_tz":180,"elapsed":9346,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"e8330e45-4b2d-425c-aa1f-92d7bbeca512"},"source":["vectorized_train, vectorized_valid, vectorized_test, _ = NN_entrada(data3.train_vector, y_train,\n","                                                                    data3.valid_vector, y_valid,\n","                                                                    data3.test_vector, y_test,\n","                                                                    data3, batch_size, True)\n","\n","_ = trainModel(mlp_TFIDF, vectorized_train, vectorized_valid, criterion, optimizer,\n","              max_size, save_filename, batch_size, n_epochs=n_epochs)"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch = 0/9, Train Loss = 0.010819, Valid Loss = 0.010747, Valid Acc = 69.8% - BEST MODEL\n","Epoch = 1/9, Train Loss = 0.006959, Valid Loss = 0.005039, Valid Acc = 86.8% - BEST MODEL\n","Epoch = 2/9, Train Loss = 0.004271, Valid Loss = 0.004544, Valid Acc = 88.0% - BEST MODEL\n","Epoch = 3/9, Train Loss = 0.003089, Valid Loss = 0.004832, Valid Acc = 87.3%\n","Epoch = 4/9, Train Loss = 0.001922, Valid Loss = 0.006247, Valid Acc = 86.0%\n","Epoch = 5/9, Train Loss = 0.001162, Valid Loss = 0.006569, Valid Acc = 86.1%\n","Epoch = 6/9, Train Loss = 0.000772, Valid Loss = 0.008750, Valid Acc = 84.9%\n","Epoch = 7/9, Train Loss = 0.000651, Valid Loss = 0.008151, Valid Acc = 86.3%\n","Epoch = 8/9, Train Loss = 0.000675, Valid Loss = 0.008035, Valid Acc = 86.4%\n","Epoch = 9/9, Train Loss = 0.000606, Valid Loss = 0.008690, Valid Acc = 86.0%\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTJSGvRMw2lw","executionInfo":{"status":"ok","timestamp":1630547273242,"user_tz":180,"elapsed":898,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"","userId":"07971392613373651702"}},"outputId":"ed5e2dbc-6a2f-4b32-9e5b-9e7ad922b9b6"},"source":["predict(mlp_TFIDF, save_filename, vectorized_test)"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 87.068 %\n"]}]}]}