{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Aula 4 - Guilherme Pereira Albino da Silva","provenance":[{"file_id":"1FFRcCT8Jagd0iKHi_xuMpp97iWzOuck9","timestamp":1631127626618},{"file_id":"1Y3rRUiQGW5CEcPRkx_sfZGAEjNwVsw-b","timestamp":1630526672721},{"file_id":"1ONeS-lZ3vVqThueoTvQRMnZ_rJJB1yOl","timestamp":1629906878859}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1OG5DT_dm6mk"},"source":["# Notebook de referência \n","\n","Nome: Guilherme Pereira Albino da Silva - RA: 182786"]},{"cell_type":"markdown","metadata":{"id":"Od7iUgHy5SSi"},"source":["## Instruções\n","\n","- Treinar uma rede neural como classificador binário na tarefa de análise de sentimentos usando dataset IMDB.\n","\n","- Experimentar e reportar a acurácia usando 3 diferentes tipos de features como entrada:\n","    1) Bag-of-words booleano\n","    2) Bag-of-words com contagem das palavras (histograma das palavras)\n","    3) TF-IDF\n","\n","Deve-se implementar o laço de treinamento e validação da rede neural.\n","\n","Neste exercício usaremos o IMDB com 20l exemplos para treino, 5k para desenvolvimento e 25k para teste."]},{"cell_type":"markdown","metadata":{"id":"CXFdJz2KVeQw"},"source":["## Preparando Dados"]},{"cell_type":"markdown","metadata":{"id":"gHMi_Kq65fPM"},"source":["Primeiro, fazemos download do dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2wbnfzst5O3k","executionInfo":{"status":"ok","timestamp":1631149678173,"user_tz":180,"elapsed":22556,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"12e3a4e4-5d17-456a-8f34-0dc952b71fc2"},"source":["!wget -nc http://files.fast.ai/data/aclImdb.tgz \n","!tar -xzf aclImdb.tgz"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File ‘aclImdb.tgz’ already there; not retrieving.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"0Giyi5Rv_NIm"},"source":["## Carregando o dataset\n","\n","Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n","\n","Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0HIN_xLI_TuT","executionInfo":{"status":"ok","timestamp":1631149680013,"user_tz":180,"elapsed":1852,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"77755d20-04e5-4eba-e517-87f64b432522"},"source":["import os\n","import random\n","\n","\n","def load_texts(folder):\n","    texts = []\n","    for path in os.listdir(folder):\n","        with open(os.path.join(folder, path)) as f:\n","            texts.append(f.read())\n","    return texts\n","\n","x_train_pos = load_texts('aclImdb/train/pos')\n","x_train_neg = load_texts('aclImdb/train/neg')\n","x_test_pos = load_texts('aclImdb/test/pos')\n","x_test_neg = load_texts('aclImdb/test/neg')\n","\n","x_train = x_train_pos + x_train_neg\n","x_test = x_test_pos + x_test_neg\n","y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n","y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n","\n","# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n","c = list(zip(x_train, y_train))\n","random.shuffle(c)\n","x_train, y_train = zip(*c)\n","\n","n_train = int(0.8 * len(x_train))\n","\n","x_valid = x_train[n_train:]\n","y_valid = y_train[n_train:]\n","x_train = x_train[:n_train]\n","y_train = y_train[:n_train]\n","\n","print(len(x_train), 'amostras de treino.')\n","print(len(x_valid), 'amostras de desenvolvimento.')\n","print(len(x_test), 'amostras de teste.')\n","\n","print('3 primeiras amostras treino:')\n","for x, y in zip(x_train[:3], y_train[:3]):\n","    print(y, x[:100])\n","\n","print('3 últimas amostras treino:')\n","for x, y in zip(x_train[-3:], y_train[-3:]):\n","    print(y, x[:100])\n","\n","print('3 primeiras amostras validação:')\n","for x, y in zip(x_valid[:3], y_test[:3]):\n","    print(y, x[:100])\n","\n","print('3 últimas amostras validação:')\n","for x, y in zip(x_valid[-3:], y_valid[-3:]):\n","    print(y, x[:100])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["20000 amostras de treino.\n","5000 amostras de desenvolvimento.\n","25000 amostras de teste.\n","3 primeiras amostras treino:\n","False Literally every aspect of this science-fiction low-budget flick falls under the categories that have\n","False I loved the first Little Mermaid. I know the songs, I love the characters and I love the story. I ca\n","True ... and I DO mean it. If not literally (after all, I have not seen every movie ever created!), at le\n","3 últimas amostras treino:\n","True This movie brings back many memories of the classic cinema of old, where actors didn't have to take \n","False To say Funky Forest: The First Contact is a bad movie is an understatement of incredible proportions\n","False This apology for a movie is about absolutely nothing! Rachel Griffiths must have needed the money. T\n","3 primeiras amostras validação:\n","True Sammi, Curr a metal rock god, they tried to stop him, they tried to ban him, the tried to censor his\n","True This movie was very very mediocre and very very gory. everyone left their acting lessons at home and\n","True This movie is hilarious, not in good way. The fights are awfully bad done, while sometimes they will\n","3 últimas amostras validação:\n","False I enjoy quality crapness, and this ranks up there with some of the finest. the cg is out of this wor\n","True (SPOILERS included) This film surely is the best Amicus production I've seen so far (even though I s\n","True I remember watching this film on Saturday afternoon TV in the 1950s or 60s. It was well presented bu\n"]}]},{"cell_type":"markdown","metadata":{"id":"N4VZ6c-Rzpgd"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"SgQKGxJa14XH"},"source":["import os\n","import random\n","\n","from collections import Counter\n","import numpy as np\n","import torch\n","\n","from re import findall, sub\n","import string\n","\n","from bs4 import BeautifulSoup\n","\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"acNLq4u8zuvN"},"source":["# text2token"]},{"cell_type":"code","metadata":{"id":"S_N-426i050-"},"source":["class text2token():\n","    \n","    def __init__(self, train_texts, test_texts, valid_texts,\n","                 mode = \"BOW\", boolean=False, max_size=None, stopwords = [],\n","                 unk=False, remove_number=None, log_e=False):      \n","        \n","        self.max_size = max_size\n","        self.boolean = boolean\n","        self.stopwords = stopwords\n","        self.unk = unk\n","        self.mode = mode\n","        self.remove_number = remove_number\n","        self.log_e = log_e\n","        \n","        splited_train_text = self.split(train_texts)\n","        self.createModel(splited_train_text)\n","        \n","        self.train_vector = self.tokenizer(splited_train_text)\n","        \n","        splited_test_text = self.split(test_texts)\n","        self.test_vector = self.tokenizer(splited_test_text)\n","        \n","        splited_valid_text = self.split(valid_texts)\n","        self.valid_vector = self.tokenizer(splited_valid_text)\n","        \n","    \n","# SPLIT ============================================================================================ \n","    \n","    def split(self, texts):\n","        \n","        tokenized_texts = []\n","        \n","        \n","        \n","        for text in texts:\n","            \n","            text = text.lower()\n","            \n","            # Remove as tags HTLM\n","            text = BeautifulSoup(text, \"lxml\").text\n","            \n","            # Remove os caracteres especiais\n","            # text = sub('[^A-Za-z\\s]+', ' ', text)\n","\n","            # Remove os números\n","            text = sub(r'[0-9+]', ' ', text)\n","            \n","            # Remove alguns pontos e separa\n","            text_splited = findall(r'\\w+|[^?\\-!.,:;\"\\'/><\\s]', text)\n","            # text_splited = findall(r\"[\\w']+|[!#$%&()*+,-./:;<=>?@[\\]^_{|}~]\", text) ->  faz diferença?\n","            \n","            if self.remove_number:\n","                # Remove palavras com menos de 2 caracteres\n","                text_splited = [word for word in text_splited if len(word) > self.remove_number]\n","\n","            \n","            tokenized_texts.append(text_splited)\n","            \n","        return tokenized_texts\n","    \n","# CREATETF ============================================================================================    \n","    \n","    def createTF(self, token_texts):\n","\n","        c = Counter()\n","\n","        for text in token_texts:\n","            c.update(set(text))\n","\n","        for stop_word in self.stopwords:\n","            if stop_word in c.keys():\n","                del c[stop_word]\n","\n","        return c\n","\n","# CREATEIDF ============================================================================================ \n","    \n","    def createIDF(self, counter):\n","\n","        idf = [self.len_corpus/count for token, count in counter.most_common(self.max_size)]\n","        if self.log_e:\n","            return np.log(idf)\n","        else:\n","            return np.log10(idf)\n","    \n","# CREATEMODEL ============================================================================================ \n","\n","    def createModel(self, tokenized_texts):\n","        \n","        if self.mode == \"BOW\":\n","\n","            c = Counter()\n","\n","            for text in tokenized_texts:\n","                c.update(text)\n","\n","            for stop_word in self.stopwords:\n","                if stop_word in c.keys():\n","                    del c[stop_word]\n","\n","            vocab = {element[0]: index for index, element in enumerate(c.most_common(self.max_size))}\n","            if self.unk:\n","                vocab['unknown'] = len(vocab)\n","                \n","            self.vocabulary = vocab\n","            \n","        if self.mode == \"TFIDF\":\n","            \n","            self.len_corpus = len(tokenized_texts)\n","            c = self.createTF(tokenized_texts)\n","            # print(counter)            \n","            \n","            vocab = {element[0]: index for index, element in enumerate(c.most_common(self.max_size))}\n","            self.vocabulary = vocab\n","            \n","            self.idf = self.createIDF(c)\n","            # print(self.idf)\n","        \n","# TOKENIZER ============================================================================================ \n","\n","    def tokenizer(self, texts):\n","        \n","        if self.mode == \"BOW\":\n","            token_texts = []\n","            if self.unk:\n","                unknown = self.vocabulary.get('unknown')\n","\n","            for i, text in enumerate(texts):\n","                bow_text = torch.zeros(len(self.vocabulary))\n","                counter = Counter(text)\n","\n","                if self.unk:\n","                    index = [self.vocabulary.get(key, unknown) for key in counter.keys()]\n","                else:\n","                    index = [self.vocabulary[key] for key in counter.keys() if key in self.vocabulary.keys()]\n","\n","                if self.boolean:\n","                    bow_text[index] = 1\n","                else:\n","                    values = [value for key, value in counter.items() if self.unk or key in self.vocabulary.keys()]\n","                    bow_text[index] = torch.Tensor(values)\n","\n","                token_texts.append(bow_text)\n","\n","            # print(transformed_texts)\n","            # print(type(bow_text))\n","            # return transformed_texts\n","            # return torch.cat(transformed_texts,dim=-1).float()\n","            return torch.vstack(token_texts).float()\n","            \n","            \n","        if self.mode == \"TFIDF\":\n","\n","            token_texts = []\n","\n","            for i, text in enumerate(texts):\n","\n","                tfidf_text = torch.zeros(len(self.vocabulary))\n","                c = Counter(text)\n","                index = []\n","                values = []\n","                for key, value in c.items():\n","                    if key in self.vocabulary.keys():\n","                        index.append(self.vocabulary[key])\n","                        values.append(value)\n","\n","                tfidf_text[index] = torch.Tensor(values)\n","                token_texts.append(tfidf_text * self.idf)\n","\n","            # return transformed_texts\n","            return torch.vstack(token_texts).float()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CIUroGpvCPoN"},"source":["train_texts = [\"Eu não não não gosto de jogar futebol\",\n","               \"eu eu gosto de jogar tenis\",\n","               \"gosto de jogar basquete, basquete!\",\n","               \"prefiro jogar volei volei do que futebol\"]\n","\n","valid_texts = [\"Não, não gosto de jogar volei, Prefiro jogar basquete\", \"gosto de jogar jogar videogame\"]\n","\n","test_texts = [\"gosto Jogar  jogar de tenis\", \"Volei? Prefiro jogar volei\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Ew9lfIcFHL_"},"source":["## Testes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ikoz4xcPEzCm","executionInfo":{"status":"ok","timestamp":1631149680500,"user_tz":180,"elapsed":17,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"1cd8477a-27d5-404c-e284-6e2d86f3fade"},"source":["# Frequencia, sem max_size, mode=Bag Of Words, sem remove_number, sem usar 'unknown'\n","print(\"Bag of Words -> vocab = máximo, não usar unknown, não remover palavras pequenas\")\n","t2t_a = text2token(train_texts, test_texts, valid_texts, mode = \"BOW\",\n","                 boolean=False, max_size=None, unk=False, remove_number=None)\n","\n","for i,m in enumerate(t2t_a.vocabulary):\n","    print(f'{m} : {i}')\n","    \n","print(\"\\n\\nBag of Words -> vocab = máximo, não usar unknown, remover palavras com duas ou menos letras\")\n","# Frequencia, sem max_size, mode=Bag Of Words, remove_number = 2, sem usar 'unknown'\n","t2t_b = text2token(train_texts, test_texts, valid_texts, mode = \"BOW\",\n","                 boolean=False, max_size=None, unk=False, remove_number=2)\n","\n","for i,m in enumerate(t2t_b.vocabulary):\n","    print(f'{m} : {i}')\n","    \n","print(\"\\n\\nBag of Words -> vocab = máximo, usar unknown, não remover palavras pequenas\")\n","\n","# Frequencia, sem max_size, mode=Bag Of Words, sem usar remove_number, usando 'unknown'\n","t2t_c = text2token(train_texts, test_texts, valid_texts, mode = \"BOW\",\n","                 boolean=False, max_size=None, unk=True, remove_number=None)\n","\n","for i,m in enumerate(t2t_c.vocabulary):\n","    print(f'{m} : {i}')\n","    \n","print(\"\\n\\nBag of Words -> vocab = 5 palavras mais frequentes, não usar unkown, não remover palavras pequenas\")\n","\n","# Frequencia, max_size = 5, mode=Bag Of Words, sem usar remove_number, sem usar 'unknown'\n","t2t_d = text2token(train_texts, test_texts, valid_texts, mode = \"BOW\",\n","                 boolean=False, max_size=5, unk=False, remove_number=None)\n","\n","for i,m in enumerate(t2t_d.vocabulary):\n","    print(f'{m} : {i}')\n","    \n","print(\"\\n\\nBag of Words -> vocab = 5 palavras mais frequentes, não usar unkown, remover palavras com duas ou menos letras\")\n","\n","# Frequencia, max_size = 5, mode=Bag Of Words, sem usar remove_number, sem usar 'unknown'\n","t2t_e = text2token(train_texts, test_texts, valid_texts, mode = \"BOW\",\n","                 boolean=False, max_size=5, unk=False, remove_number=2)\n","\n","for i,m in enumerate(t2t_e.vocabulary):\n","    print(f'{m} : {i}')\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bag of Words -> vocab = máximo, não usar unknown, não remover palavras pequenas\n","jogar : 0\n","eu : 1\n","não : 2\n","gosto : 3\n","de : 4\n","futebol : 5\n","basquete : 6\n","volei : 7\n","tenis : 8\n","prefiro : 9\n","do : 10\n","que : 11\n","\n","\n","Bag of Words -> vocab = máximo, não usar unknown, remover palavras com duas ou menos letras\n","jogar : 0\n","não : 1\n","gosto : 2\n","futebol : 3\n","basquete : 4\n","volei : 5\n","tenis : 6\n","prefiro : 7\n","que : 8\n","\n","\n","Bag of Words -> vocab = máximo, usar unknown, não remover palavras pequenas\n","jogar : 0\n","eu : 1\n","não : 2\n","gosto : 3\n","de : 4\n","futebol : 5\n","basquete : 6\n","volei : 7\n","tenis : 8\n","prefiro : 9\n","do : 10\n","que : 11\n","unknown : 12\n","\n","\n","Bag of Words -> vocab = 5 palavras mais frequentes, não usar unkown, não remover palavras pequenas\n","jogar : 0\n","eu : 1\n","não : 2\n","gosto : 3\n","de : 4\n","\n","\n","Bag of Words -> vocab = 5 palavras mais frequentes, não usar unkown, remover palavras com duas ou menos letras\n","jogar : 0\n","não : 1\n","gosto : 2\n","futebol : 3\n","basquete : 4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQiy348WE0dL","executionInfo":{"status":"ok","timestamp":1631149680836,"user_tz":180,"elapsed":347,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"cd5081fe-fe90-4abc-fe58-464fb5803e69"},"source":["t2t_freq = text2token(train_texts, test_texts, valid_texts, mode = \"BOW\",\n","                 boolean=False, max_size=None, unk=False, remove_number=2)\n","\n","print(t2t_freq.vocabulary)\n","\n","print(\"\\n\\n\")\n","for e in t2t_freq.train_vector:\n","    print(e)\n","    \n","print(\"\\n\\n\")\n","\n","for e in t2t_freq.valid_vector:\n","    print(e)\n","    \n","print(\"\\n\\n\")\n","    \n","for e in t2t_freq.test_vector:\n","    print(e)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'jogar': 0, 'não': 1, 'gosto': 2, 'futebol': 3, 'basquete': 4, 'volei': 5, 'tenis': 6, 'prefiro': 7, 'que': 8}\n","\n","\n","\n","tensor([1., 3., 1., 1., 0., 0., 0., 0., 0.])\n","tensor([1., 0., 1., 0., 0., 0., 1., 0., 0.])\n","tensor([1., 0., 1., 0., 2., 0., 0., 0., 0.])\n","tensor([1., 0., 0., 1., 0., 2., 0., 1., 1.])\n","\n","\n","\n","tensor([2., 2., 1., 0., 1., 1., 0., 1., 0.])\n","tensor([2., 0., 1., 0., 0., 0., 0., 0., 0.])\n","\n","\n","\n","tensor([2., 0., 1., 0., 0., 0., 1., 0., 0.])\n","tensor([1., 0., 0., 0., 0., 2., 0., 1., 0.])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nS42KW4mE3j9","executionInfo":{"status":"ok","timestamp":1631149680837,"user_tz":180,"elapsed":16,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"69dae603-0238-4cad-9a68-3b400cf6a033"},"source":["t2t_bool = text2token(train_texts, test_texts, valid_texts, mode = \"BOW\",\n","                 boolean=True, max_size=None, unk=False, remove_number=2)\n","\n","print(t2t_bool.vocabulary)\n","\n","print(\"\\n\\n\")\n","for e in t2t_bool.train_vector:\n","    print(e)\n","    \n","print(\"\\n\\n\")\n","\n","for e in t2t_bool.valid_vector:\n","    print(e)\n","    \n","print(\"\\n\\n\")\n","    \n","for e in t2t_bool.test_vector:\n","    print(e)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'jogar': 0, 'não': 1, 'gosto': 2, 'futebol': 3, 'basquete': 4, 'volei': 5, 'tenis': 6, 'prefiro': 7, 'que': 8}\n","\n","\n","\n","tensor([1., 1., 1., 1., 0., 0., 0., 0., 0.])\n","tensor([1., 0., 1., 0., 0., 0., 1., 0., 0.])\n","tensor([1., 0., 1., 0., 1., 0., 0., 0., 0.])\n","tensor([1., 0., 0., 1., 0., 1., 0., 1., 1.])\n","\n","\n","\n","tensor([1., 1., 1., 0., 1., 1., 0., 1., 0.])\n","tensor([1., 0., 1., 0., 0., 0., 0., 0., 0.])\n","\n","\n","\n","tensor([1., 0., 1., 0., 0., 0., 1., 0., 0.])\n","tensor([1., 0., 0., 0., 0., 1., 0., 1., 0.])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9wBziFkaE6mH","executionInfo":{"status":"ok","timestamp":1631149680838,"user_tz":180,"elapsed":13,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"9d5bb48d-ae9c-4ab7-b39d-e44ced6e8aa5"},"source":["for i in range(len(t2t_bool.train_vector)):\n","    print(f'{t2t_freq.train_vector[i]} : {t2t_bool.train_vector[i]}')\n","    \n","print(\"\\n\\n\")\n","\n","for i in range(len(t2t_bool.valid_vector)):\n","    print(f'{t2t_freq.valid_vector[i]} : {t2t_bool.valid_vector[i]}')\n","    \n","print(\"\\n\\n\")\n","\n","for i in range(len(t2t_bool.test_vector)):\n","    print(f'{t2t_freq.test_vector[i]} : {t2t_bool.test_vector[i]}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 3., 1., 1., 0., 0., 0., 0., 0.]) : tensor([1., 1., 1., 1., 0., 0., 0., 0., 0.])\n","tensor([1., 0., 1., 0., 0., 0., 1., 0., 0.]) : tensor([1., 0., 1., 0., 0., 0., 1., 0., 0.])\n","tensor([1., 0., 1., 0., 2., 0., 0., 0., 0.]) : tensor([1., 0., 1., 0., 1., 0., 0., 0., 0.])\n","tensor([1., 0., 0., 1., 0., 2., 0., 1., 1.]) : tensor([1., 0., 0., 1., 0., 1., 0., 1., 1.])\n","\n","\n","\n","tensor([2., 2., 1., 0., 1., 1., 0., 1., 0.]) : tensor([1., 1., 1., 0., 1., 1., 0., 1., 0.])\n","tensor([2., 0., 1., 0., 0., 0., 0., 0., 0.]) : tensor([1., 0., 1., 0., 0., 0., 0., 0., 0.])\n","\n","\n","\n","tensor([2., 0., 1., 0., 0., 0., 1., 0., 0.]) : tensor([1., 0., 1., 0., 0., 0., 1., 0., 0.])\n","tensor([1., 0., 0., 0., 0., 2., 0., 1., 0.]) : tensor([1., 0., 0., 0., 0., 1., 0., 1., 0.])\n"]}]},{"cell_type":"markdown","metadata":{"id":"zPaK50r1FARA"},"source":["TF-IDF\n","\n","$$\\text{TF-IDF}(t, d, C) = tf(t, d) * idf(t, C)$$\n","\n","Abrindo as funções definidas na equação principal:\n","- $tf(t, d) = \\text{numero de vezes que o termo t aparece no documento d}$\n","- $idf(t, C) = \\log{\\frac{C}{n_t}}$ \n","\n","Onde: \n","\n","- $\\text{t: token ou termo;}$\n","- $\\text{d: documento(frase, enunciado, etc);}$\n","- $\\text{C: Corpus (conjunto de documentos).}$\n","- $n_t\\text{: numero de documentos onde o token t aparece.}$"]},{"cell_type":"markdown","metadata":{"id":"pLts1S_EE9_-"},"source":["train_texts = [\"Eu não não não gosto de jogar futebol\",\n","               \"eu eu gosto de jogar tenis\",\n","               \"gosto de jogar basquete, basquete!\",\n","               \"prefiro jogar volei volei do que futebol\"]\n","\n","valid_texts = [\"Não, não gosto de jogar volei, Prefiro jogar basquete\", \"gosto de jogar jogar videogame\"]\n","\n","test_texts = [\"gosto Jogar  jogar de tenis\", \"Volei? Prefiro jogar volei\"]\n","\n","* Uma vez que o cálculo de **idf** depende apenas do dataset de treino, a variável c=4, pois, tem-se 4 textos\n","\n","Token | nt|log\n","-------|--------|----\n","jogar|4|log(4/4)\n","gosto|3|log(4/3)\n","futebol|2|log(4/2)\n","não|1|log(4/1)\n","tenis|1|log(4/1)\n","basquete|1|log(4/1)\n","que|1|log(4/1)\n","prefiro|1|log(4/1)\n","volei|1|log(4/1)\n","\n","* tf para \"gosto Jogar  jogar de tenis\"\n","\n","Token | tf\n","-------|--------\n","jogar|2\n","gosto|1\n","futebol|0\n","não|0\n","tenis|1\n","basquete|0\n","que|0\n","prefiro|0\n","volei|0\n","\n","* tf para \"Volei? Prefiro jogar volei\"\n","\n","Token | tf\n","-------|--------\n","jogar|1\n","gosto|0\n","futebol|0\n","não|0\n","tenis|0\n","basquete|0\n","que|0\n","prefiro|1\n","volei|2"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kh7MHvuXFBwh","executionInfo":{"status":"ok","timestamp":1631150330162,"user_tz":180,"elapsed":599,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"047bafc4-9481-4744-d558-c73609bded88"},"source":["t2t = text2token(train_texts, test_texts, valid_texts, mode = \"TFIDF\",\n","                 boolean=False, max_size=None, unk=False, remove_number=2, log_e=False)\n","\n","logs = [np.log10(4/4),np.log10(4/3),np.log10(4/2),np.log10(4/1),\n","        np.log10(4/1),np.log10(4/1),np.log10(4/1),np.log10(4/1),\n","        np.log10(4/1)]\n","\n","for i, m in enumerate(t2t.vocabulary):\n","    print(f'{m} : {t2t.idf[i]} : {logs[i]} : {t2t.idf[i]-logs[i]}')\n","    # print(f'{m} : {t2t.idf[i]}')\n","    \n","    \n","\n","\n","print(\"\\n\\n\")\n","for e in t2t.train_vector:\n","    print(e)\n","    \n","print(\"\\n\\n\")\n","\n","for e in t2t.valid_vector:\n","    print(e)\n","    \n","print(\"\\n\\n\")\n","    \n","primeira = [2,1,0,0,1,0,0,0,0]\n","segunda = [1,0,0,0,0,0,0,1,2]\n","\n","\n","for i, m in enumerate(t2t.vocabulary):\n","    print(f'{m} : {t2t.test_vector[0][i]} : {logs[i]*primeira[i]} : {t2t.test_vector[0][i]-logs[i]*primeira[i]}')\n","    # print(f'{m} : {t2t.idf[i]}')\n","print(\"\\n\\n\")\n","for i, m in enumerate(t2t.vocabulary):\n","    print(f'{m} : {t2t.test_vector[1][i]} : {logs[i]*segunda[i]} : {t2t.test_vector[1][i]-logs[i]*segunda[i]}')\n","    # print(f'{m} : {t2t.idf[i]}')"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["jogar : 0.0 : 0.0 : 0.0\n","gosto : 0.12493873660829992 : 0.12493873660829992 : 0.0\n","futebol : 0.3010299956639812 : 0.3010299956639812 : 0.0\n","não : 0.6020599913279624 : 0.6020599913279624 : 0.0\n","tenis : 0.6020599913279624 : 0.6020599913279624 : 0.0\n","basquete : 0.6020599913279624 : 0.6020599913279624 : 0.0\n","que : 0.6020599913279624 : 0.6020599913279624 : 0.0\n","prefiro : 0.6020599913279624 : 0.6020599913279624 : 0.0\n","volei : 0.6020599913279624 : 0.6020599913279624 : 0.0\n","\n","\n","\n","tensor([0.0000, 0.1249, 0.3010, 1.8062, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n","tensor([0.0000, 0.1249, 0.0000, 0.0000, 0.6021, 0.0000, 0.0000, 0.0000, 0.0000])\n","tensor([0.0000, 0.1249, 0.0000, 0.0000, 0.0000, 1.2041, 0.0000, 0.0000, 0.0000])\n","tensor([0.0000, 0.0000, 0.3010, 0.0000, 0.0000, 0.0000, 0.6021, 0.6021, 1.2041])\n","\n","\n","\n","tensor([0.0000, 0.1249, 0.0000, 1.2041, 0.0000, 0.6021, 0.0000, 0.6021, 0.6021])\n","tensor([0.0000, 0.1249, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n","\n","\n","\n","jogar : 0.0 : 0.0 : 0.0\n","gosto : 0.1249387338757515 : 0.12493873660829992 : 0.0\n","futebol : 0.0 : 0.0 : 0.0\n","não : 0.0 : 0.0 : 0.0\n","tenis : 0.6020600199699402 : 0.6020599913279624 : 0.0\n","basquete : 0.0 : 0.0 : 0.0\n","que : 0.0 : 0.0 : 0.0\n","prefiro : 0.0 : 0.0 : 0.0\n","volei : 0.0 : 0.0 : 0.0\n","\n","\n","\n","jogar : 0.0 : 0.0 : 0.0\n","gosto : 0.0 : 0.0 : 0.0\n","futebol : 0.0 : 0.0 : 0.0\n","não : 0.0 : 0.0 : 0.0\n","tenis : 0.0 : 0.0 : 0.0\n","basquete : 0.0 : 0.0 : 0.0\n","que : 0.0 : 0.0 : 0.0\n","prefiro : 0.6020600199699402 : 0.6020599913279624 : 0.0\n","volei : 1.2041200399398804 : 1.2041199826559248 : 0.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"LYmt4sU-zyDA"},"source":["# RNA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDUmbW2E0-nl","executionInfo":{"status":"ok","timestamp":1631149681219,"user_tz":180,"elapsed":386,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"980d623f-46f2-485e-c709-87938ee9d6c3"},"source":["# CLASS DATASETCREATOR ======================================================\n","\n","class datasetCreator(Dataset):\n","    def __init__(self, x, y):\n","        self.x = x\n","        self.y = y\n","\n","    def __len__(self) -> int:\n","        return len(self.x)\n","\n","    def __getitem__(self, idx):\n","        x = self.x[idx]\n","        y = self.y[idx]\n","        return x, y\n","\n","# NN_CREATOR ======================================================\n","\n","class NN_CREATOR(torch.nn.Module):\n","    def __init__(self, input_layer):\n","        super().__init__()\n","        self.dense = torch.nn.Sequential(\n","            \n","            torch.nn.Linear(input_layer, 1000),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Linear(1000, 256),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Linear(256, 64),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Linear(64, 32),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Linear(32, 16),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Linear(16, 1),\n","            torch.nn.Sigmoid()\n","        )\n","    \n","    def forward(self, x):\n","        x = self.dense(x)\n","        return x\n","\n","# NN_ENTRADA FUNCTION ======================================================\n","\n","def NN_entrada(vectorized_texts_train, y_train,\n","               vectorized_texts_valid, y_valid,\n","               vectorized_texts_test, y_test,\n","               vectorizer, batch_size, shuffle):\n","\n","    train_dataset = datasetCreator(vectorized_texts_train, torch.Tensor(y_train).reshape(-1,1))\n","    valid_dataset = datasetCreator(vectorized_texts_valid, torch.Tensor(y_valid).reshape(-1,1))\n","    test_dataset  = datasetCreator(vectorized_texts_test, torch.Tensor(y_test).reshape(-1,1))\n","\n","    trainLoad = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    validLoad = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","    testLoad  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return trainLoad, validLoad, testLoad, vectorizer\n","\n","# TRAIN FUNCTION ======================================================\n","\n","def trainModel(model, train, valid, criterion, optimizer,\n","              max_size, filename_save, batch_size=64, n_epochs=10):\n","  \n","    best_valid_loss = 999999999\n","    best_epoch = 0\n","\n","    for i in range(n_epochs):\n","        accumulated_loss = 0\n","        model.train()\n","\n","        for x_train, y_train in train:\n","            x_train = x_train.to(device)\n","            y_train = y_train.to(device)\n","            outputs = model(x_train)\n","            batch_loss = criterion(outputs, y_train)\n","\n","            optimizer.zero_grad()\n","            batch_loss.backward()\n","            optimizer.step()\n","            accumulated_loss += batch_loss.item()\n","\n","        train_loss = accumulated_loss / len(train.dataset)\n","    \n","        accumulated_loss = 0\n","        accumulated_accuracy = 0\n","        model.eval()\n","        \n","        # n_epochs vezes para treinar o modelo\n","        with torch.no_grad():\n","            for x_valid, y_valid in valid:\n","\n","                x_valid = x_valid.to(device)\n","                y_valid = y_valid.to(device)\n","\n","                # Predict\n","                outputs = model(x_valid)\n","\n","                # Loss\n","                batch_loss = criterion(outputs, y_valid)\n","                preds = outputs > 0.5\n","\n","                # Accuracy\n","                batch_accuracy = (preds == y_valid).sum()\n","                accumulated_loss += batch_loss\n","                accumulated_accuracy += batch_accuracy\n","\n","        valid_loss = accumulated_loss / len(valid.dataset)\n","        valid_acc = accumulated_accuracy / len(valid.dataset)\n","        \n","\n","        # Salvando o melhor modelo de acordo com a loss de validação\n","        if valid_loss < best_valid_loss:\n","            torch.save(model.state_dict(), filename_save + '.pt')\n","            best_valid_loss = valid_loss\n","            best_epoch = i\n","            print(f'Epoch = {i:d}/{n_epochs - 1:d}, Train Loss = {train_loss:.6f}, Valid Loss = {valid_loss:.6f}, Valid Acc = {valid_acc*100:.1f}% - BEST MODEL')\n","\n","        else:\n","            print(f'Epoch = {i:d}/{n_epochs - 1:d}, Train Loss = {train_loss:.6f}, Valid Loss = {valid_loss:.6f}, Valid Acc = {valid_acc*100:.1f}%')\n","\n","    return model\n","\n","# PREDICT FUNCTION ======================================================\n","\n","def predict(model, state_dict, test):\n","    accumulated_accuracy = 0\n","    model.load_state_dict(torch.load(state_dict + '.pt'))\n","    model.eval()\n","    with torch.no_grad():\n","        for x_test, y_test in test:\n","            x_test = x_test.to(device)\n","            y_test = y_test.to(device)\n","\n","            # predict da rede\n","            outputs = model(x_test)\n","\n","            # calcula a perda\n","            batch_loss = criterion(outputs, y_test)\n","            preds = outputs > 0.5\n","\n","            # calcula a acurácia\n","            batch_accuracy = (preds == y_test).sum()\n","            accumulated_accuracy += batch_accuracy\n","\n","    test_acc = accumulated_accuracy / len(test.dataset)\n","    test_acc *= 100\n","    print(f'Accuracy: {test_acc:.3f} %')\n","\n","if torch.cuda.is_available(): \n","    dev = \"cuda:0\"\n","    print(torch. cuda. get_device_name(dev))\n","else: \n","    dev = \"cpu\" \n","print(dev)\n","device = torch.device(dev)\n","\n","# Créditos ao aluno Pedro Gabriel Gengo Lourenço, em que me inspirei em algumas partes\n","# no momento estou estudando para solucionar o que está defasado."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla K80\n","cuda:0\n"]}]},{"cell_type":"markdown","metadata":{"id":"L5Wfu8H2vaK_"},"source":["# Treinando para BOW booleano"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-k28fWPM1Nx0","executionInfo":{"status":"ok","timestamp":1631149691154,"user_tz":180,"elapsed":9938,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"89159f44-4769-4958-898a-058719606cab"},"source":["max_size = 3000\n","batch_size = 16\n","n_epochs = 15\n","learningRate = 0.02\n","save_filename = 'BOW_BOOLEANO'\n","\n","mlp_BOW_BOOLEANO = NN_CREATOR(max_size)\n","mlp_BOW_BOOLEANO.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NN_CREATOR(\n","  (dense): Sequential(\n","    (0): Linear(in_features=3000, out_features=1000, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=1000, out_features=256, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=256, out_features=64, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=64, out_features=32, bias=True)\n","    (7): ReLU()\n","    (8): Linear(in_features=32, out_features=16, bias=True)\n","    (9): ReLU()\n","    (10): Linear(in_features=16, out_features=1, bias=True)\n","    (11): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"MefZQj3C1QHZ"},"source":["criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(mlp_BOW_BOOLEANO.parameters(), lr=learningRate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArL6sn450wBT"},"source":["data1 = text2token(train_texts=x_train, test_texts=x_test, valid_texts=x_valid, boolean=True, max_size = max_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RRvijh-I1RkN","executionInfo":{"status":"ok","timestamp":1631149779104,"user_tz":180,"elapsed":58333,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"49c71770-60a8-4818-cefd-49d5bee6efaf"},"source":["vectorized_train, vectorized_valid, vectorized_test, _ = NN_entrada(data1.train_vector, y_train,\n","                                                                    data1.valid_vector, y_valid,\n","                                                                    data1.test_vector, y_test,\n","                                                                    data1, batch_size, True)\n","\n","_ = trainModel(mlp_BOW_BOOLEANO, vectorized_train, vectorized_valid, criterion, optimizer,\n","              max_size, save_filename, batch_size, n_epochs=n_epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch = 0/14, Train Loss = 0.043327, Valid Loss = 0.043383, Valid Acc = 50.7% - BEST MODEL\n","Epoch = 1/14, Train Loss = 0.043325, Valid Loss = 0.043382, Valid Acc = 50.7% - BEST MODEL\n","Epoch = 2/14, Train Loss = 0.043317, Valid Loss = 0.043398, Valid Acc = 49.3%\n","Epoch = 3/14, Train Loss = 0.043293, Valid Loss = 0.043344, Valid Acc = 50.7% - BEST MODEL\n","Epoch = 4/14, Train Loss = 0.043227, Valid Loss = 0.043183, Valid Acc = 70.3% - BEST MODEL\n","Epoch = 5/14, Train Loss = 0.038904, Valid Loss = 0.026117, Valid Acc = 81.6% - BEST MODEL\n","Epoch = 6/14, Train Loss = 0.021783, Valid Loss = 0.020010, Valid Acc = 87.0% - BEST MODEL\n","Epoch = 7/14, Train Loss = 0.017613, Valid Loss = 0.020763, Valid Acc = 85.7%\n","Epoch = 8/14, Train Loss = 0.015040, Valid Loss = 0.019791, Valid Acc = 87.0% - BEST MODEL\n","Epoch = 9/14, Train Loss = 0.011609, Valid Loss = 0.021071, Valid Acc = 86.5%\n","Epoch = 10/14, Train Loss = 0.008112, Valid Loss = 0.023385, Valid Acc = 86.1%\n","Epoch = 11/14, Train Loss = 0.005111, Valid Loss = 0.025722, Valid Acc = 85.9%\n","Epoch = 12/14, Train Loss = 0.004143, Valid Loss = 0.027376, Valid Acc = 86.8%\n","Epoch = 13/14, Train Loss = 0.003055, Valid Loss = 0.029315, Valid Acc = 86.2%\n","Epoch = 14/14, Train Loss = 0.002039, Valid Loss = 0.057679, Valid Acc = 77.0%\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bOqcqeV2oY_","executionInfo":{"status":"ok","timestamp":1631149780670,"user_tz":180,"elapsed":1572,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"0eb81fdf-6d08-485c-83e1-2b9ebbdb55bb"},"source":["predict(mlp_BOW_BOOLEANO, save_filename, vectorized_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 86.896 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"fsZlBBXivkug"},"source":["# Treinando para BOW frequencia"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UG_9hCUmvkuh","executionInfo":{"status":"ok","timestamp":1631149780671,"user_tz":180,"elapsed":8,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"1f6b301e-1a2d-4a82-a4a8-82ad56dff2c8"},"source":["max_size = 3000\n","batch_size = 32\n","n_epochs = 15\n","learningRate = 0.05\n","save_filename = 'BOW_FREQ'\n","\n","mlp_BOW_FREQ = NN_CREATOR(max_size)\n","mlp_BOW_FREQ.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NN_CREATOR(\n","  (dense): Sequential(\n","    (0): Linear(in_features=3000, out_features=1000, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=1000, out_features=256, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=256, out_features=64, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=64, out_features=32, bias=True)\n","    (7): ReLU()\n","    (8): Linear(in_features=32, out_features=16, bias=True)\n","    (9): ReLU()\n","    (10): Linear(in_features=16, out_features=1, bias=True)\n","    (11): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"ATbi2gsZvku0"},"source":["criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(mlp_BOW_FREQ.parameters(), lr=learningRate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6FFEBDA4pgy"},"source":["data2 = text2token(train_texts=x_train, test_texts=x_test, valid_texts=x_valid, boolean=False, max_size = max_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ciAb2j1vku1","executionInfo":{"status":"ok","timestamp":1631149844908,"user_tz":180,"elapsed":32453,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"cc865720-fae6-449b-bbe2-413ad27a20b0"},"source":["vectorized_train, vectorized_valid, vectorized_test, _ = NN_entrada(data2.train_vector, y_train,\n","                                                                    data2.valid_vector, y_valid,\n","                                                                    data2.test_vector, y_test,\n","                                                                    data2, batch_size, True)\n","\n","_ = trainModel(mlp_BOW_FREQ, vectorized_train, vectorized_valid, criterion, optimizer,\n","              max_size, save_filename, batch_size, n_epochs=n_epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch = 0/14, Train Loss = 0.021633, Valid Loss = 0.021642, Valid Acc = 58.8% - BEST MODEL\n","Epoch = 1/14, Train Loss = 0.019735, Valid Loss = 0.024910, Valid Acc = 59.1%\n","Epoch = 2/14, Train Loss = 0.015254, Valid Loss = 0.012847, Valid Acc = 82.1% - BEST MODEL\n","Epoch = 3/14, Train Loss = 0.012947, Valid Loss = 0.011460, Valid Acc = 84.0% - BEST MODEL\n","Epoch = 4/14, Train Loss = 0.011619, Valid Loss = 0.011027, Valid Acc = 84.5% - BEST MODEL\n","Epoch = 5/14, Train Loss = 0.010880, Valid Loss = 0.015243, Valid Acc = 76.3%\n","Epoch = 6/14, Train Loss = 0.010157, Valid Loss = 0.010492, Valid Acc = 86.1% - BEST MODEL\n","Epoch = 7/14, Train Loss = 0.009452, Valid Loss = 0.013342, Valid Acc = 81.6%\n","Epoch = 8/14, Train Loss = 0.008925, Valid Loss = 0.010110, Valid Acc = 86.5% - BEST MODEL\n","Epoch = 9/14, Train Loss = 0.008588, Valid Loss = 0.011047, Valid Acc = 84.5%\n","Epoch = 10/14, Train Loss = 0.008118, Valid Loss = 0.010331, Valid Acc = 86.1%\n","Epoch = 11/14, Train Loss = 0.007963, Valid Loss = 0.014664, Valid Acc = 81.8%\n","Epoch = 12/14, Train Loss = 0.007250, Valid Loss = 0.010908, Valid Acc = 85.1%\n","Epoch = 13/14, Train Loss = 0.007138, Valid Loss = 0.010164, Valid Acc = 86.8%\n","Epoch = 14/14, Train Loss = 0.006821, Valid Loss = 0.011693, Valid Acc = 85.0%\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNX450Mgvku2","executionInfo":{"status":"ok","timestamp":1631149845774,"user_tz":180,"elapsed":891,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"55824481-1fd2-4e9f-b549-8069dfc3ace0"},"source":["predict(mlp_BOW_FREQ, save_filename, vectorized_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 86.740 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"zjqNmNxUw2lW"},"source":["# Treinando para TF-IDF"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mnoQn0xw2lt","executionInfo":{"status":"ok","timestamp":1631149845775,"user_tz":180,"elapsed":20,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"6be5ffdf-b882-4b0b-d7be-afc3f31f2851"},"source":["max_size = 3000\n","batch_size = 200\n","n_epochs = 15\n","learningRate = 0.2\n","save_filename = 'TFIDF'\n","\n","mlp_TFIDF = NN_CREATOR(max_size)\n","mlp_TFIDF.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NN_CREATOR(\n","  (dense): Sequential(\n","    (0): Linear(in_features=3000, out_features=1000, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=1000, out_features=256, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=256, out_features=64, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=64, out_features=32, bias=True)\n","    (7): ReLU()\n","    (8): Linear(in_features=32, out_features=16, bias=True)\n","    (9): ReLU()\n","    (10): Linear(in_features=16, out_features=1, bias=True)\n","    (11): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"1ckJU8lJw2lu"},"source":["criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(mlp_TFIDF.parameters(), lr=learningRate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fN8N9um94uu8"},"source":["data3 = text2token(train_texts=x_train, test_texts=x_test, valid_texts=x_valid, mode=\"TFIDF\", max_size = max_size, log_e=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ibbPhgjkDAJw"},"source":["def __init__(self, train_texts, test_texts, valid_texts,\n","                 mode = \"BOW\", boolean=False, max_size=None, stopwords = [],\n","                 unk=False, remove_number=None, log_e=False): "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DpbXHLZQw2lv","executionInfo":{"status":"ok","timestamp":1631149895758,"user_tz":180,"elapsed":10558,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"cc430c5e-153e-49f8-8467-7270fc3b1efc"},"source":["vectorized_train, vectorized_valid, vectorized_test, _ = NN_entrada(data3.train_vector, y_train,\n","                                                                    data3.valid_vector, y_valid,\n","                                                                    data3.test_vector, y_test,\n","                                                                    data3, batch_size, True)\n","\n","_ = trainModel(mlp_TFIDF, vectorized_train, vectorized_valid, criterion, optimizer,\n","              max_size, save_filename, batch_size, n_epochs=n_epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch = 0/14, Train Loss = 0.003469, Valid Loss = 0.003465, Valid Acc = 49.4% - BEST MODEL\n","Epoch = 1/14, Train Loss = 0.003460, Valid Loss = 0.003452, Valid Acc = 52.1% - BEST MODEL\n","Epoch = 2/14, Train Loss = 0.003224, Valid Loss = 0.002253, Valid Acc = 83.4% - BEST MODEL\n","Epoch = 3/14, Train Loss = 0.001837, Valid Loss = 0.001614, Valid Acc = 87.1% - BEST MODEL\n","Epoch = 4/14, Train Loss = 0.001171, Valid Loss = 0.001593, Valid Acc = 87.4% - BEST MODEL\n","Epoch = 5/14, Train Loss = 0.000779, Valid Loss = 0.001729, Valid Acc = 87.5%\n","Epoch = 6/14, Train Loss = 0.000461, Valid Loss = 0.001937, Valid Acc = 86.6%\n","Epoch = 7/14, Train Loss = 0.000347, Valid Loss = 0.002162, Valid Acc = 87.1%\n","Epoch = 8/14, Train Loss = 0.000100, Valid Loss = 0.003398, Valid Acc = 85.9%\n","Epoch = 9/14, Train Loss = 0.000072, Valid Loss = 0.003041, Valid Acc = 86.6%\n","Epoch = 10/14, Train Loss = 0.000306, Valid Loss = 0.002816, Valid Acc = 86.9%\n","Epoch = 11/14, Train Loss = 0.000049, Valid Loss = 0.003368, Valid Acc = 86.5%\n","Epoch = 12/14, Train Loss = 0.000758, Valid Loss = 0.002494, Valid Acc = 86.4%\n","Epoch = 13/14, Train Loss = 0.000077, Valid Loss = 0.003035, Valid Acc = 86.4%\n","Epoch = 14/14, Train Loss = 0.000037, Valid Loss = 0.003240, Valid Acc = 86.9%\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTJSGvRMw2lw","executionInfo":{"status":"ok","timestamp":1631149896573,"user_tz":180,"elapsed":840,"user":{"displayName":"Guilherme Pereira Albino da Silva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07971392613373651702"}},"outputId":"5a20dada-7e6c-4428-89d5-ca2005e61f28"},"source":["predict(mlp_TFIDF, save_filename, vectorized_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 87.268 %\n"]}]}]}