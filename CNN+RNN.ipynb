{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import os  # when loading file paths\n","import pandas as pd  # for lookup in annotation file\n","import spacy  # for tokenizer\n","import torch\n","from torch.nn.utils.rnn import pad_sequence  # pad batch\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image  # Load img\n","import torchvision.transforms as transforms\n","\n","import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","\n","import os  # when loading file paths\n","import pandas as pd  # for lookup in annotation file\n","import spacy  # for tokenizer\n","import torch\n","from torch.nn.utils.rnn import pad_sequence  # pad batch\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image  # Load img\n","import torchvision.transforms as transforms\n","\n","import torch\n","import torch.nn as nn\n","import statistics\n","import torchvision.models as models\n","\n","import json\n","import h5py\n","import numpy as np\n","import random"],"metadata":{"id":"RpzA1FUzEYlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ghrlEk4meq_","executionInfo":{"status":"ok","timestamp":1658318396988,"user_tz":180,"elapsed":19793,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"ab328fd7-13af-469b-af52-5ba840da3268"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!wget https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/TEST_CAPTIONS_coco_5_cap_per_img.json\n","!wget https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/TEST_IMAGES_coco_5_cap_per_img.hdf5\n","!wget https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/VAL_CAPTIONS_coco_5_cap_per_img.json\n","!wget https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/VAL_IMAGES_coco_5_cap_per_img.hdf5\n","!wget https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/TRAIN_CAPTIONS_coco_5_cap_per_img.json\n","!wget https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/TRAIN_IMAGES_coco_5_cap_per_img.hdf5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvOEAfPSOO05","executionInfo":{"status":"ok","timestamp":1658318440015,"user_tz":180,"elapsed":42125,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"e78690d7-2d75-4718-b0cf-694559dea5bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-20 11:59:57--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/TEST_CAPTIONS_coco_5_cap_per_img.json\n","Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.215.128, 173.194.217.128, 173.194.218.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.215.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 289777 (283K) [application/json]\n","Saving to: ‘TEST_CAPTIONS_coco_5_cap_per_img.json’\n","\n","TEST_CAPTIONS_coco_ 100%[===================>] 282.99K  --.-KB/s    in 0.002s  \n","\n","2022-07-20 11:59:57 (111 MB/s) - ‘TEST_CAPTIONS_coco_5_cap_per_img.json’ saved [289777/289777]\n","\n","--2022-07-20 11:59:58--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/TEST_IMAGES_coco_5_cap_per_img.hdf5\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.123.128, 142.250.98.128, 142.251.107.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.123.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 205850720 (196M) [application/octet-stream]\n","Saving to: ‘TEST_IMAGES_coco_5_cap_per_img.hdf5’\n","\n","TEST_IMAGES_coco_5_ 100%[===================>] 196.31M   122MB/s    in 1.6s    \n","\n","2022-07-20 11:59:59 (122 MB/s) - ‘TEST_IMAGES_coco_5_cap_per_img.hdf5’ saved [205850720/205850720]\n","\n","--2022-07-20 11:59:59--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/VAL_CAPTIONS_coco_5_cap_per_img.json\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.193.128, 172.217.204.128, 172.253.123.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.193.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 293430 (287K) [application/json]\n","Saving to: ‘VAL_CAPTIONS_coco_5_cap_per_img.json’\n","\n","VAL_CAPTIONS_coco_5 100%[===================>] 286.55K  --.-KB/s    in 0.003s  \n","\n","2022-07-20 12:00:00 (86.6 MB/s) - ‘VAL_CAPTIONS_coco_5_cap_per_img.json’ saved [293430/293430]\n","\n","--2022-07-20 12:00:00--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/VAL_IMAGES_coco_5_cap_per_img.hdf5\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.12.128, 172.217.193.128, 172.217.204.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.12.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 206833760 (197M) [application/octet-stream]\n","Saving to: ‘VAL_IMAGES_coco_5_cap_per_img.hdf5’\n","\n","VAL_IMAGES_coco_5_c 100%[===================>] 197.25M   168MB/s    in 1.2s    \n","\n","2022-07-20 12:00:01 (168 MB/s) - ‘VAL_IMAGES_coco_5_cap_per_img.hdf5’ saved [206833760/206833760]\n","\n","--2022-07-20 12:00:01--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/TRAIN_CAPTIONS_coco_5_cap_per_img.json\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.107.128, 74.125.196.128, 74.125.31.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.107.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6565538 (6.3M) [application/json]\n","Saving to: ‘TRAIN_CAPTIONS_coco_5_cap_per_img.json’\n","\n","TRAIN_CAPTIONS_coco 100%[===================>]   6.26M  --.-KB/s    in 0.09s   \n","\n","2022-07-20 12:00:01 (69.2 MB/s) - ‘TRAIN_CAPTIONS_coco_5_cap_per_img.json’ saved [6565538/6565538]\n","\n","--2022-07-20 12:00:01--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula10/TRAIN_IMAGES_coco_5_cap_per_img.hdf5\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.107.128, 74.125.196.128, 74.125.31.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.107.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4624222304 (4.3G) [application/octet-stream]\n","Saving to: ‘TRAIN_IMAGES_coco_5_cap_per_img.hdf5’\n","\n","TRAIN_IMAGES_coco_5 100%[===================>]   4.31G  49.3MB/s    in 38s     \n","\n","2022-07-20 12:00:39 (117 MB/s) - ‘TRAIN_IMAGES_coco_5_cap_per_img.hdf5’ saved [4624222304/4624222304]\n","\n"]}]},{"cell_type":"code","source":["spacy_eng = spacy.load(\"en_core_web_sm\")\n","\n","class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.itos)\n","\n","    @staticmethod\n","    def tokenizer_eng(text):\n","        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n","\n","    def build_vocabulary(self, sentence_list):\n","        frequencies = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            for word in self.tokenizer_eng(sentence):\n","\n","                if word not in frequencies:\n","                    frequencies[word] = 1\n","                else:\n","                    frequencies[word] += 1\n","\n","                if frequencies[word] == self.freq_threshold:\n","                    self.stoi[word] = idx\n","                    self.itos[idx]  = word\n","                    idx += 1\n","\n","    def numericalize(self, text):\n","\n","        tokenized_text = self.tokenizer_eng(text)\n","        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text]\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, img_file, target_file, image_size=128, freq_threshold=5):\n","\n","        self.img_file = img_file\n","        self.captions = json.load(open(target_file, \"r\"))\n","\n","        self.images   = None\n","\n","        self.img_transform  = transforms.Compose([transforms.Resize(image_size),\n","                                                  transforms.RandomHorizontalFlip(),\n","                                                  transforms.CenterCrop(image_size)])\n","        \n","        self.all_captions = []\n","\n","        for i in range(len(self.captions)):\n","            for text in self.captions[i]: self.all_captions.append(text)\n","\n","        # Initialize vocabulary and build vocab\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocabulary(self.all_captions)\n","\n","    def __len__(self):\n","        return len(self.captions)\n","\n","    def __getitem__(self, idx):\n","\n","        if not self.images: self.images = h5py.File(self.img_file, 'r') \n","            \n","        img = self.images[\"images\"][idx].astype(float)\n","        img = torch.from_numpy((img - img.min()) / np.max([img.max() - img.min(), 1]))\n","\n","        plt.imshow((img.numpy().transpose(1, 2, 0)+1)/2)\n","        plt.show()\n","\n","        caption = self.captions[idx]      \n","        text    = random.choice(caption)\n","\n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        numericalized_caption += self.vocab.numericalize(text)\n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","        \n","\n","        return img, torch.tensor(numericalized_caption)\n","\n","\n","class MyCollate:\n","    def __init__(self, pad_idx):\n","        self.pad_idx = pad_idx\n","\n","    def __call__(self, batch):\n","\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","\n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n","\n","        return imgs, targets"],"metadata":{"id":"VfCg2eNLzf7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_set = CustomDataset(\"./TRAIN_IMAGES_coco_5_cap_per_img.hdf5\", \"./TRAIN_CAPTIONS_coco_5_cap_per_img.json\")\n","val_set   = CustomDataset(\"./VAL_IMAGES_coco_5_cap_per_img.hdf5\",   \"./VAL_CAPTIONS_coco_5_cap_per_img.json\")\n","test_set  = CustomDataset(\"./TEST_IMAGES_coco_5_cap_per_img.hdf5\",  \"./TEST_CAPTIONS_coco_5_cap_per_img.json\")"],"metadata":{"id":"j84Tzf-EOZb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(dataset=train_set, batch_size=32, num_workers=2, shuffle=True,\n","                          collate_fn=MyCollate(pad_idx=test_set.vocab.stoi[\"<PAD>\"]))\n","\n","test_loader =  DataLoader(dataset=test_set, batch_size=1, num_workers=2, shuffle=False,\n","                          collate_fn=MyCollate(pad_idx=test_set.vocab.stoi[\"<PAD>\"]))\n","\n","valid_loader = DataLoader(dataset=val_set, batch_size=1, num_workers=2, shuffle=False,\n","                          collate_fn=MyCollate(pad_idx=test_set.vocab.stoi[\"<PAD>\"]))"],"metadata":{"id":"jDOIRCMn4XN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    step = checkpoint[\"step\"]\n","    return step"],"metadata":{"id":"AKWnQaB3E2f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSOEKwgiAqII"},"outputs":[],"source":["from torchvision.models import inception_v3, Inception_V3_Weights\n","\n","\n","class ImageCaptioning(nn.Module):\n","    def __init__(self, embed_size,hidden_size, vocab_size, num_layers, train_CNN=False):\n","    \n","        super(ImageCaptioning, self).__init__()\n","    \n","        # Encoder\n","        self.train_CNN    = train_CNN\n","        self.inception    = inception_v3(Inception_V3_Weights.DEFAULT, aux_logits=True)\n","        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n","        self.relu         = nn.ReLU()\n","        self.times        = []        \n","        self.dropout1     = nn.Dropout(0.5)  \n","\n","        # Decoder\n","        self.embed    = nn.Embedding(vocab_size, embed_size)\n","        self.lstm     = nn.LSTM(embed_size, hidden_size, num_layers)\n","        self.linear   = nn.Linear(hidden_size, vocab_size)              \n","        self.dropout1 = nn.Dropout(0.5)\n","\n","\n","    def forward(self, images, captions):\n","\n","        # Encoder\n","        features = self.inception(images)\n","\n","        ### Only fine-tuning the Encoder\n","        for name, param in self.incepetion.names_parameters():\n","            param.requires_grad = True if \"fc.weight\" in name or \"fc.bias\" in name else self.train_CNN\n","\n","        features = self.dropout1(self.relu(features))\n","\n","        # Decoder\n","        embeddings = self.dropout2(self.embed(captions))\n","        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n","        hiddens, _ = self.lstm(embeddings)\n","\n","        return self.linear(hiddens)\n","\n","    def caption_image(self, image, vocabulary, max_length=50):\n","        result_caption = []\n","\n","        with torch.no_grad():\n","\n","            features = self.inception(image)\n","\n","            for name, param in self.incepetion.names_parameters():\n","                param.requires_grad = True if \"fc.weight\" in name or \"fc.bias\" in names else self.train_CNN\n","\n","            x = self.dropout1(self.relu(features)).unsqueeze(0)\n","            states = None\n","\n","            for _ in range(max_length):\n","\n","                hiddens, states = self.lstm(x, states)\n","                output          = self.linear(hiddens.squeeze(0))\n","                predicted       = output.argmax(1)\n","\n","                result_caption.append(predicted.item())\n","                x = self.embed(predicted).unsqueeze(0)\n","\n","                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                    break\n","\n","        return [vocabulary.itos[idx] for idx in result_caption]"]},{"cell_type":"code","source":["def train(train_loader, dataset):\n","    transform = transforms.Compose([transforms.Resize((356, 356)),\n","                                    transforms.RandomCrop((299, 299)),\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n","\n","    torch.backends.cudnn.benchmark = True\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    load_model, save_model, train_CNN = False, False, False\n","\n","\n","    # Hyperparameters\n","    embed_size    = 256\n","    hidden_size   = 256\n","    vocab_size    = len(dataset.vocab)\n","    num_layers    = 1\n","    learning_rate = 3e-4\n","    num_epochs    = 100\n","\n","    # for tensorboard\n","    writer = SummaryWriter(\"runs/flickr\")\n","    step   = 0\n","\n","    # initialize model, loss etc\n","    model     = ImageCaptioning(embed_size, hidden_size, vocab_size, num_layers, train_CNN).to(device)\n","    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Only finetune the CNN\n","    for name, param in model.inception.named_parameters():\n","        param.requires_grad = True if \"fc.weight\" in name or \"fc.bias\" in name else train_CNN\n","\n","    if load_model:\n","        step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n","\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        # Uncomment the line below to see a couple of test cases\n","        # print_examples(model, device, dataset)\n","\n","        if save_model:\n","            checkpoint = {\"state_dict\" : model.state_dict(),\n","                          \"optimizer\"  : optimizer.state_dict(),\n","                          \"step\"       : step}\n","            save_checkpoint(checkpoint)\n","\n","        for idx, (imgs, captions) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n","\n","            imgs     = imgs.float().to(device)\n","            captions = captions.float().to(device)\n","\n","            outputs = model(imgs, captions[:-1])\n","            loss    = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n","\n","            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n","            step += 1\n","\n","            optimizer.zero_grad()\n","            loss.backward(loss)\n","            optimizer.step()"],"metadata":{"id":"LZI9ldHiEd1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","train(train_loader, train_set)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"90_LGIVJ8_fZ","executionInfo":{"status":"error","timestamp":1658318545878,"user_tz":180,"elapsed":34709,"user":{"displayName":"pdcle19 unicamp","userId":"00971240128642257250"}},"outputId":"53ce3b65-a463-4fb6-d5d1-3b88bcd36e55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:136: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n","  f\"Using {sequence_to_str(tuple(keyword_only_kwargs.keys()), separate_last='and ')} as positional \"\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-a7ac9b683be9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-d62c7a526796>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, dataset)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-09f8232a964b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, captions)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m### Only fine-tuning the Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInceptionOutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0maux_defined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAuxLogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAuxLogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;31m# N x 768 x 17 x 17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_7a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m# N x 128 x 5 x 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;31m# N x 768 x 1 x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;31m# Adaptive average pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (4 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-sA4mpZN-tQF"},"execution_count":null,"outputs":[]}]}